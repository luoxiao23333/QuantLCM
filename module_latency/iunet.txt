--------------------
No. 1
<class 'diffusers.models.embeddings.Timesteps'> take 0.35095890052616596 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.11273915879428387 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.06348011083900928 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.09112013503909111 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04724995233118534 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.43146894313395023 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 1057.5379279907793 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2720588818192482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.477928901091218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07015001028776169 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1431989949196577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04245992749929428 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13172999024391174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.101899029687047 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19499892368912697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048999907448887825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.037049874663352966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09347987361252308 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5495771076530218 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07788906805217266 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17155916430056095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06713997572660446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.12385006994009018 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.768359940499067 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09967992082238197 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.28957915492355824 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0951599795371294 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13984995894134045 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.44257892295718193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0765498261898756 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5742588546127081 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 6.171148037537932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03684894181787968 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 6.650286028161645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09356997907161713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18852995708584785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049220165237784386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0896090641617775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03947014920413494 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07470999844372272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09322981350123882 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18097995780408382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700012505054474 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015599885955452919 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09034993126988411 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0041480418294668 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1832498237490654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09315996430814266 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.230041988193989 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09490991942584515 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27579907327890396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0944200437515974 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13769883662462234 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3757490776479244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0722298864275217 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4959888756275177 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.4721590131521225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040950020775198936 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.891937995329499 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08904002606868744 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11819996871054173 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 15.368998982012272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0816399697214365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17881998792290688 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05122995935380459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09165005758404732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08273008279502392 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07564900442957878 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16239890828728676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04688999615609646 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14369003474712372 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034980010241270065 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0936078615486622 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408019155263901 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16286899335682392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04769978113472462 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09496998973190784 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.38462900556623936 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0916400458663702 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2580790314823389 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10138005018234253 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11266907677054405 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2631989773362875 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09764009155333042 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4111190792173147 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5344670973718166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04450022242963314 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9500060006976128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16549997963011265 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0492599792778492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14467001892626286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040138838812708855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0720699317753315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07452000863850117 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631800550967455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01619989052414894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14562904834747314 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0751578956842422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07260008715093136 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16149994917213917 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05577993579208851 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09480887092649937 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.39520906284451485 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09143003262579441 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25109993293881416 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09249895811080933 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1125698909163475 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26139989495277405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09581912308931351 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40770904161036015 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.523236045613885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04248996265232563 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9446162041276693 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14153006486594677 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16963016241788864 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.3482969999313354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08990010246634483 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18456904217600822 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05228002555668354 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14321994967758656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04037003964185715 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08300901390612125 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16638007946312428 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015930039808154106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2544489689171314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041950028389692307 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2816570233553648 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483991794288158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16438006423413754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05716993473470211 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09315996430814266 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21313992328941822 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2650800161063671 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0919499434530735 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11043902486562729 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22108899429440498 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1435300800949335 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41555892676115036 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.362278126180172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05461997352540493 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8001669086515903 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07593980990350246 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1672899816185236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04912004806101322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2524489536881447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018004983663559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07435004226863384 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743200071156025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16497005708515644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0164201483130455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25239004753530025 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3011880218982697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07289997301995754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16317912377417088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05715014412999153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09354017674922943 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21326891146600246 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09361980482935905 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2589300274848938 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09447894990444183 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11031003668904305 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21820981055498123 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1428488176316023 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4116990603506565 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3518568594008684 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05456991493701935 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.784265972673893 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2483888529241085 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2765387762337923 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.556486012414098 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07648998871445656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17681997269392014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05201995372772217 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2533090300858021 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04094000905752182 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07735006511211395 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415004074573517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16281893476843834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049179885536432266 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.252370024099946 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3120980001986027 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16898894682526588 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049629947170615196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.251139048486948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040339771658182144 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07088994607329369 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07614004425704479 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16438006423413754 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015769852325320244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25253999046981335 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2904070317745209 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.6662349700927734 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07505901157855988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1670289784669876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900991916656494 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25149015709757805 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04033907316625118 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07180985994637012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16272999346256256 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842015914618969 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015689991414546967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2541488502174616 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2915071565657854 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473002187907696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17037894576787949 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06010010838508606 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09594997391104698 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19811000674962997 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069987572729588 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2708900719881058 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09182910434901714 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10226992890238762 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20984001457691193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 4.216291941702366 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 4.477652022615075 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.415298976004124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05514989607036114 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.8608578983694315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07993006147444248 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1751999370753765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05121016874909401 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2533600199967623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978004679083824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07304013706743717 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746601726859808 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16324897296726704 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04914985038340092 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016349833458662033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.252248952165246 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3062169309705496 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 8.576291846111417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07565016858279705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1678098924458027 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4703989252448082 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03996002487838268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07486995309591293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16777007840573788 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892004653811455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595006324350834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25524990633130074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07561012171208858 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6166470013558865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16390904784202576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04977989010512829 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4723789170384407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04088995046913624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07261009886860847 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433909922838211 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16706902533769608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910002462565899 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595006324350834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2519399859011173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07738894782960415 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.612106105312705 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357005961239338 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16526016406714916 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048350077122449875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4698289558291435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07319008000195026 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16300007700920105 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873983561992645 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0156499445438385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523490693420172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0783097930252552 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6006468795239925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525290474295616 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.47434912994503975 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.493539152666926 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07730000652372837 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1686401665210724 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04890887066721916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4727388732135296 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0420399010181427 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07417984306812286 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07613003253936768 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17543998546898365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779989831149578 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015778932720422745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2528198529034853 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07647997699677944 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.622217008844018 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730298925191164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633188221603632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057049794122576714 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09391992352902889 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21395902149379253 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09337998926639557 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2643491607159376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09174994193017483 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11168979108333588 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2247400116175413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14294893480837345 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42135920375585556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.365656964480877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05579995922744274 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8005960155278444 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0756590161472559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16544898971915245 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048690009862184525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4714890383183956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07194001227617264 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751300249248743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16376911662518978 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05539017729461193 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016330042853951454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525700256228447 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07592909969389439 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.607885817065835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739898532629013 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.164759811013937 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058800214901566505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09520910680294037 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21240022033452988 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08914992213249207 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2579388674348593 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09157019667327404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11239992454648018 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22188900038599968 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14277012087404728 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4136089701205492 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3455769512802362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05450006574392319 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7794771119952202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07650000043213367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18019997514784336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04970002919435501 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3604888916015625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07202988490462303 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16200891695916653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0478401780128479 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01574005000293255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2551800571382046 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0634992029517889 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4926360454410315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07625995203852654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17912988550961018 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05971989594399929 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09367894381284714 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21385005675256252 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09019998833537102 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2611188683658838 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09187008254230022 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11134985834360123 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22010901011526585 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14290004037320614 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42125885374844074 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3577570207417011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0545999500900507 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8112268298864365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521490678191185 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3434291575103998 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.695247910916805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13975892215967178 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23243902251124382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05497015081346035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36056898534297943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040770042687654495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07522013038396835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16387994401156902 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04861992783844471 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.153619097545743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06765010766685009 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4616770204156637 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422990165650845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16404991038143635 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04505901597440243 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09655999019742012 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3922788891941309 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102979674935341 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25236885994672775 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09141000919044018 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11211005039513111 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2603200264275074 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09569898247718811 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4047991242259741 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5247671399265528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04496006295084953 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9350259099155664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09839003905653954 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19003893248736858 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049080001190304756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25168992578983307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040140002965927124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07167900912463665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384014315903187 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16353977844119072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015960074961185455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1435689628124237 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052449991926550865 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2741370592266321 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16425014473497868 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04491000436246395 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09296904318034649 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3944390919059515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09388988837599754 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2689699176698923 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09115994907915592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1127799041569233 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2623398322612047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09435904212296009 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40783919394016266 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5361460391432047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04581012763082981 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9459458999335766 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08596992120146751 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17789984121918678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049958936870098114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19776984117925167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04030996933579445 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07175002247095108 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07368996739387512 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16249902546405792 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04819990135729313 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01569977030158043 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14344998635351658 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04582991823554039 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.199526945129037 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07251999340951443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16206013970077038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04460010677576065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256997145712376 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.387449050322175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09083910845220089 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24991994723677635 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09437999688088894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1127889845520258 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2611891832202673 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09519001469016075 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40743895806372166 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.506387023255229 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04443898797035217 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9112860318273306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2601600717753172 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.39252988062798977 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.346409166231751 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2361901570111513 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39570010267198086 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10564900003373623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19925995729863644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03979983739554882 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07389998063445091 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09454996325075626 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18382910639047623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876009188592434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595006324350834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04751002416014671 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4481679536402225 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09116996079683304 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1802199985831976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038590049371123314 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09710993617773056 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.057191079482436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09393016807734966 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27654017321765423 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09190011769533157 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1389200333505869 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3764790017157793 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07220986299216747 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5015390925109386 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.304028978571296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03611994907259941 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.7121277786791325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1761589664965868 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29340898618102074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07852003909647465 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14678994193673134 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039828941226005554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07160007953643799 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08898996748030186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1774500124156475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803016781806946 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015590107068419456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09276904165744781 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04082988016307354 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.248786924406886 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08998997509479523 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18017995171248913 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03920006565749645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09406916797161102 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.078991943970323 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09680981747806072 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2788091078400612 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09279977530241013 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13777008280158043 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.374798895791173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07180008105933666 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.496838940307498 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.325609119608998 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03841007128357887 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.739828804507852 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12938003055751324 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24627987295389175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07832003757357597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1439889892935753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039880163967609406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07172022014856339 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0893299002200365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17625000327825546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048588961362838745 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015659956261515617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09013013914227486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04082988016307354 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1942179407924414 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09000999853014946 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1817690208554268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03944011405110359 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09300000965595245 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.15076082572341 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09464006870985031 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2792300656437874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177997708320618 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13816985301673412 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3763190470635891 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0719497911632061 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4987991414964199 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.389619152992964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03617000766098499 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.804408108815551 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 21.40565705485642 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09241001680493355 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.027350150048732758 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 4.357141209766269 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 1151.2005568947643 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.25303009897470474 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.11139013804495335 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.05629006773233414 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02945982851088047 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03762007690966129 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.3358488902449608 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.09186007082462311 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08110003545880318 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1709400676190853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047629931941628456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09871996007859707 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039280159398913383 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07895892485976219 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09301002137362957 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1784700434654951 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046269968152046204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01616007648408413 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09058997966349125 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9989289101213217 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06786989979445934 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1534998882561922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03833998925983906 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09599002078175545 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.056462086737156 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09306985884904861 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2777199260890484 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09209895506501198 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13846997171640396 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3806590102612972 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07216003723442554 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5023789126425982 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.30228903517127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03592995926737785 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.7017679791897535 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09070010855793953 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1783191692084074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046620145440101624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08838996291160583 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039280159398913383 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07622991688549519 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09111897088587284 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17684884369373322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04667998291552067 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.028999987989664078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09143003262579441 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9956678841263056 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09098905138671398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17838901840150356 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03926991485059261 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256997145712376 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.22125100158155 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914598349481821 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27323910035192966 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969008922576904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13737985864281654 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3745988942682743 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07208995521068573 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4932391457259655 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.439898930490017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03618001937866211 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.8464480098336935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08529005572199821 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11249003000557423 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.764280825853348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545016705989838 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1674701925367117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047129811719059944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08736993186175823 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03957003355026245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0725090503692627 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07270998321473598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15752995386719704 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04603993147611618 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01482013612985611 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14325999654829502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032729003578424454 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0417478624731302 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0721900723874569 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922985039651394 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04463992081582546 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09634997695684433 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3800888080149889 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998997509479523 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2510901540517807 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09130989201366901 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11122901923954487 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25955913588404655 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09473995305597782 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40200911462306976 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4938369859009981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04191999323666096 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.891676103696227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07526017725467682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16340985894203186 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04833005368709564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14262995682656765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039989128708839417 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07157004438340664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07319985888898373 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588701270520687 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04688999615609646 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015149824321269989 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14380901120603085 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0526080150157213 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0721900723874569 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15870993956923485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04479987546801567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09180908091366291 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3931899555027485 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24957000277936459 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09117904119193554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11277012526988983 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2609200309962034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09527988731861115 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40397001430392265 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5176660381257534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04219985567033291 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.91826606169343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13942993246018887 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16649020835757256 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.177547853440046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0756292138248682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16683898866176605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14209002256393433 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03970996476709843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07472001016139984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434003055095673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16064010560512543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048170099034905434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526789903640747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03942009061574936 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2321269605308771 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360987365245819 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16147014684975147 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0559298787266016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078020229935646 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21024001762270927 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09000906720757484 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26043993420898914 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09159999899566174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10891910642385483 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21717906929552555 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14352984726428986 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4086489789187908 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3320280704647303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054448843002319336 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7565360758453608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489020936191082 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16481010243296623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048239948228001595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2504889853298664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03965012729167938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07331999950110912 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360009476542473 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161800067871809 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521299757063389 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2816479429602623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07319916039705276 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16045896336436272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05673011764883995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09268010035157204 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21063885651528835 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010988287627697 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25706016458570957 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090818852186203 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10989001020789146 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2177800051867962 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1437799073755741 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40925992652773857 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3330879155546427 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054539879783988 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.756465993821621 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24771993048489094 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.27459999546408653 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.4090469386428595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07664901204407215 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16867904923856258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2504799049347639 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039768870919942856 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07111998274922371 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415981963276863 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1620498951524496 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048930058255791664 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523190341889858 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.279287040233612 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731500331312418 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177003271877766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25328900665044785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04011997953057289 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0703700352460146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338006980717182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16017910093069077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25189900770783424 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2684271205216646 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5931240525096655 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623900607228279 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25308900512754917 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07214001379907131 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354002445936203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16020890325307846 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785996861755848 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01498986966907978 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25217002257704735 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2734879273921251 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437984459102154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16832002438604832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056819990277290344 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09306007996201515 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19841897301375866 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09123003110289574 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26014912873506546 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125005453824997 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1163901761174202 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22210017777979374 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14319992624223232 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4158089868724346 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.333446940407157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054530100896954536 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.76740693859756 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548998109996319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1679789274930954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04941015504300594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2502400893718004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040430109947919846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07036002352833748 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16090995632112026 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04776008427143097 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01604994758963585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2519588451832533 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2767070438712835 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.430220928043127 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467996329069138 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16279006376862526 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04820013418793678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4691188223659992 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018004983663559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07134885527193546 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177003271877766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01543993130326271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25164894759655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07434980943799019 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5811570920050144 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734301283955574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16044010408222675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04805903881788254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4698289558291435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04121009260416031 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07119984365999699 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345015183091164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16018981114029884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048530055209994316 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015170080587267876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2513399813324213 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07494981400668621 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5800069086253643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07324898615479469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16061915084719658 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703016020357609 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46909903176128864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040380051359534264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07156981155276299 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07306900806725025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16054906882345676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840991459786892 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015200115740299225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2514701336622238 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0752890482544899 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5754059422761202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24843006394803524 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.33556995913386345 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.2359700202941895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372000254690647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16126991249620914 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46904897317290306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03987015224993229 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07418915629386902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08216989226639271 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17005018889904022 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01516006886959076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521490678191185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07584993727505207 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6044268850237131 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0722298864275217 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15903986059129238 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05697901360690594 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09261001832783222 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2121699508279562 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08931919001042843 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2592899836599827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09002001024782658 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10980991646647453 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2180000301450491 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1431501004844904 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4082499071955681 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3315880205482244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05482998676598072 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7559861298650503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07936009205877781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16864994540810585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04800013266503811 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4695989191532135 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04045013338327408 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07136003114283085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07296004332602024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15948014333844185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04698988050222397 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015180092304944992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526689786463976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07656984962522984 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5928170178085566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738000962883234 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.160939060151577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0570099800825119 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254016913473606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2129098866134882 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08990895003080368 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2583898603916168 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071012027561665 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10951003059744835 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21773879416286945 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14157011173665524 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4068890120834112 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3324981555342674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054538948461413383 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7569460906088352 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07622991688549519 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1650601625442505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3596888855099678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07136003114283085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338006980717182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16041891649365425 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046740053221583366 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25153998285532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06333016790449619 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4570169150829315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07358985021710396 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16241008415818214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056759919971227646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09195995517075062 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21290895529091358 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904398038983345 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25672907941043377 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10964996181428432 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21657021716237068 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14341901987791061 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4063888918608427 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3290569186210632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05432986654341221 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7552569042891264 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25122915394604206 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3399290144443512 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.479648830369115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14001899398863316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23209908977150917 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05540996789932251 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3602290526032448 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04148995503783226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0726599246263504 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073309987783432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16796006821095943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045999884605407715 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14302018098533154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06455997936427593 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.435927115380764 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07193000055849552 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15857885591685772 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04466017708182335 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0918898731470108 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3931489773094654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09503006003797054 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24712900631129742 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08954992517828941 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11231005191802979 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2592990640550852 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09457999840378761 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39936904795467854 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.507897162809968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04206993617117405 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.902405871078372 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09808898903429508 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18436904065310955 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04659988917410374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25112018920481205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0391998328268528 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0700999516993761 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07288996130228043 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15921983867883682 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04660990089178085 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014990102499723434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14326884411275387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05183997564017773 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2401570565998554 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07084012031555176 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15575997531414032 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04439987242221832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065913036465645 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3917389549314976 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909008465707302 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24500000290572643 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08947891183197498 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11211005039513111 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25743013247847557 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10196981020271778 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40484010241925716 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4888260047882795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042689964175224304 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8800259567797184 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08589006029069424 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17456989735364914 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04838989116251469 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19725877791643143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039629871025681496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0710200984030962 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07238006219267845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15715998597443104 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04621990956366062 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014630146324634552 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1427598763257265 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045370077714324 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1718880850821733 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07210997864603996 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569588202983141 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044590095058083534 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0922300387173891 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3868888597935438 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08846982382237911 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2462498378008604 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08942000567913055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11121993884444237 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2571099903434515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09489897638559341 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.398268923163414 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4784869272261858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04239007830619812 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8717858474701643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25899894535541534 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3834289964288473 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.101578896865249 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23595988750457764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39461906999349594 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1054699532687664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1987700816243887 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03936910070478916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0714198686182499 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09193993173539639 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17893011681735516 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04708999767899513 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08949916809797287 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.046980101615190506 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4203069731593132 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09182002395391464 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17808005213737488 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0392301008105278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072897955775261 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.059091908857226 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09206985123455524 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2759690396487713 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0903101172298193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1376299187541008 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.373688992112875 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07101008668541908 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4906090907752514 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.276288837194443 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03614998422563076 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.678399000316858 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17554010264575481 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2908390015363693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07664994336664677 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1439990010112524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0384598970413208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0700000673532486 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08912989869713783 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17362996004521847 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05334010347723961 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014859950169920921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09014899842441082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04087993875145912 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2296270579099655 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09008985944092274 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17914013005793095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03909994848072529 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09235995821654797 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.078141879290342 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09222910739481449 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27915998362004757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13728998601436615 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37400983273983 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07180916145443916 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4930691793560982 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.30789908953011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035659875720739365 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.713437916710973 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12798001989722252 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24468894116580486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07821992039680481 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1432399731129408 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03964989446103573 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07008993998169899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08931010961532593 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17544999718666077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04766997881233692 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015630153939127922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08986005559563637 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04067993722856045 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1827470734715462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09016995318233967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1782400067895651 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038350000977516174 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09222002699971199 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.085122141987085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09308010339736938 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2802701201289892 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09088893420994282 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13778009451925755 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3743499983102083 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07132883183658123 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4934992175549269 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.314378999173641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036109937354922295 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.718508968129754 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 21.179236937314272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09336997754871845 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025769928470253944 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.14555896632373333 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 81.78876317106187 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20197010599076748 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05661998875439167 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.044558895751833916 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025270041078329086 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03748992457985878 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.26059895753860474 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0771000050008297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06961985491216183 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15623006038367748 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09021000005304813 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03994000144302845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07221987470984459 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0912488903850317 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17769914120435715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04801992326974869 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015639932826161385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08993991650640965 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9677379857748747 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0653101596981287 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15248917043209076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038250116631388664 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09382003918290138 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.127660999074578 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09375018998980522 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2789089921861887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09240000508725643 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1379998866468668 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3741688560694456 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07157004438340664 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4918889608234167 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.362919066101313 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03585987724363804 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.75980800203979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.090740155428648 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17854897305369377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047629931941628456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08918996900320053 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395698007196188 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06985990330576897 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0917899888008833 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17755897715687752 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722992889583111 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08968007750809193 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9695680346339941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08946005254983902 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17523998394608498 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03860890865325928 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0921902246773243 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.19486197642982 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09151012636721134 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27447985485196114 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08987914770841599 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13684993609786034 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3729599993675947 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07138983346521854 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4889490082859993 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.40875899605453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041010091081261635 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.812987918034196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08480995893478394 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11008000001311302 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.722002040594816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07468904368579388 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16499892808496952 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04637008532881737 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0868900679051876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03826012834906578 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07130997255444527 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07297913543879986 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569290179759264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04625995643436909 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01490977592766285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14247000217437744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03284984268248081 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0283580049872398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07177912630140781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1544291153550148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043800100684165955 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08991011418402195 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37965900264680386 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08775992318987846 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24569896049797535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08950009942054749 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11130981147289276 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26066997088491917 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09342003613710403 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3978591412305832 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4652968384325504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04196004010736942 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8497360870242119 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07317005656659603 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15906989574432373 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046408968046307564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14191004447638988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038700178265571594 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06969994865357876 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0719800591468811 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15572900883853436 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0463901087641716 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14349003322422504 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0267579928040504 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07220986299216747 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15628989785909653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04349998198449612 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09085005149245262 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3913789987564087 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08904002606868744 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24436903186142445 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08853012695908546 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1114201731979847 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25635003112256527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09437883272767067 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39567891508340836 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4739769976586103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04163989797234535 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8593259155750275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13875006698071957 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1642899587750435 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.026878021657467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366994395852089 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16146013513207436 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047140056267380714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14144997112452984 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038419850170612335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07079890929162502 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07254001684486866 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1564600970596075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047179870307445526 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014950055629014969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.251139048486948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03871996887028217 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1948170140385628 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07150997407734394 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1553699839860201 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05557900294661522 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986005559563637 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3483190666884184 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948007598519325 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2552100922912359 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08889986202120781 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1089298166334629 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2145799808204174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14168908819556236 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4010389093309641 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4444671105593443 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05355989560484886 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8555659335106611 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07356982678174973 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1590498723089695 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04623015411198139 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24939910508692265 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03826990723609924 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07066014222800732 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07162895053625107 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1546188723295927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04599010571837425 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014510005712509155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25080982595682144 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2381768319755793 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08282996714115143 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1782400067895651 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06019999273121357 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.091110123321414 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2104700542986393 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972990326583385 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25415909476578236 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896016515791416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10929000563919544 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21944893524050713 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14277012087404728 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40762918069958687 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3186668511480093 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05369982682168484 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7606571782380342 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24665892124176025 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.27170893736183643 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.418505916371942 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475982420146465 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1636201050132513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04735984839498997 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2501890994608402 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03847014158964157 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07028994150459766 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366994395852089 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15640887431800365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04714983515441418 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01471000723540783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2519800327718258 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.248447922989726 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07173907943069935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15498907305300236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04630000330507755 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2494901418685913 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038319965824484825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06968900561332703 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07245992310345173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15551014803349972 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04618987441062927 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014829915016889572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.251278979703784 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2330471072345972 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5240150280296803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0722899567335844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15838001854717731 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04702014848589897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24970900267362595 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03823009319603443 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06998982280492783 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08064997382462025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16601895913481712 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046350061893463135 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014599878340959549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2509600017219782 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.249547814950347 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294001989066601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16104988753795624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05553988739848137 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071989916265011 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19750883802771568 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08789007551968098 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2567591145634651 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08856994099915028 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10144989937543869 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20344997756183147 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1423889771103859 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3916090354323387 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.287196995690465 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05332985892891884 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7025868874043226 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07463013753294945 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161430099979043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04721013829112053 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2497001551091671 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385800376534462 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07036002352833748 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07256004028022289 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15684007667005062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047320034354925156 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014940043911337852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2515690866857767 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2483270838856697 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.2982210870832205 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07283990271389484 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15865988098084927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04601990804076195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4696589894592762 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0406589824706316 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07111998274922371 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357005961239338 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15820004045963287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04630000330507755 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479010097682476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25050900876522064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07381988689303398 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5595972072333097 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07231906056404114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15771901234984398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04624016582965851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4681791178882122 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06947992369532585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07163011468946934 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15580002218484879 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0462997704744339 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014620134606957436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2509700134396553 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07381010800600052 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5476460102945566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07172022014856339 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1558701042085886 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045689987018704414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4681588616222143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07027015089988708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07224013097584248 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15536905266344547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04679011180996895 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014859950169920921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2513390500098467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07485994137823582 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5482869930565357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24786009453237057 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3287999425083399 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.139379063621163 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07266993634402752 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15829992480576038 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04763994365930557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46936911530792713 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038400059565901756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07102987729012966 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305014878511429 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569290179759264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04646996967494488 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2512899227440357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07477914914488792 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5575969591736794 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07150997407734394 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15606009401381016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055999960750341415 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072991088032722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21222000941634178 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08774013258516788 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25467900559306145 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08903001435101032 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1097000204026699 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21542981266975403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1418089959770441 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4028088878840208 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3104970566928387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05452008917927742 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7253269907087088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344991900026798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15849899500608444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761014133691788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46931905671954155 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03863009624183178 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07123011164367199 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305992767214775 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15789992175996304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046008965000510216 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014969846233725548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2513499930500984 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07547996938228607 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5591271221637726 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0715500209480524 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15612016431987286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05673011764883995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032990783452988 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21188892424106598 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798995986580849 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25423895567655563 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08922000415623188 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10900013148784637 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21367985755205154 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14178897254168987 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39916904643177986 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.307826954871416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054320087656378746 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7210959922522306 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503991946578026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.163898803293705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3606700338423252 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040289945900440216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07086992263793945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400009781122208 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16037002205848694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046969857066869736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015049939975142479 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25141891092061996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06296997889876366 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4568669721484184 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07278984412550926 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15994906425476074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056419987231492996 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09159999899566174 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21253013983368874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08887890726327896 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25665992870926857 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028008207678795 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10974006727337837 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21558906883001328 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14168000780045986 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40274905040860176 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3201781548559666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05405908450484276 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.741295913234353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25123986415565014 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34029013477265835 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.312049882486463 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13937987387180328 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22994889877736568 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05534989759325981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36008888855576515 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040490180253982544 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07151998579502106 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0728401355445385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16004894860088825 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047729816287755966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14350004494190216 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06453017704188824 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4316169545054436 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07231906056404114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589388120919466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04409998655319214 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09190011769533157 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3903191536664963 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08985004387795925 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24623889476060867 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09022001177072525 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11175009422004223 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2581991720944643 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0940798781812191 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3978791646659374 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4846569392830133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0417900737375021 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8787761218845844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09802891872823238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18635904416441917 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843994975090027 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25092880241572857 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04003988578915596 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07066992111504078 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07224013097584248 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1581301912665367 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04799012094736099 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14302902854979038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05191005766391754 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.248347107321024 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07184990681707859 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1583402045071125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044419197365641594 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0945099163800478 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3926090430468321 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902798492461443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24709897115826607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09088008664548397 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1119500957429409 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2580599393695593 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09400909766554832 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3975890576839447 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4911568723618984 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04239007830619812 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.899875933304429 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08644000627100468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1758788712322712 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04961993545293808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19675004296004772 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03966991789638996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07067015394568443 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07199007086455822 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15903986059129238 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04816986620426178 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14325999654829502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045168912038207054 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1793770827353 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07264991290867329 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15831994824111462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04415004514157772 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094993583858013 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3882087767124176 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968892507255077 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25277002714574337 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09052013047039509 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1116190105676651 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.257859006524086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09431992657482624 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39728893898427486 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4877868816256523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042079947888851166 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8811470363289118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25924015790224075 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.38481014780700207 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.118839098140597 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23580994457006454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39359903894364834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10597985237836838 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19832001999020576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04040985368192196 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07120007649064064 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09130919352173805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17810892313718796 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876009188592434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015440164133906364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08933991193771362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04616985097527504 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4223670586943626 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09008892811834812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1778190489858389 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03852020017802715 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167985990643501 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.059021826833487 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09216996841132641 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2753899898380041 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072897955775261 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13735005632042885 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3732901532202959 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07244991138577461 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49213902093470097 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.278758937492967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03588991239666939 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.679868161678314 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17544906586408615 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.28962898068130016 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07782992906868458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1446900423616171 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07039890624582767 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08889986202120781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17469003796577454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04798988811671734 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015360070392489433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08957996033132076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040029874071478844 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2229671701788902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08973013609647751 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17718016169965267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038560014218091965 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909687951207161 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.089131951332092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09137997403740883 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27877907268702984 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09070988744497299 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13738987036049366 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37338887341320515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07168995216488838 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49095903523266315 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.313099129125476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03627012483775616 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.714429076761007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1249189954251051 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23996899835765362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07890001870691776 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14339014887809753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05637994036078453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07111998274922371 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09045982733368874 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17752987332642078 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04816986620426178 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015389872714877129 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08957902900874615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04047015681862831 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1996771208941936 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09043002501130104 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1775000710040331 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03890995867550373 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253900498151779 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.082382190972567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106006473302841 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2784291282296181 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112013503909111 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13727904297411442 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3734987694770098 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07149018347263336 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4901890642940998 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.305038997903466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03611994907259941 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.7079491671174765 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 21.180527051910758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09411992505192757 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02624979242682457 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.12666010297834873 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 80.9680549427867 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.2029789611697197 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05358015187084675 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.044209882616996765 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026209978386759758 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03706011921167374 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2566799521446228 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07402896881103516 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06917002610862255 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15722005628049374 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04861992783844471 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.089969951659441 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03971997648477554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0700000673532486 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09128008969128132 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1775398850440979 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015520025044679642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09000883437693119 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9633081499487162 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06421003490686417 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03827991895377636 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09213993325829506 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.131841938942671 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09394995868206024 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2792098093777895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09079999290406704 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13811886310577393 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3752289339900017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07164012640714645 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4932589363306761 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.36031904630363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03550015389919281 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.752817960456014 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09134993888437748 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1789000816643238 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08863885886967182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03946991637349129 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06956979632377625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09078998118638992 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17658015713095665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04823016934096813 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015020137652754784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08970987983047962 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.970667926594615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08966983295977116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17546000890433788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042220111936330795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248987771570683 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.071451956406236 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205006062984467 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27797906659543514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09008985944092274 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13764994218945503 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3732689656317234 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07115001790225506 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.48972899094223976 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.290179047733545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035670120269060135 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.694099003449082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08452893234789371 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1106590498238802 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.60126188956201 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541012018918991 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1663300208747387 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08700997568666935 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039800070226192474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07082009688019753 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385015487670898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15958002768456936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047209905460476875 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14284905046224594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03261980600655079 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0424279607832432 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07223989814519882 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15752995386719704 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04412000998854637 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09155902080237865 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37917005829513073 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08933991193771362 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2563600428402424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10203896090388298 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11222995817661285 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2579099964350462 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09419000707566738 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3972700797021389 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4978458639234304 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041220104321837425 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8903061281889677 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177003271877766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047419918701052666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14189910143613815 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03962009213864803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06997003220021725 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335981354117393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15909993089735508 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047318870201706886 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14336011372506618 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.042268006131053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0712699256837368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15710992738604546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043828971683979034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09224005043506622 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.38198893889784813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08996017277240753 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24672900326550007 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090339919552207 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11057010851800442 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.256469938904047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09416905231773853 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.396049115806818 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4738470781594515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0417300034314394 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8644558731466532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13920990750193596 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16541988588869572 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.1069680377841 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16565993428230286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05461997352540493 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1414800062775612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039479928091168404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07112906314432621 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420009933412075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16036001034080982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04752003587782383 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015400117263197899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2515790984034538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03898981958627701 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.220097066834569 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07359986193478107 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159549992531538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05585001781582832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09063002653419971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21192990243434906 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08987984620034695 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2570291981101036 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09107007645070553 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10920991189777851 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21562911570072174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14269002713263035 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4035790916532278 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3196468353271484 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053819967433810234 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7375268507748842 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749791506677866 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16223895363509655 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0482702162116766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2498500980436802 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395399983972311 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07119006477296352 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15891017392277718 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810979589819908 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2512088976800442 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2753068003803492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738899689167738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16222987323999405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05669984966516495 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915199052542448 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2127599436789751 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09019998833537102 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25801011361181736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09039998985826969 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10945904068648815 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21461909636855125 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14288979582488537 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4032289143651724 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.322528114542365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05374010652303696 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7443359829485416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24734996259212494 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2739999908953905 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.3525468576699495 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0758189707994461 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1665989402681589 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0489500816911459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25034998543560505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04060007631778717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07069017738103867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393001578748226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15974999405443668 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803994670510292 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516589593142271 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2662969529628754 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07251999340951443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15797000378370285 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04831980913877487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25065895169973373 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039590056985616684 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07827999070286751 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07332907989621162 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589590683579445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04805997014045715 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015430152416229248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25138002820312977 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2659470085054636 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.576183993369341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07341988384723663 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16142986714839935 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24978886358439922 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939005546271801 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07081986404955387 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07323012687265873 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15861005522310734 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04839012399315834 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01484900712966919 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510200720280409 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2578279711306095 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07421988993883133 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16351905651390553 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0563301146030426 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09193993173539639 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19702990539371967 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905690249055624 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2582799643278122 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08993013761937618 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10196003131568432 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2055990044027567 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14217989519238472 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39519905112683773 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3004879001528025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05393894389271736 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7257360741496086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455004379153252 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16259006224572659 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.055380165576934814 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2502589486539364 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03976002335548401 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07055001333355904 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07380987517535686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594799105077982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049249036237597466 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2509600017219782 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.27028813585639 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.354151897132397 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363990880548954 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613989006727934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047659967094659805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4691390786319971 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04036002792418003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07092999294400215 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393909618258476 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16104918904602528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04820013418793678 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015430152416229248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2514198422431946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07424899376928806 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5742559917271137 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07255980744957924 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15817000530660152 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047449953854084015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46872906386852264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03957003355026245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07039890624582767 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07255002856254578 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582801342010498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0473798718303442 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015009893104434013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25215884670615196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07627997547388077 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5703970566391945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344991900026798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16084895469248295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734005779027939 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4683190491050482 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039560021832585335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0705900602042675 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07319985888898373 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15834998339414597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047618988901376724 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015330035239458084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25136000476777554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07415004074573517 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5638768672943115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2484000287950039 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3305799327790737 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.1967790350317955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293908856809139 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15983893536031246 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46926899813115597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039560021832585335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07075001485645771 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07347902283072472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15898910351097584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04708999767899513 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25164010003209114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07524900138378143 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5677260234951973 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393001578748226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16788998618721962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05667004734277725 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909990631043911 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2109999768435955 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08965004235506058 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2579891588538885 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091012179851532 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.109479995444417 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21567987278103828 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14270003885030746 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4045891109853983 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3228869065642357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054080039262771606 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.751946983858943 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074450159445405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608789898455143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4693889059126377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039950013160705566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07103988900780678 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385015487670898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15961984172463417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04691001959145069 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25188992731273174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07494981400668621 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5712270978838205 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07236888632178307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15850900672376156 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056339893490076065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09501981548964977 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21316902711987495 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10446994565427303 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2580599393695593 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032897651195526 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10956008918583393 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2170500811189413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14137919060885906 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4035891033709049 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3419571332633495 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05442998372018337 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7602560110390186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07602013647556305 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16453000716865063 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048138899728655815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.35967002622783184 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396000687032938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07060915231704712 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738000962883234 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15946989879012108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046819914132356644 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2511588390916586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06344006396830082 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4515169896185398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07241987623274326 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15985011123120785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05646003410220146 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09154016152024269 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21274015307426453 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884910494089127 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2558401320129633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09066914208233356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11023017577826977 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22233999334275723 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14365999959409237 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4119297955185175 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3286280445754528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05419016815721989 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.748776063323021 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25137001648545265 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34094881266355515 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.405278066173196 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14012004248797894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23171002976596355 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05518016405403614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36020902916789055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04051998257637024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07136003114283085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353979162871838 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15908991917967796 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04759989678859711 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015399884432554245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14290004037320614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06407010369002819 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4239370357245207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07222010754048824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585299614816904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04408019594848156 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110919199883938 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.39265002124011517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029894135892391 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24647987447679043 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09097019210457802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11202995665371418 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26095006614923477 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09437999688088894 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40003005415201187 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.502977218478918 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042010098695755005 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8993359990417957 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09200000204145908 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18003000877797604 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047919806092977524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510088961571455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04046992398798466 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07016980089247227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0728098675608635 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15817885287106037 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724995233118534 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015350058674812317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14296011067926884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051789917051792145 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2407980393618345 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07209996692836285 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15710899606347084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04412000998854637 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153014980256557 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3919689916074276 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08997018449008465 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24581910111010075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08965004235506058 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11190003715455532 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25766994804143906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09376904927194118 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39770896546542645 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4846271369606256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041980063542723656 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8758559599518776 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0863298773765564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17608003690838814 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04957010969519615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1970701850950718 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04008994437754154 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07032998837530613 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419893518090248 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15987898223102093 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04733004607260227 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1434599980711937 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044899992644786835 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1883580591529608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0722899567335844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1584689598530531 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044250162318348885 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09158998727798462 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3918989095836878 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957996033132076 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24677906185388565 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906200148165226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11176895350217819 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25824899785220623 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09415997192263603 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39814901538193226 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4872970059514046 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04163011908531189 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8794259522110224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26030000299215317 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3850290086120367 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.107229929417372 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2360600046813488 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3927589859813452 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10604900307953358 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19801990129053593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040220096707344055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07086992263793945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09158905595541 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1803790219128132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08914992213249207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045830151066184044 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4218369033187628 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09094900451600552 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1777291763573885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03816001117229462 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09189010597765446 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.100891994312406 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09347894228994846 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27694995515048504 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09073899127542973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13751001097261906 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3737998194992542 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0714289490133524 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4912989679723978 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.324788857251406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03610015846788883 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.725717870518565 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17542997375130653 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29064901173114777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07803994230926037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14412007294595242 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04009995609521866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07023988291621208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09108008816838264 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17666001804172993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047479989007115364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015560071915388107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08995993994176388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040279002860188484 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.227057073265314 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09020022116601467 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17594988457858562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03826990723609924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09221979416906834 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.0953210555016994 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10705995373427868 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26088906452059746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09117997251451015 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266098115593195 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3533388953655958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06810994818806648 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4670890048146248 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.293259164318442 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03371993079781532 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.6889590341597795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11972011998295784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23262994363904 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07448997348546982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13121007941663265 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04060892388224602 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06748992018401623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08466001600027084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17064018175005913 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04831003025174141 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08263904601335526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03890995867550373 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1365278623998165 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0841701403260231 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17206999473273754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03670994192361832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248894639313221 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9017421659082174 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09086006321012974 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26041013188660145 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028916247189045 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12587988749146461 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3520599566400051 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06763986311852932 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46574906446039677 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.079989088699222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034139957278966904 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.470599047839642 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.899327006191015 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08930987678468227 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026040012016892433 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1405689399689436 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 80.83520387299359 ms for forwarding
--------------------
No. 2
<class 'diffusers.models.embeddings.Timesteps'> take 0.24727010168135166 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05819997750222683 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04773982800543308 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03758980892598629 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04060985520482063 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2886001020669937 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15378999523818493 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11073984205722809 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2353801392018795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05573011003434658 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.10857894085347652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04379008896648884 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0795100349932909 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0895399134606123 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18356996588408947 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05161017179489136 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01750001683831215 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08717994205653667 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.115407794713974 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07565994746983051 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16923993825912476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.046519096940755844 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11634011752903461 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.060362000018358 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10435003787279129 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2760789357125759 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1048198901116848 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13029901310801506 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.36725890822708607 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06964989006519318 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4912191070616245 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.422029877081513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03842008300125599 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.870528053492308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.086609972640872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1766399946063757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785996861755848 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08278898894786835 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039029866456985474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06872997619211674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08723000064492226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17365999519824982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04666997119784355 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08397991769015789 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9501979220658541 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08435011841356754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17159990966320038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037029851227998734 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09340886026620865 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.112262045964599 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09227893315255642 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2746500540524721 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12711994349956512 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35677989944815636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06836000829935074 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4719288554042578 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.3198980167508125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035049859434366226 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.712877959012985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.079659977927804 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10703015141189098 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.870642054826021 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07712002843618393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1691791694611311 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722992889583111 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08330983109772205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03984011709690094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07346994243562222 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350905798375607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588989980518818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04571001045405865 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014940043911337852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1331600360572338 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03198999911546707 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0408279486000538 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07282895967364311 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159058952704072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04412000998854637 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09063002653419971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.361159211024642 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09887991473078728 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27868920005857944 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09096995927393436 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10356004349887371 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24631991982460022 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09096902795135975 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38543902337551117 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4900867827236652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04258006811141968 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8941659945994616 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08669006638228893 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17992989160120487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04612887278199196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13218005187809467 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038869911804795265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07349997758865356 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16021891497075558 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047239940613508224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13336003758013248 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0441779159009457 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07172999903559685 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15817000530660152 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05310005508363247 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0941399484872818 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36906893365085125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09399000555276871 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2504289150238037 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125005453824997 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10427017696201801 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24830992333590984 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09000906720757484 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3860290162265301 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4679771848022938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040449900552630424 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.874406123533845 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13050995767116547 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15986012294888496 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.1255579348653555 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754999928176403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16813003458082676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049179885536432266 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13210996985435486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03927992656826973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07279985584318638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402990013360977 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16155000776052475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04758988507091999 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23283902555704117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03753998316824436 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1958677787333727 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07340987212955952 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616401132196188 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05341903306543827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09284983389079571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19614980556070805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10725902393460274 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2681200858205557 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09140907786786556 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10343012399971485 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2106300089508295 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13210996985435486 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39293011650443077 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3484670780599117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05495012737810612 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.786006148904562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16503012739121914 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04897988401353359 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22725877352058887 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0392599031329155 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068980036303401 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369904778897762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16059912741184235 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795007407665253 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22858008742332458 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2227680999785662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07280008867383003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16178889200091362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053419964388012886 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09261001832783222 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19397889263927937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896994404494762 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25462894700467587 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016995318233967 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10389997623860836 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20953011699020863 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1320790033787489 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3922688774764538 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3005868531763554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05443999543786049 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7383159138262272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22558914497494698 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2531690988689661 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.3062559347599745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07686996832489967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1705600880086422 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22940896451473236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07548998109996319 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453002035617828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227993182837963 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892982542514801 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015609897673130035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2300699707120657 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2582279741764069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738899689167738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1619388349354267 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22782990708947182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03916001878678799 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06766989827156067 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07309019565582275 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596398651599884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0153400469571352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23002899251878262 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2221268843859434 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.527145203202963 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366994395852089 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1639598049223423 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873005673289299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2274590078741312 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906991332769394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06857002153992653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413001731038094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16100984066724777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.056928955018520355 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018559861928224564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23274985142052174 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2601781636476517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482897490262985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1666590105742216 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05405000410974026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09406008757650852 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18189894035458565 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10438007302582264 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.259229214861989 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09058997966349125 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09582005441188812 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20020990632474422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1328189391642809 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3831388894468546 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3106570113450289 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05414988845586777 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7536059021949768 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482990622520447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16295001842081547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048318877816200256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2276699524372816 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038969796150922775 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678100623190403 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07850979454815388 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17527886666357517 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05757017061114311 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015510013327002525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22851885296404362 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2494770344346762 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.376150900498033 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074789859354496 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634398940950632 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047789886593818665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4246791359037161 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040109967812895775 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07727881893515587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734301283955574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16061984933912754 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048070214688777924 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22909906692802906 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06947992369532585 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5162471681833267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07307995110750198 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16172020696103573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047588953748345375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4239401314407587 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039909034967422485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06801006384193897 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08863001130521297 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19237003289163113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048110028728842735 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22820895537734032 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06961985491216183 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.538367010653019 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07323012687265873 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16079004853963852 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04719989374279976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42376015335321426 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04023010842502117 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06805011071264744 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07323990575969219 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604000572115183 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015620142221450806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22924900986254215 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06965012289583683 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5009171329438686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 3.6030521150678396 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 3.701041918247938 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 8.421783102676272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09631994180381298 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19051902927458286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04906998947262764 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4270789213478565 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04043988883495331 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07186015136539936 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07887999527156353 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16614003106951714 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048649031668901443 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016129808500409126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22968999110162258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07035001181066036 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5543068293482065 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.072818947955966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16193906776607037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05321996286511421 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09341002441942692 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19608903676271439 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09002001024782658 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25877892039716244 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913599506020546 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10393001139163971 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2157201524823904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13811909593641758 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41155912913382053 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3308969791978598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05150004290044308 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7571658827364445 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0753400381654501 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637190580368042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.424358993768692 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0470699742436409 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07316004484891891 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08121016435325146 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16781012527644634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767905920743942 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22946996614336967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06989995017647743 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5335169155150652 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07288902997970581 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16082893125712872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05315011367201805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09276997298002243 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19350904040038586 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037996642291546 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2528000622987747 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09539909660816193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10242988355457783 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20853988826274872 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13184011913836002 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3884299658238888 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2940578162670135 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05118013359606266 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7107161693274975 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07573002949357033 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16400008462369442 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785996861755848 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3267989959567785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040049897506833076 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06837002001702785 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739500392228365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16078888438642025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04727998748421669 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22902991622686386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.062030041590332985 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4118868857622147 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335003465414047 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16437005251646042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06849993951618671 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0937699805945158 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19450881518423557 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998997509479523 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25969906710088253 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029009379446507 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10249996557831764 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20698015578091145 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13168994337320328 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38588885217905045 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2931269593536854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05079014226794243 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.731917029246688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23025902919471264 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31862896867096424 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.250398889183998 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1305900514125824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22092903964221478 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052530085667967796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32674986869096756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040348852053284645 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07661012932658195 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392000406980515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1662799622863531 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04718010313808918 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13113883323967457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060439808294177055 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.393606886267662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07361988537013531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16188016161322594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04208902828395367 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10995008051395416 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37126895040273666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09133992716670036 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25134882889688015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057018905878067 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10351999662816525 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24603982456028461 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08832896128296852 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3817090764641762 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4851668383926153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04003988578915596 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8812159541994333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09147007949650288 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18180999904870987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22903992794454098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737009294331074 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07128017023205757 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258006371557713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1595797948539257 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736008122563362 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015369849279522896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1314000692218542 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04874914884567261 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2273669708520174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07697008550167084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1739200670272112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0469498336315155 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09150896221399307 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36734994500875473 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015901014208794 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25392998941242695 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08996017277240753 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1034298911690712 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24474016390740871 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08859997615218163 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3808590117841959 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4546369202435017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04050903953611851 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.87790603376925 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0805598683655262 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17125997692346573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0498499721288681 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17964886501431465 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03942986950278282 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06703007966279984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310020737349987 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15844008885324 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04692981019616127 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015319790691137314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13096886686980724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04243990406394005 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1403579264879227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08628005161881447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19020005129277706 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04452001303434372 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36145979538559914 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09043002501130104 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24952995590865612 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0908400397747755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10249018669128418 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2440300304442644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08972012437880039 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.380589859560132 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4462578110396862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040618935599923134 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8777758814394474 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23915013298392296 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35791005939245224 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.975810069590807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22143987007439137 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3879200667142868 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1032492145895958 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18057995475828648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395399983972311 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06887991912662983 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08766003884375095 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17681904137134552 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015290221199393272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08332007564604282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04381011240184307 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3875779695808887 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08465000428259373 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17407000996172428 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0368200708180666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1121601089835167 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.019491141662002 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09554997086524963 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25895005092024803 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09174901060760021 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12638000771403313 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35586999729275703 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07348880171775818 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.48144906759262085 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.256288917735219 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03460003063082695 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.656298017129302 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1653700601309538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2786288969218731 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13227993622422218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039830105379223824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06984011270105839 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08402904495596886 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17143902368843555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06644986569881439 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08288002572953701 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039010075852274895 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.209137961268425 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08415011689066887 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17381878569722176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036689918488264084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0922901090234518 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.910742001608014 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11256011202931404 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2744700759649277 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153014980256557 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12612994760274887 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.354039017111063 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06921985186636448 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.473228981718421 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.151969147846103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03771018236875534 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.561578087508678 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12140884064137936 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23613893426954746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13136002235114574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040120212361216545 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06783893331885338 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08381996303796768 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17018988728523254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159836038947105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0827200710773468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038828933611512184 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1451169848442078 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08394010365009308 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17322995699942112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037190038710832596 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09243004024028778 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.980443114414811 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09250896982848644 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.28549996204674244 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10503898374736309 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12779003009200096 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35576988011598587 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06794906221330166 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4712888039648533 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.208679009228945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034800032153725624 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.604998907074332 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.79731714911759 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08627003990113735 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02756994217634201 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.22354908287525177 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 84.17119807563722 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.21921913139522076 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.07427996024489403 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.05049002356827259 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025559915229678154 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03757001832127571 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.28497003950178623 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07727881893515587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07101008668541908 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602198462933302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05850987508893013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08779997006058693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04209904000163078 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06842007860541344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08457992225885391 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17193006351590157 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047590117901563644 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015730038285255432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08239923045039177 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9736979845911264 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06637000478804111 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16075000166893005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042869942262768745 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1015891321003437 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.949682926759124 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09895907714962959 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2604199107736349 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030988439917564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266400795429945 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35457988269627094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06792997010052204 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.469869002699852 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.154309095814824 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03447011113166809 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.568589083850384 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08452008478343487 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17289002425968647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048479996621608734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08144881576299667 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04020007327198982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06751995533704758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09056017734110355 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19028992392122746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057169003412127495 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01543993130326271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0828700140118599 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9777480736374855 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08330005221068859 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17095007933676243 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03672996535897255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09339000098407269 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.093762021511793 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09290897287428379 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2563800662755966 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09137904271483421 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12513995170593262 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35165995359420776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765918806195259 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4655690863728523 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.273549119010568 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03495998680591583 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.669117905199528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07800990715622902 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10495982132852077 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.401242904365063 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08528889156877995 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1767389476299286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08103996515274048 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939983434975147 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06748992018401623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07318914867937565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592689659446478 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04977989010512829 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13048993423581123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03163004294037819 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.036378089338541 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07308903150260448 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15968899242579937 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042759813368320465 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072991088032722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3579889889806509 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904398038983345 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25090877898037434 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10314001701772213 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24462002329528332 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0880388543009758 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37928903475403786 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.440997002646327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0399099662899971 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8333359621465206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487018592655659 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16240007244050503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04695006646215916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12972881086170673 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03948016092181206 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06665010005235672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07254001684486866 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15710992738604546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04639988765120506 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13314001262187958 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0144880507141352 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07248017936944962 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17646001651883125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042899977415800095 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09271013550460339 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3664290998131037 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08994992822408676 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24845008738338947 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09070895612239838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10322011075913906 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2446700818836689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08933013305068016 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37970999255776405 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4470871537923813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04378007724881172 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8663560040295124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13186992146074772 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16224989667534828 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.0224279295653105 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07554912008345127 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16586901620030403 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048209913074970245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1302901655435562 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04011997953057289 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06984011270105839 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16148993745446205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047999899834394455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01546996645629406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2305989619344473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037400051951408386 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1851671151816845 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15962007455527782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05261995829641819 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.091188820078969 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19468995742499828 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071989916265011 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26461901143193245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122001938521862 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10294909588992596 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20711892284452915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13209017924964428 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3862089943140745 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2965269852429628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05073007196187973 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.709846779704094 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07497984915971756 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16298983246088028 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2286890521645546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396000687032938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06899004802107811 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422012276947498 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16189995221793652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04878011532127857 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302390057593584 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2239969801157713 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724999699741602 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15952996909618378 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05368911661207676 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09235995821654797 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19613001495599747 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08997879922389984 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25442009791731834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09117997251451015 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10379008017480373 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20879018120467663 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13283989392220974 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3881698939949274 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2921781744807959 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05124881863594055 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7072060145437717 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22660987451672554 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25400007143616676 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.18434720672667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07674912922084332 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1688490156084299 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04903995431959629 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2293698489665985 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.043770065531134605 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06864895112812519 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423991337418556 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613099593669176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04801992326974869 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23031886667013168 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2334671337157488 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07303990423679352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610799226909876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047830166295170784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22908905521035194 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039529986679553986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06753997877240181 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436005398631096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16068993136286736 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725879989564419 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015039928257465363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22952002473175526 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.213278155773878 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.491314895451069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08745910599827766 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1865290105342865 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048830173909664154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22871000692248344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765895523130894 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07474003359675407 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1606801524758339 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0481500755995512 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2301391214132309 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2432069052010775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437006570398808 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17913011834025383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05379016511142254 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205914102494717 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18190010450780392 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045004844665527 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2578690182417631 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106006473302841 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09606010280549526 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2005789428949356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13229995965957642 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3807989414781332 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2770970351994038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05129002965986729 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7119969706982374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16259890981018543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854006692767143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2285900991410017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03980891779065132 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06823008880019188 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360987365245819 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15926011838018894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731002263724804 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015540048480033875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23049907758831978 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2184970546513796 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.279860993847251 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16267900355160236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42662909254431725 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04185992293059826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06807013414800167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07986999116837978 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677200198173523 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048369867727160454 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23058010265231133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06933999247848988 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5177770983427763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301894947886467 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16042892821133137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.055709853768348694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4258090630173683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06763008423149586 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07375981658697128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15908991917967796 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048298854380846024 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22993003949522972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0696300994604826 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5062070451676846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348903454840183 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610389444977045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04757009446620941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4263389855623245 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06798980757594109 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07401010952889919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16064010560512543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04698894917964935 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015249941498041153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22996985353529453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06957980804145336 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4987171161919832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22703921422362328 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3075490240007639 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.987569991499186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459009066224098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177003271877766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42356899939477444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039529986679553986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06762007251381874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07251999340951443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15807012096047401 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04756986163556576 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22866996005177498 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06902008317410946 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4904371928423643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730389729142189 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16411906108260155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05352986045181751 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09172013960778713 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19301916472613811 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935993537306786 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2534599043428898 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09041908197104931 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10224990546703339 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20721997134387493 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13164011761546135 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3849188797175884 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2791468761861324 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05045998841524124 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6962361987680197 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384992204606533 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16105989925563335 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047339824959635735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42320904321968555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053409021347761154 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06837002001702785 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15934999100863934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047369860112667084 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0153400469571352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22926903329789639 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06939982995390892 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5108969528228045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07276004180312157 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16031903214752674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0528399832546711 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09211990982294083 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1927900593727827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927914313971996 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2541998401284218 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104912169277668 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1020398922264576 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20746001973748207 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.130870146676898 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38299988955259323 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.279307994991541 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050320057198405266 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6895560547709465 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07533002644777298 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16321009024977684 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04858989268541336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32524880953133106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03993022255599499 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06701983511447906 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07328903302550316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15816884115338326 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047460198402404785 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22805994376540184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05835900083184242 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3846971560269594 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08100992999970913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16774982213974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0527198426425457 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09138020686805248 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19414001144468784 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09000999853014946 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25228923186659813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037996642291546 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1014599110931158 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20548980683088303 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13160915113985538 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38242898881435394 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2778069358319044 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05037989467382431 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6971470322459936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22960896603763103 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3150389529764652 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.0058289244771 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13006990775465965 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22022007033228874 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05303998477756977 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32583996653556824 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039689941331744194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06810994818806648 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15977001748979092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13113999739289284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05946005694568157 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.355946995317936 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07239985279738903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907990746200085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04201987758278847 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09124982170760632 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.367569038644433 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0947299413383007 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25958893820643425 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090740155428648 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10294909588992596 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2438488882035017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0877799466252327 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3764592111110687 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4636069536209106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03972998820245266 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8516969867050648 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09068986400961876 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17733918502926826 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046999892219901085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22734003141522408 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03962009213864803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06680004298686981 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07223011925816536 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1553799957036972 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04585995338857174 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014929799363017082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1304189208894968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04759989678859711 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1792168952524662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0721300020813942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15665986575186253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04197005182504654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897599384188652 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36502908915281296 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.087349908426404 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24462002329528332 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08876901119947433 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10163988918066025 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2422700636088848 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0879000872373581 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37450995296239853 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.421918161213398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03920006565749645 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8031660001724958 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07980014197528362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667600590735674 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477498397231102 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17847889102995396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038560014218091965 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06634020246565342 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07291999645531178 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15652994625270367 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04594004712998867 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014949822798371315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13023009523749352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04237983375787735 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1166678741574287 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07229018956422806 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15743891708552837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04115002229809761 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08979998528957367 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3643091768026352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08826004341244698 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2455189824104309 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08897995576262474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10171998292207718 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24133012630045414 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08726888336241245 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37222891114652157 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.422746805474162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038980040699243546 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8038959242403507 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23802905343472958 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35508908331394196 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.675279958173633 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2203888725489378 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3736990038305521 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10242988355457783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18020998686552048 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869994543492794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06662006489932537 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08533988147974014 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17061992548406124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04642992280423641 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014449935406446457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08189002983272076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04293886013329029 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3426970690488815 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0838001724332571 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16830000095069408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03621005453169346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0910500530153513 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.0005219634622335 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09147007949650288 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2559199929237366 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08946890011429787 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12495997361838818 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.350399874150753 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06637885235249996 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46083913184702396 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.163848865777254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03356998786330223 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.5456990376114845 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16348017379641533 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2747990656644106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07342989556491375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13057002797722816 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03843987360596657 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06579002365469933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0827200710773468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16644899733364582 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046209897845983505 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08175009861588478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03769015893340111 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1554581578820944 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08413009345531464 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16965903341770172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03625988028943539 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09044003672897816 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.919241949915886 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09119999594986439 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25786878541111946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09336997754871845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1253399532288313 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3506690263748169 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06704987026751041 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46167895197868347 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.086909979581833 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03403984010219574 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.470439093187451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12262002564966679 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23394008167088032 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07390999235212803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13024010695517063 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03836001269519329 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06628991104662418 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08358992636203766 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16832002438604832 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04620011895895004 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014819903299212456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0818988773971796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038350000977516174 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1167170014232397 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08440995588898659 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16941013745963573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03628013655543327 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09033898822963238 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9179520681500435 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09008008055388927 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2580690197646618 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913013152778149 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12514996342360973 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3505290951579809 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06713997572660446 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4617590457201004 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.079618887975812 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038899946957826614 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.4703690111637115 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.322639029473066 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08710892871022224 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02541998401284218 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11881999671459198 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 78.62434908747673 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.2328590489923954 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.048659974709153175 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04336005076766014 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025640008971095085 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03684009425342083 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24801003746688366 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07063010707497597 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06624986417591572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15278998762369156 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866020753979683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08256989531219006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03947014920413494 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06764009594917297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08473987691104412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17121993005275726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785996861755848 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01537008211016655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08269981481134892 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9313190821558237 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06692903116345406 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15315902419388294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03672996535897255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09353994391858578 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9641819894313812 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09834906086325645 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26489002630114555 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09594904258847237 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1272200606763363 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35278010182082653 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06745895370841026 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46625896356999874 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.184008972719312 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03472995012998581 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.584327969700098 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08415011689066887 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17009000293910503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04682992585003376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08086883462965488 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869016654789448 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0663301907479763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0844199676066637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16959011554718018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0459989532828331 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015059951692819595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08216011337935925 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9247679263353348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08273008279502392 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16644014976918697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03653997555375099 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09148009121417999 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.0717218071222305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09090988896787167 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25547901168465614 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08885003626346588 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12457999400794506 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3500690218061209 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06691995076835155 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.460529001429677 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.232929019257426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033670105040073395 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.614308873191476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07786904461681843 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10303920134902 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.258352177217603 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467996329069138 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16289995983242989 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04751002416014671 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08009001612663269 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03823009319603443 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06706919521093369 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258006371557713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1568901352584362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04570977762341499 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014400109648704529 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13036001473665237 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031039118766784668 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0008378885686398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724398996680975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16439985483884811 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04239007830619812 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08946005254983902 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3574490547180176 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0889201182872057 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2450789324939251 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08907006122171879 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10235910303890705 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24168891832232475 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08754990994930267 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37291902117431164 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4167570043355227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03915000706911087 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.807316904887557 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15842914581298828 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04597986117005348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1292899250984192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0384598970413208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0663988757878542 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07216003723442554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15480001457035542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04588998854160309 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01443992368876934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13015000149607658 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0033869184553623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07114000618457794 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15579001046717167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041190069168806076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968985639512539 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36800908856093884 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08760998025536537 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24800910614430904 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09567011147737503 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10347901843488216 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24370895698666573 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08970010094344616 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37741917185485363 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.440676860511303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03929995000362396 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8215370364487171 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12873904779553413 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15368894673883915 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.885448073968291 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0737698283046484 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16214000061154366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046380097046494484 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13045012019574642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03829016350209713 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06825011223554611 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0727199949324131 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15647010877728462 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04592002369463444 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014730030670762062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23132888600230217 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03635999746620655 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1550378985702991 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07257983088493347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15650014393031597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05307886749505997 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08930009789764881 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.195350032299757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08733011782169342 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25290902704000473 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08818996138870716 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10255980305373669 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20580901764333248 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1324201002717018 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3825589083135128 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2660380452871323 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05096010863780975 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6721270512789488 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325899787247181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15901890583336353 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046999892219901085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23003993555903435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040640123188495636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06880913861095905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07313000969588757 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1574200578033924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04634005017578602 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014429911971092224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2308790571987629 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.206347020342946 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07216981612145901 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1554097980260849 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053369905799627304 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09062886238098145 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19370997324585915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08773989975452423 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25131902657449245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08919998072087765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10280916467308998 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20465906709432602 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13154977932572365 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37953886203467846 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2674471363425255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05113007500767708 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6724171582609415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2268790267407894 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2519891131669283 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.055057980120182 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498008199036121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16381009481847286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04729907959699631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22958987392485142 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03796000964939594 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06803986616432667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16206922009587288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04688999615609646 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014420133084058762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23214006796479225 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2111971154808998 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07314002141356468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15721004456281662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0466399360448122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23049907758831978 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03802007995545864 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067099928855896 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07303990423679352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15760981477797031 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734005779027939 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014689983800053596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23106997832655907 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1987481266260147 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4514649994671345 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07261987775564194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15857885591685772 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04644016735255718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2297000028192997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03791996277868748 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06697885692119598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07289997301995754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15713018365204334 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04664994776248932 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014350051060318947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23107905872166157 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1962468270212412 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07613003253936768 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645099837332964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0530898105353117 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032920934259892 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18317997455596924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08832011371850967 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25243894197046757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909986354410648 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09605009108781815 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19846903160214424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1348499208688736 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3915191628038883 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.26917683519423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050430186092853546 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.682966947555542 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396889850497246 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159428920596838 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04631001502275467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2294799778610468 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842986188828945 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0672689639031887 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07207994349300861 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.155959976837039 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046590110287070274 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014970079064369202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23370911367237568 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2139270547777414 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.198160953819752 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07559987716376781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16491999849677086 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0467600766569376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42995880357921124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03839004784822464 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06795884110033512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07286015897989273 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15759002417325974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046590110287070274 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01458008773624897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23160921409726143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06886990740895271 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4937769155949354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07204990833997726 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1557201612740755 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04561012610793114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4323988687247038 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08016894571483135 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07443991489708424 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159319955855608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04666997119784355 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999881386756897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23137894459068775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06898981519043446 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5246968250721693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07164012640714645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15678000636398792 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04597008228302002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4287790507078171 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03797002136707306 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0669499859213829 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07193000055849552 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15513994731009007 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04624994471669197 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014320015907287598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2309391275048256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06878003478050232 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4770268462598324 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2282888162881136 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3084791824221611 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.970259964466095 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305992767214775 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15839003026485443 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04578894004225731 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4293189849704504 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03829994238913059 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0683697871863842 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258006371557713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15598000027239323 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04555005580186844 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014499993994832039 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2320590429008007 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07201987318694592 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4876171480864286 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258914411067963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15786895528435707 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05276990123093128 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09042001329362392 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19574002362787724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08744001388549805 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25230017490684986 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08908892050385475 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10269996710121632 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20599993877112865 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1319199800491333 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.382099999114871 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2687079142779112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050590140745043755 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6747461631894112 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333001121878624 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1590200699865818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046819914132356644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4297490231692791 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038640107959508896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06856909021735191 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07266993634402752 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15563005581498146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045720022171735764 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014789868146181107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23218919523060322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07013999857008457 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4878669753670692 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0717600341886282 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1566801220178604 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05283905193209648 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106006473302841 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19606994464993477 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10542920790612698 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2568999771028757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08919998072087765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10286900214850903 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2061990089714527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13232999481260777 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3819891717284918 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3186079449951649 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050729839131236076 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.723696943372488 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16171997413039207 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04663015715777874 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3301389515399933 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03846990875899792 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06722984835505486 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15764986164867878 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045689987018704414 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014449935406446457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23127999156713486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058859819546341896 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3775280676782131 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07109018042683601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1586289145052433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05263998173177242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09026983752846718 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1970399171113968 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08841999806463718 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25125988759100437 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08943001739680767 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10309997014701366 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2055999357253313 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13361009769141674 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38543902337551117 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2772579211741686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05083903670310974 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6863758210092783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2318299375474453 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3162398934364319 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.961380157619715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13165990822017193 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2190901432186365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05222996696829796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.33022905699908733 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03948016092181206 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789015606045723 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07081986404955387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1542989630252123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045800115913152695 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014749821275472641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13237004168331623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05988008342683315 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3442770577967167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07101893424987793 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15565892681479454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04181009717285633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08976995013654232 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3670589067041874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08825003169476986 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.244928989559412 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08914992213249207 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10349997319281101 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24381885305047035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08866982534527779 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3765891306102276 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.428517047315836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039590056985616684 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8111560493707657 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09296904318034649 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1941891387104988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05707982927560806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23179012350738049 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038689933717250824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742915138602257 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07312977686524391 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15591992996633053 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04564994014799595 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014780089259147644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13202009722590446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048279063776135445 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2178069446235895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07125991396605968 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15482003800570965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042109983041882515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09041000157594681 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.368678942322731 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08815014734864235 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.244559021666646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944002911448479 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10390998795628548 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24369917809963226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08844002149999142 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3755090292543173 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.428796909749508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03962009213864803 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8085071351379156 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07965881377458572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16678916290402412 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18172990530729294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03844010643661022 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06708991713821888 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0716999638825655 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15542004257440567 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671001806855202 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014849938452243805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1332701649516821 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04498986527323723 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1437968350946903 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07224990986287594 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15619001351296902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041899969801306725 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911988697946072 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3689590375870466 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0883401371538639 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24416903033852577 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08919998072087765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10358914732933044 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2436989452689886 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08809007704257965 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37475908175110817 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.427927054464817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03906991332769394 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8089069053530693 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24159997701644897 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35998900420963764 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.70359006896615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22348901256918907 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37645897828042507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1037099864333868 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18274900503456593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038850121200084686 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06747990846633911 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08505000732839108 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16961898654699326 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046819914132356644 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014669960364699364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08331984281539917 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043899985030293465 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3542571105062962 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08454895578324795 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17025903798639774 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04313001409173012 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218999184668064 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.008091986179352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904398038983345 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2572902012616396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895489938557148 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12761005200445652 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35404996015131474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06767990998923779 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4660699050873518 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.182678811252117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0342000275850296 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.573898088186979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16629998572170734 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27842004783451557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07431907579302788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1327598001807928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06688013672828674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0836800318211317 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16742898151278496 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046269968152046204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01471000723540783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0833999365568161 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037850113585591316 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1660379823297262 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08594011887907982 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1727500930428505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03651995211839676 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897599384188652 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9526920299977064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09053992107510567 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.257349107414484 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948007598519325 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12573902495205402 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3509591333568096 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712018512189388 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4625690635293722 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.116889951750636 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03448990173637867 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.510488990694284 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11906004510819912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2310501877218485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07397006265819073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13102893717586994 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03837002441287041 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06660004146397114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0833701342344284 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16733002848923206 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04672887735068798 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01416006125509739 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08224998600780964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03841007128357887 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.118508167564869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0838299747556448 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1678699627518654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03635999746620655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08877995423972607 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9269020780920982 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057018905878067 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25883899070322514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08785002864897251 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12638000771403313 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35225902684032917 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653997115790844 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46241888776421547 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.0888401456177235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03431015647947788 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.471549229696393 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.415857899934053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08811010047793388 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025090062990784645 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11938903480768204 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 78.1449198257178 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.18961983732879162 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04733004607260227 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04118005745112896 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024660024791955948 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03717001527547836 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2416290808469057 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07001007907092571 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06500002928078175 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15190010890364647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04800013266503811 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08256896398961544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039880163967609406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06771995685994625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08509005419909954 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17105997540056705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705996252596378 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08357991464436054 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9306578431278467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06165984086692333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14678994193673134 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036659883335232735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09314902126789093 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9751920849084854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09280000813305378 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26128883473575115 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072991088032722 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12649991549551487 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35298895090818405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06678001955151558 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46518887393176556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.15659898519516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03434019163250923 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.537058925256133 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08409004658460617 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17212005332112312 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048039015382528305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08162995800375938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039499951526522636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06633996963500977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08546002209186554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17101014964282513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047239940613508224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0823198352009058 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9382080752402544 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08329981938004494 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16964995302259922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03713020123541355 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149988181889057 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.920882008969784 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10594003833830357 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26079919189214706 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09141000919044018 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12637884356081486 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3537992015480995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06718002259731293 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4661991260945797 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.118879955261946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03445008769631386 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.506488960236311 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07829000242054462 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10428018867969513 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.118043076246977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16702990978956223 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08049909956753254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04111998714506626 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06710016168653965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07318984717130661 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15863985754549503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047479989007115364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015170080587267876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.131240114569664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031030038371682167 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0214580688625574 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07295003160834312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15819002874195576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041869934648275375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09119999594986439 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3602788783609867 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08997996337711811 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25034998543560505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030918590724468 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10207993909716606 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24325004778802395 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08834991604089737 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3840089775621891 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4458668883889914 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04008994437754154 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8346658907830715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739500392228365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039004549384117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.130578875541687 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03945990465581417 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728013977408409 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386994548141956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159720191732049 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04618009552359581 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1317600253969431 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0133979376405478 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07195002399384975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15782006084918976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04221987910568714 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09156903252005577 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36734994500875473 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09035901166498661 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24746987037360668 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09109987877309322 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10331906378269196 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24384912103414536 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08868006989359856 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37740892730653286 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4432568568736315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0399700365960598 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8311368767172098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12800004333257675 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1539990771561861 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.955457920208573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502990774810314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16521010547876358 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12996909208595753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042987711727619 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06808992475271225 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342989556491375 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17214007675647736 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049759168177843094 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22994005121290684 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036499928683042526 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1885880958288908 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07329881191253662 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15943916514515877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052930088713765144 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09118998423218727 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19301893189549446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006005711853504 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25359983555972576 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09113899432122707 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1020701602101326 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20631984807550907 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13218005187809467 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38418895564973354 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2789270840585232 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05073007196187973 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6913560684770346 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387995719909668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1606498844921589 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22823899053037167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950018435716629 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06772996857762337 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07322896271944046 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15955884009599686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22985017858445644 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2120080646127462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07276004180312157 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15919003635644913 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05299015901982784 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09647989645600319 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19655912183225155 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08989986963570118 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25372905656695366 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032012894749641 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10409997776150703 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20811008289456367 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13317912817001343 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3866390325129032 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.291607040911913 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051059992983937263 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.702945912256837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2278890460729599 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2575591206550598 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.153336958959699 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076299998909235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16721989959478378 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049010151997208595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2299589104950428 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04003988578915596 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06826990284025669 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385015487670898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596999354660511 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048620160669088364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2311798743903637 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2241580989211798 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301894947886467 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15904894098639488 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048740068450570107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22980989888310432 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039659906178712845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06783916614949703 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07336004637181759 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159549992531538 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785018973052502 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159836038947105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23139896802604198 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2128469534218311 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.480064984411001 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08084997534751892 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16895984299480915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23044017143547535 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039499951526522636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06762007251381874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0737789086997509 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593688502907753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23117009550333023 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2248679995536804 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360009476542473 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16274000518023968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052900053560733795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0922901090234518 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18297904171049595 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09124004282057285 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2567300107330084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0911499373614788 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0964500941336155 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2004699781537056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13320986181497574 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37972000427544117 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.271597109735012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05073007196187973 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6870161052793264 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456005550920963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177003271877766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048460206016898155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23068906739354134 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975001163780689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760982796549797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07303990423679352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15759002417325974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04783901385962963 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01535983756184578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23240013979375362 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2196481693536043 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.229082027450204 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402896881103516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16217888332903385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047479989007115364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4297490231692791 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04041008651256561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06814999505877495 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07347995415329933 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16996893100440502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057660043239593506 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23173983208835125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06943894550204277 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5259759966284037 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338984869420528 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15968992374837399 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779989831149578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42897905223071575 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04002009518444538 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06732996553182602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298984564840794 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15898002311587334 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23136893287301064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06952998228371143 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5013869851827621 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298006676137447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15841005370020866 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04685018211603165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42941910214722157 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040220096707344055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0675390474498272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07281010039150715 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158810056746006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794006235897541 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23186905309557915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06925989873707294 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4977669343352318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22790906950831413 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30825892463326454 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.99030901119113 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456005550920963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16171997413039207 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42928894981741905 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975001163780689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15941890887916088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0467000063508749 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23155007511377335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06986898370087147 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5007071197032928 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07175002247095108 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15668990090489388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053159892559051514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065004996955395 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19588996656239033 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08838996291160583 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2519588451832533 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08876994252204895 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10284013114869595 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20681996829807758 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13172905892133713 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38273888640105724 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2693169992417097 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050209928303956985 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.674146857112646 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07361010648310184 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15940889716148376 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04588998854160309 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4257990512996912 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03848993219435215 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06724987179040909 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07319008000195026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15740003436803818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04587997682392597 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014599878340959549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22983993403613567 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06995000876486301 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4797172043472528 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07155886851251125 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15578907914459705 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05270005203783512 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09111990220844746 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19447901286184788 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08786004036664963 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.253880163654685 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0885289628058672 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10145013220608234 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2039298415184021 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13125990517437458 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.378140015527606 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2726569548249245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05030003376305103 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6746658366173506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07322989404201508 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15901005826890469 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04666997119784355 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3288690932095051 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040800077840685844 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06718002259731293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320987060666084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575700007379055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04655006341636181 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014749821275472641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298089675605297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058759935200214386 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3780270237475634 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07246993482112885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1578500960022211 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05242996849119663 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09059906005859375 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1942699309438467 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08847005665302277 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2512088976800442 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927984163165092 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10224012657999992 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20485883578658104 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13240007683634758 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38076890632510185 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2667770497500896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05079992115497589 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6726769972592592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302199136465788 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31393906101584435 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.900500066578388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13003917410969734 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21717906929552555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05191005766391754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32766908407211304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396000687032938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759981624782085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275003008544445 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15696999616920948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04546996206045151 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014770077541470528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13156910426914692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05858996883034706 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.345637021586299 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07120007649064064 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15574018470942974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043140025809407234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895101111382246 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36792014725506306 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0887101050466299 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24377997033298016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08726981468498707 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10241009294986725 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24199998006224632 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0879000872373581 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37269899621605873 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4203579630702734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039179809391498566 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.804425846785307 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08995016105473042 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17638993449509144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0463901087641716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22889883257448673 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899946957826614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0667900312691927 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0720098614692688 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15514995902776718 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04603993147611618 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479010097682476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13075885362923145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04803994670510292 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1779479682445526 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0721300020813942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16062986105680466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041850144043564796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08959905244410038 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37056999281048775 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08973991498351097 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2448300365358591 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08932896889746189 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10344991460442543 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24279998615384102 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08831988088786602 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.374919967725873 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4309370890259743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03939005546271801 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8167358357459307 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07999013178050518 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664001028984785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049480004236102104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1797389704734087 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899946957826614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06633996963500977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07250998169183731 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15655998140573502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04677893593907356 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014599878340959549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13135001063346863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041869934648275375 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1185880284756422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07025990635156631 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1541399396955967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04150997847318649 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895101111382246 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3673990722745657 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.087750144302845 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24515902623534203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08878996595740318 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10288017801940441 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25071995332837105 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08805911056697369 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3824189770966768 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4367969706654549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03912998363375664 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8143360503017902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2411890309303999 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35869888961315155 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.641499957069755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.220548827201128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37282914854586124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10371999815106392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18068007193505764 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03913906402885914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06704987026751041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08486979641020298 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16959011554718018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04685996100306511 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01445016823709011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08220900781452656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042659929022192955 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3445368967950344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08362997323274612 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16854004934430122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03642984665930271 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09123887866735458 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.941071918234229 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901299063116312 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25577982887625694 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08882908150553703 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1256200484931469 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3507200162857771 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712996400892735 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4612100310623646 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.098738940432668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03368011675775051 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.480129038915038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16261986456811428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27377018705010414 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07487903349101543 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1310801599174738 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06637000478804111 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0857191625982523 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16950909048318863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04655006341636181 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01471000723540783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08267001248896122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037590041756629944 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1613278184086084 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08351891301572323 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16845902428030968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03672996535897255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09137997403740883 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.971341997385025 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021908044815063 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2574098762124777 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912989869713783 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12548919767141342 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3511989489197731 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06713997572660446 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4624191205948591 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.133419996127486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0342000275850296 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.516038043424487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11983001604676247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23178011178970337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07355003617703915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13059889897704124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039120204746723175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06609992124140263 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08411984890699387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16875006258487701 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046220142394304276 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08239899761974812 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03824010491371155 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1172180529683828 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08393987081944942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16828998923301697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03615999594330788 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10350882075726986 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.915972076356411 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09860005229711533 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.262698857113719 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09014992974698544 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12675998732447624 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35282899625599384 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670100562274456 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4634088836610317 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.107629112899303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03362004645168781 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.488109076395631 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.328719168901443 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08957902900874615 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02492009662091732 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11730007827281952 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 77.96742091886699 ms for forwarding
--------------------
No. 3
<class 'diffusers.models.embeddings.Timesteps'> take 0.24536997079849243 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05718995817005634 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.047659967094659805 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.037400051951408386 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04051998257637024 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.28929999098181725 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1491790171712637 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0927201472222805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2018599770963192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05268002860248089 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.10566902346909046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04251999780535698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07847999222576618 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09079999290406704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19560987129807472 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05919020622968674 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.020239967852830887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08682999759912491 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.106017967686057 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506995461881161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16580009832978249 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09925989434123039 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.080340964719653 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.12341979891061783 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3025687765330076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09705009870231152 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1330999657511711 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3751490730792284 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07193977944552898 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5013491027057171 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.423689028248191 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03669015131890774 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.844217957928777 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08975900709629059 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18635904416441917 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05180994048714638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08597015403211117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04181009717285633 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07055001333355904 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0902889296412468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18183887004852295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014629913493990898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08679996244609356 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.998638104647398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08749915286898613 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1764688640832901 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038449885323643684 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09462004527449608 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.133631009608507 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0938300509005785 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2574799582362175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09117880836129189 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1272300723940134 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3563500940799713 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06897910498082638 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4726389888674021 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.3244889713823795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03524008207023144 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.7304780930280685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0833999365568161 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11577014811336994 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.921411940827966 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09054993279278278 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18479907885193825 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04695006646215916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08239992894232273 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03824010491371155 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07293000817298889 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07297005504369736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15958002768456936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04645995795726776 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01819990575313568 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13731000944972038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03211991861462593 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0631869081407785 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07178983651101589 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15819002874195576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043839914724230766 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0908698420971632 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3621301148086786 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953898213803768 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.250119948759675 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089969951659441 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10379892773926258 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24712900631129742 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08952012285590172 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38353889249265194 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.446967013180256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040640123188495636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8401260022073984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736401416361332 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16073998995125294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04639988765120506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.133089954033494 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038259197026491165 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06793998181819916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07279007695615292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15814020298421383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046479981392621994 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015219906345009804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.137699069455266 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0269880294799805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0717600341886282 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16666995361447334 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04357006400823593 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09175902232527733 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37115998566150665 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10718894191086292 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25134021416306496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911010809242725 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10404898785054684 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24634902365505695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0896600540727377 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38324901834130287 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4825169928371906 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041800085455179214 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8917170818895102 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12865010648965836 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15521002933382988 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.089227041229606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07452000863850117 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1650000922381878 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047209905460476875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13172882609069347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037899939343333244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07298006676137447 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734599307179451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15974999405443668 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045909080654382706 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23278011940419674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03797002136707306 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1787079274654388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07267901673913002 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596389338374138 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05319993942975998 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19504991360008717 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08808006532490253 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2770000137388706 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09274901822209358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10382011532783508 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21100998856127262 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13407994993031025 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39375992491841316 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.319246832281351 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05158013664186001 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.737426035106182 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07529999129474163 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16434001736342907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04897988401353359 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23120897822082043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06991997361183167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0784390140324831 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16589905135333538 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04897988401353359 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23206998594105244 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2393081560730934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724398996680975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15968992374837399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05374988541007042 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09301002137362957 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1951188314706087 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08974992670118809 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2546191681176424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09073992259800434 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10362989269196987 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20991009660065174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13363896869122982 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3951189573854208 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3073468580842018 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05485001020133495 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7379459459334612 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22868881933391094 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25680894032120705 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.257476983591914 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07614004425704479 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16917986795306206 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2316588070243597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047289999201893806 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06972020491957664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07575005292892456 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16443990170955658 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913005977869034 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23254984989762306 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.255198149010539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07318006828427315 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16150902956724167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04800991155207157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23071980103850365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906991332769394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06808014586567879 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16076001338660717 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04835985600948334 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558009535074234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23197894915938377 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.225107116624713 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.526335185393691 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643202267587185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2311889547854662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039689941331744194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0689199659973383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07401988841593266 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1659600529819727 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05736900493502617 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01838989555835724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2366399858146906 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.263157930225134 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07512909360229969 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16679917462170124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054969917982816696 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09329011663794518 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1832291018217802 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09130011312663555 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26080990210175514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09165005758404732 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09647989645600319 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20244019106030464 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1326089259237051 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3841291181743145 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.295297173783183 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05492009222507477 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.742485910654068 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757100060582161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1652201171964407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04943902604281902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298599574714899 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040079932659864426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06823986768722534 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0863298773765564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1841199118643999 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05456991493701935 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015759840607643127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23030908778309822 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2634568847715855 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.384021041914821 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437984459102154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16342010349035263 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4275490064173937 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05342904478311539 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07102987729012966 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07336982525885105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16205990687012672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048419926315546036 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015560071915388107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23103877902030945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07010996341705322 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5318270307034254 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414002902805805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16262894496321678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4271289799362421 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03890017978847027 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06910017691552639 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08826004341244698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1883399672806263 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23010000586509705 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07002009078860283 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5471670776605606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07313000969588757 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16095000319182873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4272689111530781 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03882008604705334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06758002564311028 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294910028576851 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596191432327032 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048170099034905434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015430152416229248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2301300410181284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06978888995945454 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5023171436041594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22767996415495872 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3286600112915039 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.074699874967337 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483991794288158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1639300025999546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0482101459056139 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4273788072168827 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039929989725351334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07012998685240746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07332907989621162 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1606391742825508 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23155007511377335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07342011667788029 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5269969590008259 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730899628251791 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16140006482601166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05421997047960758 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09277998469769955 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19446900114417076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09388988837599754 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2831891179084778 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09064003825187683 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1034298911690712 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20833010785281658 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13270904310047626 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3895689733326435 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3303670566529036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0519799068570137 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7506468575447798 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07540988735854626 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637397799640894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047859037294983864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.427519204095006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04109996370971203 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06908015348017216 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07807998917996883 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16569998115301132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04721991717815399 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015350058674812317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23055006749927998 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0707400031387806 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5180970076471567 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305899634957314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161309028044343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05373009480535984 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09269989095628262 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20326906815171242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08839997462928295 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2540098503232002 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090339919552207 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10321987792849541 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20996993407607079 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1385090872645378 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39910897612571716 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3209169264882803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051339855417609215 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7385159153491259 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07554003968834877 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643600407987833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04814891144633293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3275698982179165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395399983972311 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07072906009852886 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16146991401910782 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05868007428944111 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302289940416813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05925004370510578 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4167169574648142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731500331312418 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16110995784401894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05405908450484276 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09374995715916157 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1952601596713066 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08961907587945461 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25560008361935616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09023002348840237 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10828999802470207 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.23530004546046257 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13366015627980232 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4198190290480852 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3270680792629719 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05113007500767708 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.745786052197218 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23179990239441395 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3172599244862795 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.235129157081246 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1368899829685688 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2344599924981594 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05280994810163975 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3285389393568039 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04050997085869312 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06888993084430695 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07308879867196083 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594889909029007 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737009294331074 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0157100148499012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13255001977086067 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06050989031791687 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3949170242995024 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07263012230396271 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16001006588339806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04272023215889931 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09188009425997734 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36811898462474346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09003002196550369 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25007897056639194 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030988439917564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1039199996739626 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2461387775838375 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09494996629655361 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3930788952857256 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4734470751136541 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04336005076766014 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8824769649654627 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09276904165744781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18228893168270588 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049190130084753036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22952002473175526 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039830105379223824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06837910041213036 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0875699333846569 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18943008035421371 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705996252596378 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13261893764138222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04866998642683029 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2555071152746677 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724999699741602 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16059004701673985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0428101047873497 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0918989535421133 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37565012462437153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09655882604420185 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25143008679151535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065982885658741 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1042298972606659 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2464300487190485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08863001130521297 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38380990736186504 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.483087195083499 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040529994294047356 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8797360826283693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08014007471501827 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17039990052580833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18039904534816742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0400301069021225 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06764987483620644 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07322011515498161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.160659896209836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0474490225315094 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13157003559172153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04300009459257126 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1453379411250353 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07224990986287594 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159829156473279 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04233000800013542 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1058399211615324 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3690989688038826 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015994146466255 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2501890994608402 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09103002957999706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10314001701772213 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2455189824104309 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08933991193771362 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38323900662362576 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4759269542992115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040839891880750656 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8697758205235004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24055899120867252 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3599789924919605 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.007259901612997 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22158888168632984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3756689839065075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10379008017480373 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18136994913220406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039718812331557274 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06918981671333313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09230989962816238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19100005738437176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05692988634109497 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01735985279083252 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08322903886437416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044040149077773094 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.408417010679841 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08494989015161991 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1733400858938694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03711995668709278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09364890865981579 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.00112196803093 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09787990711629391 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2871390897780657 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09156996384263039 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12582982890307903 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35416893661022186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680598895996809 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47042896039783955 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.22909895516932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03527989611029625 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.624049110338092 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16422010958194733 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29107904992997646 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07458985783159733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1324291806668043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03995979204773903 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06743008270859718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08489005267620087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1838000025600195 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.056989025324583054 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01856987364590168 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08718995377421379 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038430094718933105 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.236097188666463 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08440017700195312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17417012713849545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03714999184012413 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09147985838353634 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.928821999579668 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09271991439163685 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25963899679481983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252992458641529 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12663984671235085 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35552913323044777 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681199599057436 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47147879377007484 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.119819892570376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03438000567257404 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.515398923307657 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1279090065509081 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24601910263299942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07501011714339256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13215001672506332 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039689941331744194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06718980148434639 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.083989929407835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17047999426722527 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04694005474448204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08376012556254864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190020263195038 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.169636845588684 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08390005677938461 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18249009735882282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03733998164534569 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09322888217866421 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9932020008563995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0940198078751564 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2620290033519268 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11044996790587902 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.131119042634964 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3629988059401512 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06824987940490246 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4813589621335268 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.227688932791352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034659868106245995 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.635888082906604 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.821936894208193 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08611008524894714 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02608983777463436 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.20073004998266697 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 80.80495498143137 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19364990293979645 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05219015292823315 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04361988976597786 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025869812816381454 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03708992153406143 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25398912839591503 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0726599246263504 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06982008926570415 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15681004151701927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04888908006250858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08279993198812008 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040830112993717194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684000551700592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09059999138116837 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1933190505951643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053590163588523865 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015399884432554245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08349004201591015 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9761080145835876 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06464007310569286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15222979709506035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03699888475239277 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09388010948896408 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.968941979110241 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09240000508725643 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2599491272121668 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153992868959904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12675998732447624 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3537689335644245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06684986874461174 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4664389416575432 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.152560072019696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03445986658334732 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.539928795769811 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09110895916819572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1968990545719862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854006692767143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08164998143911362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04029995761811733 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0667001586407423 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08520879782736301 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1707791816443205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047919806092977524 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08290004916489124 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9717380162328482 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08892896585166454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18751900643110275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03643985837697983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09280000813305378 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.110112087801099 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252899326384068 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2570000942796469 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12725894339382648 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3562790807336569 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06892019882798195 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4724489990621805 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.2920200396329165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03411993384361267 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.703888135030866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.079659977927804 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10576006025075912 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.406903017312288 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07770000956952572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1813701819628477 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057608820497989655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08477014489471912 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398498959839344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757979281246662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07378985174000263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589090097695589 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04721013829112053 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015689991414546967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1324100885540247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031360192224383354 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0564879048615694 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0752890482544899 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633788924664259 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04215002991259098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09146006777882576 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3608290571719408 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09007006883621216 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2483490388840437 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09050988592207432 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10394002310931683 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24392898194491863 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08872011676430702 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3777989186346531 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4357471372932196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03969017416238785 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8278760835528374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414002902805805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16174884513020515 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047789886593818665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13148016296327114 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03968016244471073 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06704987026751041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742089468985796 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593390479683876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047079985961318016 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1329400110989809 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0270080529153347 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07197889499366283 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15703891403973103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04215002991259098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09154994040727615 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36925915628671646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08890009485185146 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24944893084466457 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09027007035911083 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10362011380493641 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24462887085974216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08909008465707302 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3794590011239052 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4468871522694826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03967992961406708 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8333059269934893 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12851902283728123 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15483913011848927 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.000877125188708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07559987716376781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17449003644287586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057930126786231995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13465899974107742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03976002335548401 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06867991760373116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392000406980515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15980005264282227 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797987639904022 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01566903665661812 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23219012655317783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039800070226192474 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2221180368214846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08171889930963516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17471914179623127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05421019159257412 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0927499495446682 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19425898790359497 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010010398924351 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2533900551497936 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071896784007549 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1029099803417921 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20857993513345718 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13354001566767693 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38804998621344566 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2849480845034122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05085999146103859 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7245961353182793 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459009066224098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16292999498546124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22998894564807415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03942986950278282 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06756000220775604 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07299007847905159 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907012857496738 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048350077122449875 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23122993297874928 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.218188088387251 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07264991290867329 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158810056746006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05388888530433178 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09249988943338394 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19342009909451008 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968915790319443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2587700728327036 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970987983047962 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10464899241924286 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21177902817726135 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13403990305960178 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39129890501499176 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.293158158659935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051239971071481705 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7050260212272406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2275700680911541 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2533600199967623 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.226568017154932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07608998566865921 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16632885672152042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0492599792778492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23041991516947746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039190053939819336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06772903725504875 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472001016139984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604398712515831 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04858989268541336 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015350058674812317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23197894915938377 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2232169974595308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07159006781876087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15625008381903172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047729816287755966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23002899251878262 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03908993676304817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06711995229125023 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07311999797821045 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1632289495319128 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477498397231102 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23114983923733234 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.210987800732255 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.477664966136217 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07340009324252605 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16151904128491879 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302501816302538 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039479928091168404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06804009899497032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073679955676198 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1661200076341629 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047800131142139435 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015199882909655571 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2301088534295559 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2203571386635303 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07259007543325424 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16095000319182873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05195010453462601 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09118998423218727 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1818300224840641 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045912884175777 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2564201131463051 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057996794581413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0957101583480835 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19954005256295204 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1327800564467907 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37809903733432293 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2681579682976007 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050348928198218346 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.680825836956501 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483013905584812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16288994811475277 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048969872295856476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22911885753273964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046999892219901085 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07434003055095673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362989708781242 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592098269611597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01543993130326271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23038010112941265 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.237988006323576 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.235461121425033 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477006874978542 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628589816391468 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4273091908544302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04164990969002247 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06779003888368607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16033905558288097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2300799824297428 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06893998943269253 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5063569881021976 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365993224084377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15940004959702492 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048179877921938896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42614899575710297 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03952020779252052 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06683007813990116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301010191440582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15887897461652756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824995994567871 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015169847756624222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2297600731253624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06895000115036964 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.493728021159768 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724999699741602 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15906011685729027 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047239940613508224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42601884342730045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039380043745040894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702984683215618 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07290998473763466 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15827897004783154 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04835985600948334 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2305100206285715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06942008621990681 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.493198098614812 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22636004723608494 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3066500648856163 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.954680101945996 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419916801154613 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16138912178575993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048029934987425804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4263890441507101 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03993022255599499 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789015606045723 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07279007695615292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15850900672376156 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522991806268692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2301300410181284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0696300994604826 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4967869501560926 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07299007847905159 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15927990898489952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05346001125872135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09138998575508595 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19444897770881653 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09471015073359013 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2542890142649412 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909001100808382 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10322988964617252 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20841998048126698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13218913227319717 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38643903099000454 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2901269365102053 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05087978206574917 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7025568522512913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459009066224098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16900920309126377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048519810661673546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4266388714313507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040430109947919846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760982796549797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07381988689303398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591099426150322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04775007255375385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015390105545520782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23054913617670536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0700000673532486 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.512447139248252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258006371557713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15959911979734898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05289982073009014 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09657000191509724 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19366992637515068 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978880941867828 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25444990023970604 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069987572729588 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1023588702082634 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2070590853691101 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13120006769895554 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3831889480352402 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2856079265475273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050629954785108566 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6966459807008505 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074789859354496 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16229995526373386 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0474101398140192 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3276490606367588 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03961008042097092 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678100623190403 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363990880548954 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17507001757621765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04813913255929947 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01546996645629406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2300799824297428 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05870009772479534 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4052879996597767 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07368996739387512 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.160908792167902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05292985588312149 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09228987619280815 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1953491009771824 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08992990478873253 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2540391869843006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09093014523386955 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10297982953488827 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2066800370812416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1328790094703436 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3849088679999113 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2835969682782888 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05080993287265301 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6965561080724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23093889467418194 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3164189402014017 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.038648964837193 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13021985068917274 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21885009482502937 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05233916454017162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.328300055116415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03994000144302845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681891106069088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735800713300705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15931017696857452 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047170091420412064 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13119890354573727 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059949932619929314 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3652569614350796 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310998626053333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608699094504118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04261010326445103 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0916188582777977 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3666197881102562 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897988211363554 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24780002422630787 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10360009036958218 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24408008903265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08803000673651695 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3764198627322912 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4398670755326748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03975885920226574 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8310460727661848 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09137997403740883 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18006982281804085 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04827999509871006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22947904653847218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04011020064353943 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06688013672828674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07280008867383003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588989980518818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047479989007115364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015550060197710991 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1318200957030058 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04810001701116562 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1991881765425205 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07215887308120728 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15826895833015442 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04230998456478119 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09154016152024269 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3679092042148113 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09609991684556007 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24942890740931034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029987268149853 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10360986925661564 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2446998842060566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08835899643599987 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37792883813381195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4507570303976536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039969803765416145 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8395259976387024 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07985997945070267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16872980631887913 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04980899393558502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17980998381972313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03984011709690094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06727990694344044 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394887506961823 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1583790872246027 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761992022395134 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015220139175653458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13135001063346863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04243990406394005 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1378780473023653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07292884401977062 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15929900109767914 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04236004315316677 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909299124032259 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36437902599573135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032990783452988 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24882913567125797 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10206992737948895 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24237018078565598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08755899034440517 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37527899257838726 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4387872070074081 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040109967812895775 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8277759663760662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.241199042648077 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3612388391047716 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.78699978441 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2210990060120821 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.374428927898407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1037998590618372 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18092012032866478 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040489016100764275 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06714998744428158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0860199797898531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1725400798022747 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048179877921938896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015200115740299225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08230912499129772 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04322989843785763 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3591570314019918 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08439994417130947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17125997692346573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036250101402401924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09235902689397335 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9820619858801365 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09193993173539639 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2572790253907442 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078998118638992 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12580002658069134 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35230908542871475 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06664986722171307 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46391901560127735 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.155469058081508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03432994708418846 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.54313906468451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1634701620787382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27513899840414524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07375981658697128 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13184989802539349 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05577993579208851 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067469896748662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08455011993646622 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17085997387766838 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01535983756184578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0827200710773468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03848993219435215 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1919480748474598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0838299747556448 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17049000598490238 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03696000203490257 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09096995927393436 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.923820797353983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09240000508725643 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2589798532426357 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051989763975143 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12600002810359 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3522292245179415 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0671599991619587 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46566897071897984 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.100069101899862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03425986506044865 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.488528870046139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12154900468885899 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2350490540266037 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1313299871981144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03996002487838268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06672996096313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08435011841356754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16934005543589592 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880991764366627 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08304999209940434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03900006413459778 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.136076869443059 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08435011841356754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1714599784463644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03670994192361832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110989049077034 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9339519571512938 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09337998926639557 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2624690532684326 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045982733368874 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12612901628017426 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35347905941307545 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06754999049007893 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4659190308302641 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.117849912494421 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03408989869058132 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.506648914888501 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.44974802993238 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08675013668835163 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02598995342850685 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11434010230004787 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 78.76659999601543 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20663905888795853 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.048209913074970245 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0428101047873497 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02541998401284218 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03711017780005932 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24836906231939793 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07011019624769688 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0664200633764267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15368987806141376 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048560090363025665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0832490622997284 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06868992932140827 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08518993854522705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17133005894720554 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794984124600887 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579164028167725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0830299686640501 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9382879361510277 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06184005178511143 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14779018238186836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036659883335232735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09711901657283306 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.982161870226264 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10089995339512825 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2985391765832901 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09379000402987003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13037887401878834 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.379759119823575 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07082009688019753 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4972890019416809 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.264458945021033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03482005558907986 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.649917991831899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08592987433075905 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17682998441159725 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08230982348322868 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042009823024273 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681891106069088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08500018157064915 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17252983525395393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04782015457749367 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08359993807971478 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9498491417616606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08454988710582256 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17244997434318066 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03681005910038948 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09357999078929424 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.0794010274112225 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09471992962062359 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26007997803390026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09058997966349125 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12727011926472187 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35383901558816433 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06745010614395142 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46551902778446674 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.262519000098109 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03416999243199825 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.650758044794202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07884996011853218 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10358006693422794 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.395213056355715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07509998977184296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18205889500677586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05844002589583397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08436990901827812 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039389822632074356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06867991760373116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338984869420528 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15744008123874664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04542991518974304 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014800112694501877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13192999176681042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031129922717809677 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0505879763513803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07187016308307648 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15553017146885395 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041350023820996284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08999998681247234 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36058900877833366 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905003778636456 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2471189945936203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08903001435101032 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10358006693422794 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2446589060127735 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08840998634696007 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3767991438508034 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.42553704790771 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03929995000362396 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8057669512927532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07282011210918427 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15813997015357018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04653912037611008 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1313299871981144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03793998621404171 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0667900312691927 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07195002399384975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15554996207356453 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04627997986972332 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014399876818060875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13221986591815948 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.998408067971468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07152999751269817 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15554996207356453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042119063436985016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09337998926639557 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36974879913032055 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08786004036664963 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25049992837011814 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896994404494762 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10348018258810043 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24331011809408665 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08864910341799259 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3759388346225023 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4479670207947493 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039389822632074356 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8308660946786404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12884987518191338 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15475996769964695 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.938597954809666 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07461919449269772 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16258889809250832 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687998443841934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1306000631302595 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03855000250041485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06730994209647179 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1570500899106264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045759836211800575 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014479970559477806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23141899146139622 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03678002394735813 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1550469789654016 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07221009582281113 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15647010877728462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05250005051493645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884887211024761 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1939898356795311 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08770008571445942 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2517988905310631 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09606010280549526 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10375003330409527 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20722905173897743 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13167993165552616 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.384968938305974 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2745470739901066 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05109002813696861 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6789070796221495 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0740888062864542 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15914905816316605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04697986878454685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.229269964620471 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03803009167313576 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06790901534259319 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07280008867383003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.157470116391778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04660990089178085 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014449935406446457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2313691657036543 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2000170536339283 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07167994044721127 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15620002523064613 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052870018407702446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09000906720757484 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19332999363541603 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08774991147220135 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25210902094841003 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08810008876025677 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10389881208539009 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20666909404098988 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1322000753134489 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3824189770966768 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2695170007646084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05074008367955685 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6739568673074245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22662896662950516 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25135884061455727 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.056366953998804 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480010390281677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17972011119127274 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04754913970828056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23018987849354744 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867992199957371 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765010766685009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734599307179451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1572989858686924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04610000178217888 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014789868146181107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2312499564141035 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.224597916007042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07190997712314129 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15610991977155209 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671001806855202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22999988868832588 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03709900192916393 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06638001650571823 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07222010754048824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15658000484108925 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04650000482797623 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014330027624964714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23083901032805443 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1915778741240501 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4571851827204227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07264991290867329 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15877000987529755 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04621990956366062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23039896041154861 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03803009167313576 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06762007251381874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07260986603796482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1565590500831604 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04655006341636181 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014399876818060875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23089000023901463 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1969180777668953 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07303920574486256 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16055884771049023 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05310005508363247 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980021812021732 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18792902119457722 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998997509479523 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2564301248639822 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08981907740235329 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09506009519100189 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19833003170788288 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13197003863751888 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3749399911612272 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.262387027963996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0502800103276968 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6713861841708422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07380987517535686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15944987535476685 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046869972720742226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22784899920225143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03776000812649727 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06675999611616135 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07239985279738903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15589897520840168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04644016735255718 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014479970559477806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22974004969000816 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1931881308555603 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.159601870924234 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07389998063445091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15946989879012108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04557007923722267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.426009064540267 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0380899291485548 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06726989522576332 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07282919250428677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1578889787197113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046449946239590645 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014350051060318947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22945995442569256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07606903091073036 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4872970059514046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07304991595447063 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15870016068220139 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04522991366684437 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.425978796556592 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03805989399552345 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067099928855896 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07254001684486866 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15667988918721676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04616007208824158 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014459947124123573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22959895431995392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06850995123386383 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4768671244382858 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0720699317753315 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15600002370774746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0452499371021986 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4260190762579441 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03798981197178364 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06729993037879467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07149018347263336 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15502003952860832 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04642992280423641 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016059959307312965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23056892678141594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0681800302118063 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4731569681316614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22616912610828876 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3067590296268463 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.903849214315414 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07288996130228043 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15709991566836834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046239932999014854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4276891704648733 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0407099723815918 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06770901381969452 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07302011363208294 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15600980259478092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04560011439025402 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014570076018571854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22984901443123817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06908993236720562 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4815970789641142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0716398935765028 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15602982603013515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05328003317117691 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980906568467617 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1939998473972082 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08696992881596088 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2510689664632082 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08887005969882011 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10245013982057571 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20533893257379532 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13078004121780396 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37987902760505676 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2630871497094631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05070003680884838 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6688071191310883 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360987365245819 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15951902605593204 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705996252596378 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4270588979125023 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038239872083067894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06836000829935074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07227994501590729 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1556898932904005 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04534982144832611 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2305388916283846 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07017003372311592 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4851570595055819 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07188995368778706 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15648896805942059 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05266978405416012 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078998118638992 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1952899619936943 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08870009332895279 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2520699054002762 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0885289628058672 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10215002112090588 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2059401012957096 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13280007988214493 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.382099999114871 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2755179777741432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05052005872130394 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6801361925899982 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400987669825554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16041984781622887 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04636007361114025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3276688512414694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03797002136707306 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06705010309815407 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07307878695428371 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15749898739159107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045930035412311554 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830147847533226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23005995899438858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05823979154229164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3697168324142694 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07144990377128124 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15545007772743702 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05254009738564491 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11308910325169563 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19596004858613014 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10369019582867622 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2529490739107132 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08866982534527779 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10217982344329357 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20545907318592072 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1312699168920517 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.379529083147645 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.313877059146762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050729839131236076 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7174771055579185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2306690439581871 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31474907882511616 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.927049977704883 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1308000646531582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21943007595837116 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05174009129405022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3279000520706177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038980040699243546 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06752903573215008 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07183011621236801 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15525007620453835 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04660990089178085 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014819903299212456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13103988021612167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05917903035879135 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3405871577560902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07097003981471062 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15506986528635025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04170997999608517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09063002653419971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.365598825737834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08848984725773335 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24953903630375862 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0976300798356533 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10304991155862808 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2433890476822853 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08803000673651695 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37459912709891796 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4394570607692003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039209844544529915 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8202969804406166 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09257905185222626 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17962907440960407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04732981324195862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22925995290279388 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03818911500275135 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702006794512272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07145991548895836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1542100217193365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04613981582224369 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014499993994832039 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13111880980432034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04854006692767143 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1817470658570528 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07054000161588192 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15481002628803253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04218984395265579 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09066890925168991 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36710011772811413 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08891988545656204 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2456901129335165 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08924910798668861 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10372977703809738 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2430700697004795 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08749007247388363 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3739700186997652 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.427986891940236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040079932659864426 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.808325992897153 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07855985313653946 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16540009528398514 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048220157623291016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18035899847745895 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039006307721138 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06740004755556583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07239985279738903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1555299386382103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04631909541785717 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014740042388439178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13122987002134323 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04196004010736942 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1174380779266357 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07156003266572952 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1557590439915657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042400090023875237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09019998833537102 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3657389897853136 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08792988955974579 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24645891971886158 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911010809242725 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10258005931973457 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2416800707578659 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08757901377975941 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3729090094566345 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.426636939868331 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039029866456985474 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8065259791910648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.240229070186615 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35811890847980976 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.642970049753785 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22089900448918343 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3742491826415062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10293000377714634 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18086005002260208 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03871903754770756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06745010614395142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08452008478343487 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1698499545454979 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04735984839498997 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014760065823793411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08232914842665195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042839907109737396 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.353326952084899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08461996912956238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1699498388916254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03811996430158615 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104912169277668 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9932229556143284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09089987725019455 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25364989414811134 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986890316009521 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12592016719281673 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3525298088788986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06698910146951675 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46325894072651863 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.155988968908787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03408989869058132 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.541129037737846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1624699216336012 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27337903156876564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0739500392228365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13109995052218437 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038560014218091965 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06624008528888226 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08303998038172722 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16704900190234184 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04646996967494488 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014719786122441292 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08185999467968941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03742007538676262 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1555980890989304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08391006849706173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1679188571870327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0359599944204092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08987006731331348 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9404709823429585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09200000204145908 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2565698232501745 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08965888991951942 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1264498569071293 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35220012068748474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712903268635273 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46419911086559296 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.115168867632747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03425986506044865 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.496457917615771 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11863000690937042 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2307200338691473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0737400259822607 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13051996938884258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03845011815428734 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06657000631093979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08399900980293751 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16796891577541828 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046659959480166435 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014800112694501877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08223997429013252 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03817002288997173 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1196080595254898 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08400995284318924 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16904901713132858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036670127883553505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09113992564380169 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9308620616793633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125005453824997 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2589188516139984 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967984467744827 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12656999751925468 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3521689213812351 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06697000935673714 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4638789687305689 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.099589936435223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034250086173415184 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.4821588564664125 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.369678968563676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08785002864897251 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025609973818063736 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11843000538647175 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 78.04447994567454 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.1896698959171772 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04607904702425003 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04134001210331917 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025040004402399063 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.036770012229681015 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24036900140345097 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0694498885422945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06494997069239616 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14845887199044228 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046629924327135086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08291006088256836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03945012576878071 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749993190169334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08525000885128975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16923900693655014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046090222895145416 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014859950169920921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08249003440141678 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9165580850094557 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06097997538745403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14488003216683865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036659883335232735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09156996384263039 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9786421693861485 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065982885658741 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25985902175307274 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0898200087249279 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12590992264449596 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35203900188207626 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653997115790844 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4622088745236397 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.144570022821426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03350991755723953 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.518159130588174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08419901132583618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1692089717835188 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04624994471669197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0810499768704176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038109952583909035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.065990025177598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0849999487400055 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16864901408553123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806020297110081 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014760065823793411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08252006955444813 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9266079869121313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0834090169519186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1680490095168352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03651995211839676 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9225320797413588 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09154900908470154 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2657598815858364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08887005969882011 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12869900092482567 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3557289019227028 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06719003431499004 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46620890498161316 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.099789937958121 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03394880332052708 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.481357919052243 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07866998203098774 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10345992632210255 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.042962877079844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07523014210164547 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16352999955415726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04684017039835453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08122902363538742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03818981349468231 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06687012501060963 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258006371557713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15572994016110897 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046170083805918694 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01446995884180069 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1328790094703436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03077997826039791 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0013580322265625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07170997560024261 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1564000267535448 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04215002991259098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177904576063156 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36113010719418526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09317882359027863 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2537101972848177 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08897995576262474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10409997776150703 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2448291052132845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08905981667339802 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3767390735447407 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4372069854289293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03933999687433243 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8181761261075735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585199497640133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04595005884766579 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13197003863751888 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038519036024808884 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06734998896718025 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724398996680975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1554400660097599 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04649977199733257 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014730030670762062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13416889123618603 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0079480707645416 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08164998143911362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17171003855764866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04200008697807789 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08984887972474098 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3709099255502224 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08777901530265808 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24669989943504333 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08888007141649723 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10380009189248085 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24384004063904285 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08949008770287037 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3763190470635891 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.432896824553609 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0395490787923336 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8319759983569384 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1288400962948799 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15383982099592686 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.916157970204949 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548998109996319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16369996592402458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834914579987526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1315500121563673 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03878003917634487 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06796000525355339 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376004941761494 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15777884982526302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680990241467953 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014709774404764175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23265019990503788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03627012483775616 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1678470764309168 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0727199949324131 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15747989527881145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05295011214911938 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034015238285065 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19516912288963795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08880021050572395 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25327899493277073 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969008922576904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10309997014701366 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20592007786035538 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13328902423381805 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38293912075459957 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2713770847767591 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0506499782204628 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6763568855822086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07341988384723663 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591099426150322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046859029680490494 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23098988458514214 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03881007432937622 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06730994209647179 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365015335381031 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1577991060912609 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04640012048184872 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01461012288928032 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23280899040400982 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2025570031255484 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09726989082992077 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19434001296758652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05802977830171585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09423913434147835 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19514979794621468 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08819997310638428 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25292905047535896 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08860998786985874 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10447017848491669 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20701996982097626 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13394909910857677 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38466881960630417 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.277026953175664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05074008367955685 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7312769778072834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22907904349267483 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2579088322818279 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.133207120001316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518008351325989 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16447994858026505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477801077067852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23129885084927082 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03853999078273773 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728991866111755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07295981049537659 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15725009143352509 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725996404886246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014769844710826874 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23298989981412888 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2074180413037539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0723500270396471 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1553699839860201 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04623900167644024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2317400649189949 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04097004421055317 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06723008118569851 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258891128003597 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16351905651390553 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04657008685171604 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014660181477665901 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2326001413166523 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2051078956574202 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.453855937346816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294001989066601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16013998538255692 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047079985961318016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23137894459068775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038380036130547523 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06714998744428158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298984564840794 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15598000027239323 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04677986726164818 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01470884308218956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23315008729696274 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2014380190521479 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07218006066977978 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15888898633420467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052549876272678375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09113014675676823 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18413993529975414 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927006274461746 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25357981212437153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913897909224033 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09653996676206589 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19840989261865616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1335199922323227 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37634000182151794 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2564579956233501 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050449976697564125 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6639658715575933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387017831206322 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15958002768456936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047239940613508224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23195892572402954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03875000402331352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07338006980717182 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342989556491375 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1576300710439682 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046558910980820656 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014749821275472641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2294799778610468 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2070781085640192 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.165001912042499 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07200916297733784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15685893595218658 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045629916712641716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.426239101216197 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03857002593576908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0671599991619587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0723798293620348 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15702005475759506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0466399360448122 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01460895873606205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22956007160246372 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0684601254761219 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4750270638614893 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07243920117616653 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15759910456836224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04618987441062927 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42548892088234425 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867992199957371 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06683007813990116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07256981916725636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15566986985504627 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725996404886246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014770077541470528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22903992794454098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06858003325760365 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.474627060815692 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07137004286050797 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15594903379678726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04564994014799595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4265890456736088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041109975427389145 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06709014996886253 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07234001532196999 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15655998140573502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725996404886246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22931001149117947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06807991303503513 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4797269832342863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22607902064919472 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30447891913354397 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.886219976469874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310998626053333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15807896852493286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046479981392621994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42620906606316566 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039280159398913383 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067099928855896 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730298925191164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15703006647527218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04540011286735535 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014569144695997238 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2293901052325964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06958004087209702 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4789269771426916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07237889803946018 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15701912343502045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05263998173177242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19502895884215832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08766003884375095 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26305997744202614 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983910083770752 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10282988660037518 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2055000513792038 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13174000196158886 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3808788023889065 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2787370942533016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05059991963207722 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6828160732984543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734301283955574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15901005826890469 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04606996662914753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42644888162612915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039289938285946846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06761983968317509 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07262011058628559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15540001913905144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04582013934850693 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014870194718241692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22972910664975643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06919982843101025 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4797968324273825 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07166992872953415 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15585008077323437 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053268857300281525 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09070010855793953 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19470998086035252 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08751894347369671 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25200005620718 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948985487222672 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10233884677290916 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20454893819987774 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13051996938884258 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.377509044483304 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2633479200303555 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050489092245697975 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.667845994234085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545994594693184 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16326992772519588 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04675006493926048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3277091309428215 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842008300125599 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06647990085184574 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07281010039150715 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15690992586314678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04569999873638153 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2293300349265337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05834992043673992 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3719180133193731 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0720189418643713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15736883506178856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0528101809322834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08999998681247234 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19510905258357525 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08841999806463718 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25114999152719975 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09000883437693119 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10291999205946922 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2098497934639454 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1315101981163025 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38445997051894665 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2706280685961246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04936009645462036 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6742560546845198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23048999719321728 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31370995566248894 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.887530002743006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13003009371459484 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2164999023079872 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051819952204823494 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 3.385402960702777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.056339893490076065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07734913378953934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08255010470747948 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17228000797331333 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047350069507956505 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018609920516610146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1343290787190199 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059480080381035805 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 4.482230171561241 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445993833243847 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16331998631358147 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042419880628585815 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09662890806794167 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3690090961754322 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25344989262521267 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09156903252005577 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10316004045307636 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2487099263817072 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08848006837069988 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38233003579080105 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4624360483139753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04036002792418003 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8617459572851658 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17965002916753292 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23002899251878262 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04087993875145912 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067469896748662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335888221859932 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1597690861672163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04691001959145069 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1317099668085575 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048759859055280685 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2034480459988117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07240008562803268 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15842891298234463 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04285993054509163 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09193015284836292 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3675988409668207 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09832018986344337 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24833902716636658 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09109009988605976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10317005217075348 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24459883570671082 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09222002699971199 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38381898775696754 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.458986895158887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04015001468360424 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.848096027970314 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08058897219598293 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16941898502409458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0502199400216341 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18008006736636162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039800070226192474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06778002716600895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07295003160834312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582200638949871 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558009535074234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13149995356798172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04267995245754719 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1393080931156874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284991443157196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15901983715593815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042190076783299446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153992868959904 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36711920984089375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051011875271797 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24965917691588402 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057996794581413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1030599232763052 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24368916638195515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08867005817592144 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37794909439980984 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4463469851762056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039770035073161125 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8419669941067696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24132896214723587 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3616290632635355 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 12.971443124115467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22061890922486782 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3726491704583168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10373000986874104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18165912479162216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039006307721138 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06799004040658474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08552009239792824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17268001101911068 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824879579246044 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08294009603559971 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043740030378103256 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3647968880832195 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08505000732839108 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17287000082433224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036939047276973724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09241001680493355 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9457520470023155 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09342003613710403 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2583288587629795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09107985533773899 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12650014832615852 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35393889993429184 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06677000783383846 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46639912761747837 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.1262699998915195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03435998223721981 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.518659017980099 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16336911357939243 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27417903766036034 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1315500121563673 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04122988320887089 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06723892875015736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08511007763445377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17173984088003635 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08267001248896122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03802916035056114 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1829771101474762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08746003732085228 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17527001909911633 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03672018647193909 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923799816519022 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.973082173615694 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09287986904382706 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2593989484012127 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162980131804943 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12704008258879185 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35471911542117596 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06761006079614162 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46946899965405464 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.156709114089608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03442983143031597 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.549828987568617 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12002000585198402 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23404904641211033 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07405993528664112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1315500121563673 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012905992567539 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06662006489932537 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08459994569420815 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1706599723547697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047549838200211525 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015350058674812317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08277897723019123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03863009624183178 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1341169010847807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08394988253712654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.171339837834239 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036739977076649666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09289896115660667 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9129231590777636 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09213900193572044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2670600078999996 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09093992412090302 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12606987729668617 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35392981953918934 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06693997420370579 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46740006655454636 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.099739180877805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03414996899664402 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.488768918439746 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.464828005060554 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08977996185421944 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026409979909658432 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15358906239271164 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 81.12322399392724 ms for forwarding
--------------------
No. 4
<class 'diffusers.models.embeddings.Timesteps'> take 0.250119948759675 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.06128987297415733 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03850995562970638 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04088995046913624 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.29772892594337463 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15054992400109768 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09572017006576061 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.20663999021053314 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053140101954340935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.10987999849021435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04239892587065697 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0792602077126503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09287986904382706 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18787989392876625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05042995326220989 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0168501865118742 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09022001177072525 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.101137138903141 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0772599596530199 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1743501052260399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03948993980884552 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09821890853345394 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.012441961094737 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10724016465246677 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2810489386320114 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09730015881359577 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13156910426914692 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3707690630108118 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07232022471725941 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49399910494685173 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.3068590350449085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036330195143818855 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.7362879160791636 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08900021202862263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18258998170495033 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0491999089717865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08479994721710682 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0410501379519701 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07041892968118191 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08969008922576904 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18008006736636162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04819990135729313 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501990482211113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08596014231443405 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9813979268074036 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08760998025536537 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17715012654662132 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038109952583909035 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09699002839624882 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.153501009568572 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09718001820147038 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27024908922612667 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09670993313193321 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13114884495735168 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3638989292085171 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07107993587851524 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4818288143724203 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.388529039919376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035359058529138565 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.791918141767383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0825000461190939 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1096001360565424 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.838611077517271 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08037989027798176 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1762700267136097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05086907185614109 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08475012145936489 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04358985461294651 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08255988359451294 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0799400731921196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17715897411108017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014750054106116295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13683992438018322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033090123906731606 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1024679988622665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0756590161472559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677291002124548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04427018575370312 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11317990720272064 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37243892438709736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09401002898812294 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26605906896293163 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09606988169252872 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10717916302382946 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25601894594728947 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09871996007859707 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40919892489910126 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5424268785864115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0416699331253767 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9515359308570623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0765800941735506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16922899521887302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04827999509871006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13575004413723946 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04032999277114868 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07005012594163418 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0765800941735506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16568019054830074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015009893104434013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13745995238423347 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.054797787219286 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07661990821361542 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1675398088991642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04478008486330509 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11735991574823856 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3801791463047266 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09516999125480652 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2737089525908232 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11576991528272629 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10755890980362892 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25269901379942894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09338022209703922 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3940889146178961 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5832569915801287 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041980063542723656 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0033668261021376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13267900794744492 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16046897508203983 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.381317041814327 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07984996773302555 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1772597897797823 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05163997411727905 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13553909957408905 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04382012411952019 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07563992403447628 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07889000698924065 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17364905215799809 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05041016265749931 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016510020941495895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2393599133938551 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03941007889807224 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2523781042546034 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07949001155793667 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17291982658207417 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05509005859494209 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09911996312439442 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20113890059292316 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09538000449538231 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27935905382037163 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09720004163682461 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10801013559103012 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2194289118051529 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13795006088912487 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4054189193993807 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3666169252246618 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05685002543032169 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8237759359180927 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0931890681385994 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19481894560158253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05207001231610775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23742904886603355 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041830120608210564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07097981870174408 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07847021333873272 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18054991960525513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.062000006437301636 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018930062651634216 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23975991643965244 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3423569034785032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0770101323723793 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17106905579566956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0548800453543663 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09789993055164814 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19721011631190777 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0963299535214901 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.27245981618762016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09652902372181416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10741990990936756 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21966011263430119 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1371600665152073 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40591019205749035 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3555281329900026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05188002251088619 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7906262073665857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23178989067673683 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25963992811739445 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.584116956219077 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07652910426259041 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17052888870239258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901993088424206 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2340199425816536 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04718010313808918 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07089017890393734 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466995157301426 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16196002252399921 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04764995537698269 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01561013050377369 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23354892618954182 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2588570825755596 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735200010240078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628398895263672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23506907746195793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04552979953587055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06932998076081276 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435004226863384 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16151904128491879 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04837987944483757 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23420993238687515 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2538081500679255 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.559334971010685 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473002187907696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16387994401156902 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23296894505620003 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039629871025681496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06936001591384411 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438007742166519 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16075000166893005 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04818011075258255 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015420140698552132 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23402017541229725 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.234488096088171 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07432885468006134 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16511883586645126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05493988282978535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09236996993422508 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19166897982358932 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10820990428328514 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26019010692834854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0966999214142561 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20184996537864208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13509904965758324 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3887689672410488 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3305270113050938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054770149290561676 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7698658630251884 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08291006088256836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17237011343240738 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048699090257287025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23299013264477253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399099662899971 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06847991608083248 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437006570398808 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16147899441421032 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04692003130912781 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579862520098686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23364904336631298 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2415170203894377 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.356789868324995 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07424992509186268 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16328017227351665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047519803047180176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4330091178417206 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03957003355026245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06916909478604794 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453002035617828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16152998432517052 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04845019429922104 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579862520098686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2343489322811365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07028994150459766 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5204569790512323 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734599307179451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16175000928342342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04739896394312382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43282913975417614 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04002009518444538 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365993224084377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609697937965393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048209913074970245 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23502996191382408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07117004133760929 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5209871344268322 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377914153039455 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16229902394115925 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43317885138094425 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03965012729167938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06851996295154095 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07285992614924908 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594501081854105 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015349825844168663 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23370003327727318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07079006172716618 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5170869883149862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23032911121845245 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32398896291852 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.0461499486118555 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07448019459843636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637497916817665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722992889583111 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43438980355858803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03926013596355915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06977003067731857 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0753400381654501 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16189995221793652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047129811719059944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23429887369275093 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0713299959897995 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.520967110991478 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08496991358697414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17694896087050438 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05470006726682186 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09304005652666092 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1974799670279026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912011981010437 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2576399128884077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906891655176878 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10442989878356457 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21005002781748772 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13385014608502388 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3918998409062624 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3014080468565226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05188002251088619 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7393359448760748 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0786101445555687 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16841990873217583 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04877988249063492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4342789761722088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03893999382853508 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06911903619766235 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07861992344260216 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1762700267136097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057060038670897484 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018849968910217285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23376895114779472 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07147993892431259 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5630670823156834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07217982783913612 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039912588894367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05353987216949463 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09249988943338394 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1971600577235222 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08903001435101032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2642599865794182 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09049917571246624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10291999205946922 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20820996724069118 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1322298776358366 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3878700081259012 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3009670656174421 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050999922677874565 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7178759444504976 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07600989192724228 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16535003669559956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483999028801918 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3278790973126888 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06879004649817944 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751300249248743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16336003318428993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23032911121845245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0592600554227829 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4054470229893923 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360987365245819 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16277004033327103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055080046877264977 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10979012586176395 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1953798346221447 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08958997204899788 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2533600199967623 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09097997099161148 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10285899043083191 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20867912098765373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1332398969680071 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3885389305651188 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3185169082134962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05118991248309612 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7468971200287342 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23193005472421646 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.317589845508337 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.233559180051088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13095908798277378 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.220918795093894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05280994810163975 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32866885885596275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04020007327198982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06857002153992653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348018698394299 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596100628376007 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047289999201893806 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01547904685139656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13230997137725353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0605299137532711 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.367466989904642 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284991443157196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16014999710023403 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04257913678884506 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09195995517075062 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3696188796311617 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10742992162704468 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2497900277376175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09075901471078396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10333000682294369 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24604983627796173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08970987983047962 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38668978959321976 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.490787137299776 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043449923396110535 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8993059638887644 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10651000775396824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19720988348126411 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2303889486938715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03982987254858017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820005364716053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07330905646085739 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15876907855272293 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1321299932897091 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049010151997208595 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.227078028023243 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07103988900780678 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15893904492259026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042839907109737396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09185005910694599 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3681688103824854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09008008055388927 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24966895580291748 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09064003825187683 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10353000834584236 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24612899869680405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08922000415623188 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3837591502815485 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.453867182135582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04069996066391468 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8490860238671303 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08066999725997448 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17103878781199455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04915008321404457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18048007041215897 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03927992656826973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06809993647038937 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07243012078106403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15880982391536236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13884902000427246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043090200051665306 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.15162692964077 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07146992720663548 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15863007865846157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04295003600418568 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197904728353024 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36333990283310413 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08965912275016308 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24954997934401035 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953013457357883 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10279892012476921 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25077909231185913 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09499001316726208 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39670919068157673 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4607070479542017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04046992398798466 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8529468216001987 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24013989605009556 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3582399804145098 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.92651004344225 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2259700559079647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3803290892392397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1042000949382782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18217903561890125 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03972998820245266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068240100517869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0879999715834856 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17632916569709778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04826020449399948 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08330983109772205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044029904529452324 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3817371800541878 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08486001752316952 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17966888844966888 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037929974496364594 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0927499495446682 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.001222085207701 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09296881034970284 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2578301355242729 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09144889190793037 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1264498569071293 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35514007322490215 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06798910908401012 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47085899859666824 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.187758943066001 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03423006273806095 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.5907180067151785 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16513001173734665 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27718907222151756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07421988993883133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13229995965957642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039350008592009544 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736000068485737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08370005525648594 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17080898396670818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0474101398140192 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08323998190462589 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03885990008711815 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1854080948978662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0837401021271944 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17303996719419956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0368200708180666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09247008711099625 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9263819344341755 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09310012683272362 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2596289850771427 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912998802959919 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12733880430459976 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3574090078473091 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06971997208893299 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4760988522320986 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.121140042319894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034610042348504066 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.517198937013745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12270011939108372 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23648981004953384 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07397006265819073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1315888948738575 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039950013160705566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06707990542054176 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08306000381708145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1689600758254528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04753889515995979 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015400117263197899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08332007564604282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038770027458667755 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1441679671406746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0843200832605362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18375995568931103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04205992445349693 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09687012061476707 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9909619372338057 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252899326384068 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2609000075608492 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09192992001771927 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1270801294595003 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3551200497895479 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06776000373065472 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4709390923380852 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.188219016417861 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03462005406618118 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.608057836070657 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.65813704393804 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0858099665492773 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026060035452246666 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.18136901780962944 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 81.0634340159595 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19737984985113144 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05294894799590111 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.043419888243079185 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025739893317222595 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04375004209578037 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2639989834278822 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.08564000017940998 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07021916098892689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15783892013132572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04905997775495052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08327001705765724 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969017416238785 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06794021464884281 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08441903628408909 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17061899416148663 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0478401780128479 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08313986472785473 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.938998069614172 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0638000201433897 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15000905841588974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036739977076649666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09138998575508595 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.914272179827094 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920700840651989 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2613991964608431 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09743007831275463 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1331898383796215 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3714489284902811 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760004907846451 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4877189639955759 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.126560106873512 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034490134567022324 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.511259194463491 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08597993291914463 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17409003339707851 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048519810661673546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08510006591677666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047770095989108086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07308903150260448 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08586980402469635 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17206999473273754 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04740990698337555 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015330035239458084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08354010060429573 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9692281018942595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0849999487400055 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17091003246605396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036949990317225456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09234994649887085 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.1192821227014065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923501793295145 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25845994241535664 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09090895764529705 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1283399760723114 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35676988773047924 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06774882785975933 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46973908320069313 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.299159092828631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03435998223721981 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.691478028893471 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07942994125187397 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1058999914675951 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.319713063538074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07591885514557362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16795890405774117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08147000335156918 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039999838918447495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07403991185128689 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08792988955974579 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1795790158212185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047170091420412064 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01574005000293255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13335002586245537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031559960916638374 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0644670110195875 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07245992310345173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15828991308808327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04199007526040077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09751995094120502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3661189693957567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09041000157594681 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2492191269993782 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067985229194164 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10548904538154602 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24794903583824635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08903001435101032 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38220896385610104 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4590669889003038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03991997800767422 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8474070820957422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07495982572436333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16199913807213306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047999899834394455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13254000805318356 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939005546271801 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06725895218551159 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294001989066601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15797000378370285 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0467000063508749 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13363896869122982 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0196769144386053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07216981612145901 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1577299553900957 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042689964175224304 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09124004282057285 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3714989870786667 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968892507255077 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24627987295389175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904700718820095 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10459893383085728 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24681887589395046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09004003368318081 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38178893737494946 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4473970513790846 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041800085455179214 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.839386997744441 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295700203627348 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15615997835993767 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.028987001627684 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552001625299454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16505992971360683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04802015610039234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13304897584021091 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03972998820245266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06904988549649715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07361988537013531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1590200699865818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04715914838016033 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015589874237775803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23320992477238178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03697001375257969 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1819880455732346 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275911048054695 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585891004651785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05352986045181751 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09154994040727615 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19487994723021984 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08956994861364365 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2537101972848177 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09108008816838264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10301009751856327 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2074800431728363 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13393000699579716 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38657989352941513 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2836079113185406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05129911005496979 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.695685787126422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487996481359005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623299904167652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04857010208070278 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2318990882486105 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03924989141523838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068610068410635 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325015030801296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17602997832000256 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901993088424206 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015308847650885582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2332199364900589 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2399281840771437 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07340987212955952 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15876907855272293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05352986045181751 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09225006215274334 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1922298688441515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08864910341799259 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2533302176743746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021000005304813 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10353000834584236 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21183001808822155 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1329299993813038 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39052008651196957 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2849278282374144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05085999146103859 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6965458635240793 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22628996521234512 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25232997722923756 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.167327985167503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07543899118900299 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16639893874526024 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04939013160765171 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2286899834871292 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03932882100343704 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06756000220775604 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073309987783432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15936000272631645 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015040161088109016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22993911989033222 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.217256998643279 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305992767214775 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15905010513961315 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04746997728943825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22960896603763103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04041008651256561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736000068485737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402896881103516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16018911264836788 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047930050641298294 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01498986966907978 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22950000129640102 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2133580166846514 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.473754109814763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07304991595447063 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16147014684975147 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04766997881233692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22888905368745327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03933999687433243 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06788014434278011 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363990880548954 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592300832271576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047669047489762306 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22958009503781796 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.211537979543209 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07316889241337776 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16180891543626785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05261995829641819 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104982018470764 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18158997409045696 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069009684026241 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25539007037878036 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909091904759407 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09608990512788296 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20029000006616116 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13299006968736649 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38032978773117065 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2698869686573744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05031004548072815 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6832260880619287 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08989009074866772 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17819995991885662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22868881933391094 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039869919419288635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749993190169334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354002445936203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15993905253708363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792003892362118 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0153400469571352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22968999110162258 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2310980819165707 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.222082206979394 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07314886897802353 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16022915951907635 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792003892362118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4267089534550905 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039739999920129776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06763986311852932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07277983240783215 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589898020029068 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048318877816200256 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23022014647722244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06866990588605404 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4976870734244585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15903892926871777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04658987745642662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4256290849298239 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039139995351433754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06667012348771095 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258006371557713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15802006237208843 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794914275407791 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2305000089108944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06962986662983894 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4920069370418787 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591891050338745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42577902786433697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03972998820245266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728991866111755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07214001379907131 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1576689537614584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22993003949522972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0690289307385683 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.491395989432931 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22664014250040054 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3066197969019413 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.943740088492632 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07403013296425343 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16010901890695095 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04775007255375385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4262190777808428 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978004679083824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06741005927324295 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07270881906151772 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15715905465185642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22972002625465393 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06946921348571777 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.492006005719304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733700580894947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1597600057721138 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053029973059892654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253993630409241 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1941290684044361 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08997996337711811 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2535588573664427 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065982885658741 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10247016325592995 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20640017464756966 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13191904872655869 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38476893678307533 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2886668555438519 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05049002356827259 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.700226916000247 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1618298701941967 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734913818538189 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4260800778865814 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03936886787414551 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765010766685009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416005246341228 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15952996909618378 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04739989526569843 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23033889010548592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0695700291544199 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5007168985903263 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07245992310345173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15906011685729027 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052799005061388016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0916400458663702 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19474001601338387 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0882099848240614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2530990168452263 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09386008605360985 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1024790108203888 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20694895647466183 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13130996376276016 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3830490168184042 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2940880842506886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04996010102331638 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7028870061039925 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07553980685770512 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633001957088709 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047629931941628456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3282390534877777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765988655388355 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418985478579998 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16052881255745888 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048010144382715225 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2299700863659382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05913013592362404 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.391697907820344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07277005352079868 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15974999405443668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052549876272678375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126006625592709 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19585900008678436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08855992928147316 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2529991324990988 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09115994907915592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10264990851283073 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20608003251254559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13283896259963512 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38403901271522045 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2809771578758955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05171005614101887 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6972869634628296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23188907653093338 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3184888046234846 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.015309089794755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13069994747638702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2194500993937254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05218898877501488 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32832007855176926 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040040118619799614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728898733854294 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338006980717182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15923986211419106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015070196241140366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1313788816332817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05908007733523846 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3552568852901459 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07223011925816536 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15936000272631645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04208995960652828 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09152921847999096 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36691012792289257 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09026005864143372 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24628988467156887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032920934259892 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10337983258068562 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24508009664714336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0884300097823143 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37773000076413155 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4390968717634678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03976002335548401 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8292658496648073 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09104982018470764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17910008318722248 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049029942601919174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24244911037385464 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04225992597639561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06819004192948341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735800713300705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588989980518818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04752003587782383 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015330035239458084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13172999024391174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0487098004668951 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2188679538667202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07318006828427315 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596990041434765 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042220111936330795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09130011312663555 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36693899892270565 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895399134606123 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25176885537803173 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010010398924351 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1029099803417921 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2437999937683344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0887489877641201 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37676887586712837 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4433269388973713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04007015377283096 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.832675887271762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08008000440895557 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.169259961694479 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049419933930039406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18006889149546623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040109967812895775 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0676899217069149 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07255002856254578 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15766010619699955 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705018363893032 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13124989345669746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04306016489863396 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1368580162525177 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07225992158055305 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15805894508957863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04188995808362961 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09189010597765446 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3639189526438713 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09011011570692062 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2530592028051615 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10227994062006474 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24372991174459457 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08844886906445026 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3772189375013113 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4456368517130613 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039629871025681496 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8382759299129248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24027912877500057 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35970890894532204 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.784559952095151 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2207688521593809 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3740491811186075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10274001397192478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18113898113369942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03893999382853508 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06697000935673714 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08477992378175259 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16893004067242146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04760012961924076 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014510005712509155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08245883509516716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043120002374053 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3459271285682917 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08434010669589043 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17005996778607368 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03643985837697983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106914512813091 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.003911977633834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09019998833537102 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2722588833421469 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927984163165092 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12657907791435719 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3522690385580063 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702006794512272 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46301912516355515 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.181219195947051 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033689895644783974 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.565239116549492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1636201050132513 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27615996077656746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07329019717872143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13163895346224308 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903009928762913 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06626010872423649 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08409004658460617 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16817008145153522 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04708999767899513 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014819903299212456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08221878670156002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037949997931718826 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.163987908512354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08418015204370022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16877008602023125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03661983646452427 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08963886648416519 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.931912127882242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094015695154667 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2575698308646679 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918996900320053 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12590992264449596 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3507190849632025 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06791995838284492 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46331901103258133 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.0951789598912 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033769989386200905 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.478309001773596 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12065889313817024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2351989969611168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07787998765707016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13102986849844456 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869994543492794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06646988913416862 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0837098341435194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1678199041634798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046070199459791183 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08275010623037815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03793998621404171 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1260479222983122 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0833999365568161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16867881640791893 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03706011921167374 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09035994298756123 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9350020233541727 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09062979370355606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25842897593975067 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895101111382246 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1263190060853958 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3508289810270071 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06699003279209137 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46070897951722145 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.098850000649691 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03435998223721981 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.482458043843508 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.39470779709518 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08710986003279686 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025219982489943504 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11936994269490242 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 78.57878995127976 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.1956589985638857 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.050560105592012405 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04267995245754719 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02495013177394867 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03648013807833195 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2455699723213911 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0710589811205864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07039005868136883 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162170035764575 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705018363893032 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08299015462398529 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03878003917634487 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06673997268080711 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08457014337182045 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16787019558250904 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04600011743605137 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015149824321269989 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08230889216065407 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9276280179619789 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.061450060456991196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14540995471179485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0366398598998785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09185983799397945 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.977032145485282 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09145005606114864 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26325881481170654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957996033132076 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1283399760723114 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.355549156665802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0676899217069149 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4673691000789404 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.155988968908787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03387988545000553 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.532429087907076 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08561019785702229 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17188000492751598 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045928871259093285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08215988054871559 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03912998363375664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06683007813990116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08573988452553749 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16940012574195862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04583899863064289 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08335011079907417 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9260880760848522 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08438993245363235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1676699612289667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03648991696536541 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071896784007549 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.092192044481635 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0945699866861105 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2605200279504061 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08922000415623188 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12821913696825504 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3559589385986328 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750994361937046 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4668189212679863 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.267638945952058 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033319927752017975 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.6485680397599936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07972004823386669 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10484014637768269 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.2385129109025 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16361987218260765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04624994471669197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08162995800375938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038190046325325966 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07169018499553204 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15526986680924892 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04601990804076195 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014740042388439178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13320893049240112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.030760187655687332 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0035480372607708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0712699256837368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15485985204577446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04159985110163689 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09022909216582775 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.362589955329895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08787889964878559 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24772994220256805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918996900320053 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1035591121762991 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24339905939996243 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08918019011616707 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37623895332217216 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4250969979912043 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04850979894399643 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8176070880144835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10143010877072811 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1875788439065218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04591001197695732 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13231998309493065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03802007995545864 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06674905307590961 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07165991701185703 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1541499514132738 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04571001045405865 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01455005258321762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1328790094703436 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0259267874062061 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07108994759619236 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1537699718028307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04163011908531189 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08943001739680767 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3658188506960869 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0880600418895483 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24450896307826042 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08826004341244698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1036589965224266 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24456903338432312 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0891699455678463 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3771090414375067 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4248869847506285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039289938285946846 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8030270002782345 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12894906103610992 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15439908020198345 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.903127836063504 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0737400259822607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16192998737096786 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687020555138588 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13141892850399017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037739984691143036 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06890995427966118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07253000512719154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15555997379124165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045318854972720146 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014940043911337852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23308000527322292 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03765011206269264 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1591280344873667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293885573744774 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15711900778114796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05295989103615284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08891988545656204 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19655912183225155 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08719996549189091 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25140983052551746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08712895214557648 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10298006236553192 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20602997392416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13306993059813976 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38297008723020554 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2650168500840664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05075009539723396 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6703361179679632 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365993224084377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1590200699865818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04675006493926048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23102900013327599 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03820005804300308 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789993494749069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07263012230396271 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567790750414133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04619988612830639 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014380086213350296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23256009444594383 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2003679294139147 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07138983346521854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1556389033794403 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05338992923498154 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09004981257021427 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19846903160214424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08707004599273205 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2580799628049135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08919020183384418 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10368996299803257 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20485022105276585 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13151997700333595 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38002897053956985 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2789468746632338 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050609931349754333 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6829159576445818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22684992291033268 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2513900399208069 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.061178166419268 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749099999666214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16312883235514164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23018009960651398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037850113585591316 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06743008270859718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301988080143929 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15706010162830353 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04650000482797623 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014849938452243805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23112911731004715 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2037770356982946 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0720699317753315 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15606009401381016 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04601990804076195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.230149133130908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037529971450567245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06674998439848423 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07181987166404724 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15576998703181744 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04677893593907356 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014479970559477806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23095007054507732 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1924679856747389 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4381550028920174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0717600341886282 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1576391514390707 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04661991260945797 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23125996813178062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04024012014269829 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742891855537891 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07328996434807777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15740981325507164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04594004712998867 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014400109648704529 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23080897517502308 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2021372094750404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07171998731791973 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15887990593910217 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05267001688480377 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913897909224033 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18225004896521568 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08808006532490253 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2513390500098467 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953013457357883 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09619002230465412 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19790907390415668 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13286015018820763 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3747190348803997 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2468670029193163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05060015246272087 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6535469330847263 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07397914305329323 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607190351933241 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22986996918916702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03779004327952862 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06669992581009865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07277005352079868 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15651993453502655 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047110021114349365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23094890639185905 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2013469822704792 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.155080998316407 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07973005995154381 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16703014262020588 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04534912295639515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42902003042399883 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03807013854384422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749993190169334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07178005762398243 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15527009963989258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0463300384581089 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014689983800053596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23089908063411713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06868992932140827 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4897871296852827 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07186993025243282 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15493901446461678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04473002627491951 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.428458908572793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03768014721572399 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06686989217996597 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07143011316657066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15495019033551216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046489061787724495 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01443992368876934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23073004558682442 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06916001439094543 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4723469503223896 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07200008258223534 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15582889318466187 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04527997225522995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4285790491849184 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03798981197178364 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06681983359158039 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07162988185882568 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15422911383211613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06280001252889633 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014649936929345131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23179012350738049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06880005821585655 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4930169563740492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22823014296591282 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31014997512102127 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.92093013599515 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392908446490765 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15873904339969158 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04616007208824158 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4289390053600073 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038599828258156776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07185013964772224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664198935031891 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04524015821516514 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014598947018384933 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23111002519726753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07002009078860283 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.491667004302144 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07201987318694592 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15759910456836224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05337991751730442 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057996794581413 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19622989930212498 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08728890679776669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25173998437821865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967006579041481 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10404991917312145 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2067300956696272 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1343800686299801 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38723903708159924 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2773778289556503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 2.9853128362447023 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 4.645359935238957 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08861999958753586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18478999845683575 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4337090067565441 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04030903801321983 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.074450159445405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08402997627854347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17196987755596638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047449953854084015 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015659956261515617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23229909129440784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07106992416083813 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5625571832060814 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320987060666084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16161007806658745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053649069741368294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09361002594232559 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19681011326611042 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08941907435655594 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25772000662982464 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104004129767418 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10370882228016853 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21403911523520947 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13266992755234241 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3931890241801739 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3105380348861217 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05090981721878052 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7292071133852005 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07555005140602589 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1650999765843153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3280290402472019 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03961985930800438 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06813998334109783 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734301283955574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598598901182413 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047699082642793655 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01556985080242157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2301300410181284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05937996320426464 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3972579035907984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07263896986842155 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16113882884383202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05419994704425335 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09301002137362957 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19481917843222618 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08943001739680767 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2582999877631664 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1030599232763052 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20748982205986977 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13246899470686913 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3867188934236765 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2918170541524887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05076010711491108 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7271661199629307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23166881874203682 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3220790531486273 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 13.105863006785512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13026013039052486 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22058002650737762 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052199000492691994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.328670023009181 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398498959839344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06851879879832268 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07256004028022289 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592698972672224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047479989007115364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.131980050355196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060129910707473755 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3635270297527313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07205014117062092 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15899981372058392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04206993617117405 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09172898717224598 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36761979572474957 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153899736702442 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24939002469182014 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902500469237566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303989984095097 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24561979807913303 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08816015906631947 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37943990901112556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.449817093089223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03963918425142765 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8400161061435938 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08672988042235374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1757501158863306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302990760654211 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039460137486457825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07238006219267845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15814905054867268 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671001806855202 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015629921108484268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13135001063346863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04878011532127857 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.197087811306119 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338891737163067 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602389384061098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04217005334794521 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09183003567159176 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3685189876705408 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978997357189655 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2472491469234228 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034015238285065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10363990440964699 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24566915817558765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08964003063738346 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3819689154624939 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4474268537014723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04029995761811733 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.840256154537201 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08001993410289288 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16988907009363174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04877988249063492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18040998838841915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0392301008105278 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06738980300724506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07265899330377579 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15804916620254517 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475000124424696 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015520025044679642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13148016296327114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043120002374053 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1369979474693537 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07141008973121643 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15734904445707798 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042839907109737396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09108986705541611 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3664891701191664 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08956994861364365 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24862890131771564 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09086006321012974 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10331999510526657 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24475017562508583 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08870917372405529 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3796089440584183 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4454671181738377 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039590056985616684 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8337960354983807 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24129892699420452 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3602688666433096 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.790800046175718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22253883071243763 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37858891300857067 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1035500317811966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18172012642025948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039329053834080696 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06842007860541344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08455989882349968 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17181993462145329 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04751002416014671 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08286908268928528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04363991320133209 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3687571045011282 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08413009345531464 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17204019241034985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03670994192361832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920190941542387 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.960801986977458 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134993888437748 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25975913740694523 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09055016562342644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.126910163089633 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3548990935087204 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06739981472492218 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4690589848905802 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.142188863828778 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034640077501535416 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.534088937565684 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16358005814254284 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2760200295597315 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07369997911155224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13226899318397045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039984196424484 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06666011177003384 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08349004201591015 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16895984299480915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047798966988921165 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01546996645629406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08298014290630817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03952020779252052 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1772681027650833 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0845098402351141 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1873502042144537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037149060517549515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09117997251451015 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.937202040106058 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09228009730577469 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2592999953776598 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09118998423218727 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1262498553842306 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3533388953655958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06683985702693462 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46609900891780853 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.116829881444573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03436999395489693 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.524978972971439 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12054899707436562 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23450888693332672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07468997500836849 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1316498965024948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039640115574002266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06751902401447296 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08428981527686119 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17038010992109776 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048250192776322365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01527019776403904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08303998038172722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03925897181034088 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1486269067972898 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08408003486692905 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17359992489218712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036599813029170036 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913599506020546 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.932771971449256 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09227916598320007 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2599898725748062 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017996490001678 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12592901475727558 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3539291210472584 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07043988443911076 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4865890368819237 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.131409969180822 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0346300657838583 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.524159176275134 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.507707959041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08884002454578876 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026690075173974037 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.19692908972501755 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 81.39908290468156 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.21214992739260197 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.07048994302749634 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.052199000492691994 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02635992132127285 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03748992457985878 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2834489569067955 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07692980580031872 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06606988608837128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15475996769964695 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810909740626812 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08340016938745975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03964989446103573 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0677797943353653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08475990034639835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17113890498876572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015639932826161385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08361996151506901 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9371880441904068 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.062030041590332985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1493000891059637 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03718910738825798 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0926000066101551 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.990472061559558 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205890819430351 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2655398566275835 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09590992704033852 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1287700142711401 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3582288045436144 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06904010660946369 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47391909174621105 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.190839990973473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03484007902443409 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.576878087595105 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08627003990113735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1746099442243576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047629931941628456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08336012251675129 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039558857679367065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06729015149176121 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08604000322520733 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17214985564351082 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734005779027939 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08368911221623421 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9476582054048777 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08456013165414333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17084996215999126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03684009425342083 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09184004738926888 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9453119970858097 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197904728353024 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.261849956586957 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177881292998791 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12874999083578587 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3572499845176935 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759888492524624 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47107902355492115 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.132419057190418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03475998528301716 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.523518892005086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0799400731921196 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10615005157887936 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 13.19959294050932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07584900595247746 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1669689081609249 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08137989789247513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039739999920129776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757979281246662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07292884401977062 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15903892926871777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0470699742436409 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01537008211016655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13408996164798737 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03350991755723953 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.031047897413373 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15951902605593204 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04257005639374256 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09058997966349125 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3615689929574728 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08994992822408676 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2502189017832279 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09058997966349125 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10368996299803257 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24533900432288647 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08934014476835728 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38019916974008083 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4431870076805353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039659906178712845 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8350558821111917 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731288455426693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16055908054113388 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04752003587782383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13273977674543858 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039659906178712845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760004907846451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730298925191164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162909971550107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761014133691788 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015600118786096573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13411999680101871 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.025608042255044 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310998626053333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15959003940224648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042530009523034096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09184004738926888 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37023890763521194 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08893013000488281 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26067905128002167 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09007006883621216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10441010817885399 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24612899869680405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08968007750809193 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38140895776450634 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4630069490522146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04016002640128136 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8549170345067978 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12972881086170673 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15705893747508526 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.007096963003278 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07585994899272919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677300315350294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1325700432062149 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975001163780689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0704699195921421 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369997911155224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598901581019163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2336690668016672 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03829994238913059 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1875980999320745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07383991032838821 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16237003728747368 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05371915176510811 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09204004891216755 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19744993187487125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015901014208794 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25692000053822994 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0911499373614788 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10332884266972542 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20824884995818138 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13378006406128407 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38808886893093586 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2939379084855318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051770126447081566 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.71235715970397 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07507996633648872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634990330785513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824995994567871 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22986996918916702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038460129871964455 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684899277985096 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07303990423679352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15919003635644913 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015200115740299225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23129885084927082 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2194269802421331 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07270998321473598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15985011123120785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05367887206375599 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254994802176952 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19816984422504902 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905089545994997 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25472999550402164 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028986096382141 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1045700628310442 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20856992341578007 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13304012827575207 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38711889646947384 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2916580308228731 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05151890218257904 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7061359249055386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22831978276371956 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2617000136524439 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.19045807980001 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07797894068062305 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16944902017712593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2308099064975977 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04006014205515385 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687700230628252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486995309591293 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16323011368513107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01564016565680504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2319288905709982 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2485471088439226 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07241987623274326 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158810056746006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23113912902772427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03916001878678799 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06841006688773632 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310020737349987 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588989980518818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048419926315546036 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23172982037067413 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2160479091107845 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5101150386035442 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07404899224638939 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16299891285598278 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048969872295856476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23016007617115974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039209844544529915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06899889558553696 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372000254690647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16075000166893005 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803994670510292 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01535983756184578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23101898841559887 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2216169852763414 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734901987016201 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16345013864338398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053690047934651375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09210919961333275 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18332991749048233 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09052990935742855 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.26507885195314884 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09150989353656769 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09690993465483189 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20125904120504856 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13391999527812004 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3828990738838911 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.286366954445839 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05115009844303131 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7062369734048843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435912266373634 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1625490840524435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04957010969519615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23010000586509705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03961916081607342 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06737001240253448 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07297005504369736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1583700068295002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803994670510292 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23216893896460533 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2201170902699232 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.251810954883695 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416005246341228 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16249995678663254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04758010618388653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42923889122903347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039289938285946846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06807991303503513 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344991900026798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601697877049446 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04819990135729313 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015729106962680817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2317698672413826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06926991045475006 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.50764686986804 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07262895815074444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596091315150261 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.44232909567654133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041740015149116516 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06764987483620644 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07329997606575489 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16016908921301365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873983561992645 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23141992278397083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06952998228371143 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5212460421025753 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609299797564745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047079985961318016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42949896305799484 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03910018131136894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06837002001702785 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07257005199790001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15822891145944595 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047969864681363106 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2311889547854662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06985990330576897 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5019569545984268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22805994376540184 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3084801137447357 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.999139113351703 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734599307179451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16135978512465954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04758010618388653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42966892942786217 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0400301069021225 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07408997043967247 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07381010800600052 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16073998995125294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712003283202648 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23166905157268047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0706501305103302 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5144571661949158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16058003529906273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053510069847106934 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09185913950204849 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19612000323832035 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08985004387795925 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25607901625335217 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029987268149853 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10325992479920387 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2081801649183035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1328799407929182 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38774916902184486 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2903069145977497 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05054008215665817 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7049170564860106 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473002187907696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16170903109014034 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04782993346452713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4290889482945204 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03962009213864803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06721983663737774 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355003617703915 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15868013724684715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01537008211016655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2317188773304224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06968993693590164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.499447040259838 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07257005199790001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611590851098299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05301996134221554 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09137019515037537 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1941800583153963 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08929986506700516 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25430996902287006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08987891487777233 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10228995233774185 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20554009824991226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13115000911056995 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38218009285628796 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2777280062437057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050549861043691635 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.690196106210351 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07523014210164547 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16346992924809456 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3275990020483732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03977981396019459 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670100562274456 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365993224084377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589590683579445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04765018820762634 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2299700863659382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058759935200214386 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.388906966894865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07236003875732422 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922007150948048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052819959819316864 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09243888780474663 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19403989426791668 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08925003930926323 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2694090362638235 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09174994193017483 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10270997881889343 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2070888876914978 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13171997852623463 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3840790595859289 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2977670412510633 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0499701127409935 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7078171949833632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23152981884777546 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31746900640428066 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.0416901987046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13082893565297127 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2200491726398468 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05238992162048817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32797898165881634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039869919419288635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06761006079614162 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07286015897989273 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15844008885324 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04696007817983627 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0157100148499012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13162009418010712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05958997644484043 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3534969184547663 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07273000665009022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599099487066269 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04206993617117405 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110989049077034 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3665788099169731 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08954992517828941 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25338889099657536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08943979628384113 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10979012586176395 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.251170014962554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08876901119947433 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3851591609418392 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4523069839924574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03967992961406708 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8417558167129755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09087007492780685 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1782400067895651 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048140063881874084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22922991774976254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03999006003141403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0668501015752554 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07240893319249153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575190108269453 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047549838200211525 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015409896150231361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1312799286097288 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04810001701116562 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1968580074608326 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07187901064753532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15820888802409172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190998151898384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09073992259800434 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3666188567876816 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927006274461746 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24746917188167572 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012012742459774 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1026899553835392 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24287006817758083 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08838903158903122 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3760890103876591 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4360768254846334 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039659906178712845 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.822745893150568 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07924996316432953 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16760989092290401 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04955008625984192 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18109986558556557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04294980317354202 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802985444664955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327902130782604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15860889106988907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04726019687950611 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015599885955452919 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13121985830366611 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04248996265232563 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1399879585951567 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07193908095359802 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15725893899798393 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042239902541041374 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914598349481821 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3676989581435919 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08989009074866772 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24911900982260704 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09035994298756123 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10271999053657055 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2429590094834566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08815992623567581 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3759688697755337 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.4410868752747774 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0395399983972311 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8265759572386742 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2411890309303999 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3601789940148592 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.754220023751259 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22149900905787945 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37535885348916054 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10305014438927174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18056901171803474 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040659913793206215 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0671599991619587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08749985136091709 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21792901679873466 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806998185813427 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08299993351101875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04345993511378765 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4089071191847324 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1810290850698948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03939005546271801 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0922901090234518 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9455618243664503 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252992458641529 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.25773909874260426 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09064003825187683 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12733996845781803 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35336893051862717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06706011481583118 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4657090175896883 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.121499998494983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033769989386200905 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.525108892470598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16357912681996822 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27522887103259563 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1313299871981144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039248960092663765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06692996248602867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08532986976206303 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17192005179822445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015770085155963898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08251913823187351 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038190046325325966 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.176806865260005 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08439994417130947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.171000137925148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0366398598998785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.091219088062644 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.976422129198909 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913599506020546 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2603291068226099 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126984514296055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1268899068236351 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35340897738933563 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06718980148434639 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46677887439727783 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.15298917889595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033800024539232254 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.54029899649322 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12121000327169895 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23429910652339458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07393001578748226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13104011304676533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04054908640682697 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06718002259731293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08407002314925194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16980012878775597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761992022395134 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015540048480033875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08267001248896122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03858981654047966 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1345669627189636 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08399016223847866 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1709300559014082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036590034142136574 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915199052542448 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.9156018756330013 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177997708320618 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2612899988889694 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902591273188591 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12579979375004768 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3526299260556698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06692903116345406 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.464628916233778 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 5.093669053167105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03403006121516228 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.482789129018784 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 20.493878051638603 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.089969951659441 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026140129193663597 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.127759063616395 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 78.71378911659122 ms for forwarding
