--------------------
No. 1
<class 'diffusers.models.embeddings.Timesteps'> take 0.3468599170446396 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.08942000567913055 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.06633996963500977 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.06917910650372505 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04592002369463444 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.3959687892347574 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 976.3288770336658 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2734989393502474 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.5010291934013367 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07427902892231941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1332398969680071 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04382012411952019 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11884002014994621 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10317005217075348 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19655004143714905 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051320064812898636 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.036490149796009064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09469897486269474 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5731658786535263 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07882993668317795 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17411005683243275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04039006307721138 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.12153922580182552 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 4.483930999413133 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10008015669882298 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.689751960337162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09475997649133205 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14230003580451012 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.4433488938957453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07616984657943249 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5770788993686438 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 9.293681010603905 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038010068237781525 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 9.747311007231474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09287986904382706 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18572993576526642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04999013617634773 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08993013761937618 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07183896377682686 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09390991181135178 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18827011808753014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05013006739318371 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01582014374434948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09146006777882576 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0082290973514318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09168009273707867 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1867399550974369 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03961985930800438 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09439000859856606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.686922136694193 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0958398450165987 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6828620359301567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09386008605360985 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13925996609032154 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37884898483753204 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07270998321473598 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5011491011828184 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.344762958586216 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037449877709150314 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.764742873609066 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08870009332895279 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.122518977150321 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 21.3627559132874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08011003956198692 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17826003022491932 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09037996642291546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039280159398913383 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07658894173800945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503991946578026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656601671129465 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05255010910332203 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14630891382694244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0342600978910923 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0947079863399267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07526017725467682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16458984464406967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04716985858976841 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09326916188001633 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36896998062729836 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09196903556585312 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.367969973012805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09161001071333885 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11183996684849262 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26269000954926014 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09627919644117355 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4085688851773739 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6164460685104132 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04368019290268421 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0274859853088856 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07674004882574081 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16873981803655624 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050168950110673904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14320015907287598 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04006992094218731 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0723500270396471 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07474911399185658 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16891909763216972 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04960992373526096 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01667998731136322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14447001740336418 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0854180436581373 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07478916086256504 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16549904830753803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04548998549580574 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09518000297248363 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37185894325375557 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218999184668064 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3672991879284382 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09436998516321182 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1131589524447918 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2624490298330784 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0970400869846344 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40877913124859333 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6265970189124346 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0436101108789444 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0387149415910244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14123995788395405 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1699700951576233 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.53067696839571 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07782992906868458 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1756199635565281 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051239971071481705 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1455999445170164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0413600355386734 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07983995601534843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07615005597472191 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16652909107506275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0502199400216341 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016349833458662033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25597005151212215 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04084990359842777 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2750981841236353 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07588998414576054 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16718008555471897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05733012221753597 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09480002336204052 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22404897026717663 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923799816519022 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22128899581730366 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09306007996201515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11017010547220707 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22077001631259918 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14524906873703003 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41661900468170643 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3302869629114866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0570099800825119 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7837071791291237 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07860013283789158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17337012104690075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05107908509671688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25144987739622593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04074978642165661 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0731500331312418 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07608998566865921 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16752001829445362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05097989924252033 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016949838027358055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2531090285629034 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.308186911046505 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751300249248743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.165530014783144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05734991282224655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09514880366623402 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22279005497694016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09305006824433804 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22044009529054165 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09244983084499836 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11046906001865864 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21913880482316017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1434099394828081 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.412538880482316 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.327096950262785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055099837481975555 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7587558832019567 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24991994723677635 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2780898939818144 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.51723681949079 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07890909910202026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17586909234523773 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05137990228831768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2531500067561865 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04349881783127785 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07379008457064629 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07821992039680481 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16899988986551762 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04995986819267273 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016520032659173012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2533290535211563 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.322397030889988 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07573002949357033 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16625993885099888 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051029957830905914 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516589593142271 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04029017873108387 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07174001075327396 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0764988362789154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1686890609562397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05121994763612747 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016039935871958733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25328993797302246 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3019070029258728 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.677974058315158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07711001671850681 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17181993462145329 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05076988600194454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2520389389246702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041300198063254356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07311999797821045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07726997137069702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1680299174040556 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051009003072977066 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016279984265565872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25494000874459743 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3124279212206602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07703015580773354 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.171249033883214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05738995969295502 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09625009261071682 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20808889530599117 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0978400930762291 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20731915719807148 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09257998317480087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10298006236553192 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21346984431147575 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14443905092775822 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40795886889100075 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3007170055061579 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05479017272591591 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7468859441578388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07764995098114014 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16930978745222092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05142996087670326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.252279918640852 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041079940274357796 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0724999699741602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07982994429767132 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16996008343994617 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05037011578679085 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016279984265565872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25247898884117603 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3075668830424547 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.530500853434205 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07830001413822174 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17157988622784615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049560097977519035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4712590016424656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04079006612300873 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07628998719155788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07620011456310749 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17458992078900337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0517391599714756 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016180099919438362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2534401137381792 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07627997547388077 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6327369958162308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07642991840839386 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16806996427476406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04985998384654522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4702191799879074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042987711727619 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0718599185347557 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482897490262985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1642489805817604 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05030003376305103 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01616007648408413 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.252370024099946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07827905938029289 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6108171548694372 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07641990669071674 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16699987463653088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4704589955508709 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040190061554312706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07283012382686138 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07505994290113449 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16357982531189919 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05049002356827259 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016300007700920105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25264895521104336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07598008960485458 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6061170026659966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2522990107536316 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.443778932094574 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.487149115651846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07758010178804398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17244997434318066 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04984904080629349 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47252001240849495 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04066992551088333 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07645017467439175 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07713004015386105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16696983948349953 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04982994869351387 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016330042853951454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25290902704000473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07904018275439739 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.626546960324049 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517985068261623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16791909001767635 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057129887863993645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09580003097653389 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2218990121036768 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09300978854298592 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22199004888534546 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0928191002458334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11019990779459476 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2208198420703411 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14581996947526932 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4181200638413429 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3342280872166157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056930119171738625 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7722260672599077 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07706996984779835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17524021677672863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050609931349754333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4705791361629963 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04076003096997738 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0734089408069849 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0761798582971096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656899694353342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04975008778274059 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01659989356994629 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527490723878145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07677008397877216 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6208169981837273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449020631611347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1647688914090395 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05755014717578888 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0940798781812191 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22046896629035473 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09174994193017483 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22171996533870697 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09155902080237865 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1102800015360117 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22010994143784046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14369003474712372 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41443901136517525 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3227469753473997 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05538994446396828 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7536459490656853 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07715006358921528 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17105997540056705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05015893839299679 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36114989779889584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04116981290280819 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07504899986088276 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.077039934694767 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17143995501101017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04971004091203213 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016429927200078964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25388901121914387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06449990905821323 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5167868696153164 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07646996527910233 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1683998852968216 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0567389652132988 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09431014768779278 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22128992713987827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218882769346237 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21888990886509418 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09185005910694599 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11052889749407768 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22059911862015724 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14437013305723667 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4149989690631628 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.322577940300107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05486002191901207 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7585060559213161 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527800388634205 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.343449879437685 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.62304899096489 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14193006791174412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23814896121621132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05755014717578888 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3613389562815428 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041109975427389145 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07519987411797047 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0760501716285944 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17049885354936123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049380119889974594 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016399892047047615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14509004540741444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06523006595671177 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4693369157612324 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07504015229642391 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16463897190988064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045350054278969765 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09437999688088894 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3727490548044443 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09359000250697136 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36918907426297665 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0911499373614788 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11367001570761204 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2650090027600527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09610014967620373 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41043898090720177 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6189869493246078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045249005779623985 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0305851940065622 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09948015213012695 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19263988360762596 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05045998841524124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521490678191185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040280167013406754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07835985161364079 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502897642552853 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16379915177822113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879990592598915 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016279984265565872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14468003064393997 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05230004899203777 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.290657790377736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08075986988842487 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17151981592178345 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04503992386162281 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09910017251968384 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36810897290706635 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09269011206924915 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3712091129273176 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09133992716670036 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11350889690220356 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2616590354591608 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09610992856323719 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4072489682585001 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6156770288944244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04359986633062363 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.031165873631835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0894500408321619 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18311990424990654 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05188002251088619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19942899234592915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040179817005991936 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07211999036371708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07495982572436333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637099776417017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979991354048252 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016479985788464546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1444290392100811 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04621990956366062 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2182679492980242 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07508997805416584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16801990568637848 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045250169932842255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09460980072617531 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3705490380525589 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09334995411336422 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.367569038644433 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09269011206924915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1121999230235815 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2610590308904648 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09609991684556007 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40659913793206215 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6144569963216782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045640161260962486 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0294261630624533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26061898097395897 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.38807885721325874 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.684428038075566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.237609026953578 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39756903424859047 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10726996697485447 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.20214891992509365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040950020775198936 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07203011773526669 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09468896314501762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18504890613257885 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04970002919435501 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016609905287623405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09086006321012974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04800991155207157 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4625969342887402 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09214901365339756 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1831590197980404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09549991227686405 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6864629946649075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09474996477365494 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6849931348115206 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09384006261825562 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1392001286149025 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37893885746598244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07293000817298889 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5020091775804758 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.34725284948945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037689926102757454 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.77701211720705 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17650891095399857 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2939689438790083 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07887999527156353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1461401116102934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04051998257637024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07254001684486866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09016995318233967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18007983453571796 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049999915063381195 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016320031136274338 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09038904681801796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04186015576124191 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2563171330839396 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09138998575508595 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18130987882614136 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039319973438978195 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09570899419486523 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6869330797344446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.096549978479743 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.678182139992714 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09487010538578033 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1396089792251587 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3779688850045204 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07314002141356468 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.500658992677927 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.345942944288254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03718980588018894 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.757963078096509 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13221008703112602 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2511001657694578 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07915007881820202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14497898519039154 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039929989725351334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07134000770747662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09075994603335857 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17897994257509708 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049419933930039406 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0159598421305418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04161009564995766 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2073279358446598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09039998985826969 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18054014071822166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09477999992668629 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6839121021330357 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09416998364031315 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6855030339211226 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09408988989889622 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1385691575706005 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3764890134334564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07294001989066601 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5041889380663633 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.342263987287879 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03716000355780125 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.754151873290539 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 30.46341799199581 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09325984865427017 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.028270063921809196 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 4.100132035091519 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 1081.1415847856551 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.2615600824356079 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.0987187959253788 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.05983980372548103 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.030110124498605728 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03916001878678799 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.332878902554512 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0887701753526926 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08085998706519604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17605978064239025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05034985952079296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09496998973190784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04123011603951454 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0759689137339592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09341002441942692 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1829999964684248 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04958990029990673 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01795007847249508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09176880121231079 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0221870616078377 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06990996189415455 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15950016677379608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09900005534291267 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6973231472074986 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09443983435630798 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.679602174088359 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0970400869846344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14086998999118805 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3848988562822342 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07434003055095673 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5100090056657791 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.371712872758508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038079917430877686 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.786133024841547 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09151012636721134 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18073990941047668 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08861999958753586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04043988883495331 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07123895920813084 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18028984777629375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015759840607643127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09054015390574932 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9821790736168623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09024003520607948 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.178568996489048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039040111005306244 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09300000965595245 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6859819665551186 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09234994649887085 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6809518933296204 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09163003414869308 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13908999972045422 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37684920243918896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07320987060666084 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4971991293132305 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.321942994371057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03660004585981369 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.727081818506122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08629006333649158 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11407001875340939 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.742259988561273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07667997851967812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16922899521887302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873005673289299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0877799466252327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07109018042683601 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472001016139984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16075978055596352 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048029934987425804 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015650177374482155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1443200744688511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033850083127617836 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.055316999554634 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546995766460896 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17939996905624866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04478008486330509 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09307009167969227 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.37651904858648777 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09278999641537666 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.366089167073369 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08993013761937618 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11187000200152397 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2623500768095255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1086799893528223 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4189999308437109 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6233669593930244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04333001561462879 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.043546177446842 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07653003558516502 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16557984054088593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0482101459056139 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14298898167908192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039419857785105705 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07137004286050797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400009781122208 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16024010255932808 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04997896030545235 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14446000568568707 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0583980474621058 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07328996434807777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16075000166893005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04491908475756645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09257998317480087 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3691590391099453 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905301421880722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36811898462474346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911010809242725 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11793989688158035 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26628910563886166 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09592995047569275 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4090790171176195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5980270691215992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04271022044122219 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.996285980567336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13971910811960697 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16620894894003868 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.426975829526782 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07750000804662704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1692699734121561 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1425000373274088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039259204640984535 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07449998520314693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07520988583564758 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16221008263528347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795985296368599 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2531388308852911 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04020007327198982 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.228878041729331 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436005398631096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16168993897736073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057108933106064796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0926600769162178 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2212801482528448 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09024911560118198 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2192300744354725 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09000999853014946 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11030910536646843 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21866895258426666 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14456012286245823 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.410629203543067 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3044378720223904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05461997352540493 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7322171479463577 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07585994899272919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16460008919239044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880991764366627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523288130760193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040289945900440216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07196003571152687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16159005463123322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822993651032448 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015238998457789421 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.252509955316782 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2854170054197311 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419008761644363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16179890371859074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05804002285003662 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233015589416027 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.222058966755867 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09014992974698544 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2191800158470869 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08971896022558212 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11024996638298035 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21842983551323414 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14255009591579437 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4081199876964092 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2997270096093416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05497992970049381 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7222859896719456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24868990294635296 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2748100087046623 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.347458111122251 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07759989239275455 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17562019638717175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04943995736539364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25199889205396175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03945012576878071 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07312977686524391 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524015381932259 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624799333512783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05048885941505432 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015389872714877129 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525800373405218 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2956580612808466 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467996329069138 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616289373487234 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04909001290798187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2512198407202959 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950996324419975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07142010144889355 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414002902805805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594298519194126 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014750054106116295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25204895064234734 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2620370835065842 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.601363928988576 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426994852721691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643600407987833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048930058255791664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2508291509002447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04373001866042614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07240008562803268 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435004226863384 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16230996698141098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04944019019603729 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2524300944060087 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2785180006176233 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486902177333832 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1654189545661211 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05686003714799881 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09263004176318645 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2071489579975605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09135017171502113 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2072600182145834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.091959023848176 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10585994459688663 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21345983259379864 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1439198385924101 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40498003363609314 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2785270810127258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055029988288879395 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7048961017280817 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07684994488954544 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1659700646996498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04930002614855766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2515988890081644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039479928091168404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07248995825648308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429881952702999 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16116886399686337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842015914618969 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015200115740299225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.252279918640852 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2746569700539112 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.362070932984352 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07608998566865921 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16628997400403023 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910002462565899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47011906281113625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04090997390449047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07320893928408623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07631001062691212 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16481010243296623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049800146371126175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015930039808154106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525090239942074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07563992403447628 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5993169508874416 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07596006616950035 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16552908346056938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0570700503885746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4698790144175291 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04091998562216759 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0722899567335844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465016096830368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16304990276694298 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015930039808154106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516398672014475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07601012475788593 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6051169950515032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07523014210164547 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16529904678463936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048999907448887825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47033908776938915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04021008498966694 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0771000050008297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506017573177814 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16411906108260155 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050129834562540054 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015689991414546967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25239004753530025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07582013495266438 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.601046184077859 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24940003640949726 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.33944007009267807 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.306169856339693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485994137823582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16479892656207085 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049309805035591125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4705090541392565 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04169996827840805 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0731500331312418 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076299998909235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16411906108260155 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922994412481785 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015689991414546967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25301985442638397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07685902528464794 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.60207599401474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749700702726841 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1639199908822775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05655991844832897 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09439000859856606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22041890770196915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21878909319639206 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125005453824997 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11001015082001686 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21958001889288425 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14360900968313217 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41150907054543495 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3115969486534595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05486002191901207 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.739386934787035 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07625995203852654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16581988893449306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913914017379284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4704489838331938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041040126234292984 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07264991290867329 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579987868666649 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17915992066264153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05119992420077324 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016499077901244164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2531500067561865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07691001519560814 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6242670826613903 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538893260061741 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645791344344616 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056969933211803436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09355996735394001 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22121891379356384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082001633942127 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21892995573580265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09147985838353634 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10950001887977123 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21730992011725903 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14277896843850613 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4064389504492283 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3047270476818085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05550007335841656 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.73345603980124 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07741013541817665 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1678699627518654 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04912004806101322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.360880047082901 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040329061448574066 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0714501366019249 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757100060582161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16467995010316372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894006997346878 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25243894197046757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0646600965410471 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4791171997785568 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07569906301796436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16507902182638645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05740020424127579 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09357999078929424 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22068899124860764 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09138998575508595 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21854019723832607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09237905032932758 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11041015386581421 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21856999956071377 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14333007857203484 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40968996472656727 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.308618113398552 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0542900525033474 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7463660333305597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2524901647120714 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34309993498027325 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.491159046068788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14071003533899784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23336010053753853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05695992149412632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36130892112851143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040929997339844704 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07202988490462303 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07470906712114811 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16191904433071613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01604994758963585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14444999396800995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06536999717354774 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4439679216593504 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579009979963303 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656091772019863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0452499371021986 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09426986798644066 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36860909312963486 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09255995973944664 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3672889433801174 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177997708320618 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11325906962156296 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2620290033519268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09663007222115993 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4069290589541197 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.606206875294447 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04292908124625683 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.011565025895834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09899004362523556 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18924986943602562 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05013006739318371 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25212904438376427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04133000038564205 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07164012640714645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07496005855500698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16219913959503174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14477991499006748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052179908379912376 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2715279590338469 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535913027822971 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16389903612434864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045179855078458786 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09334017522633076 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36805891431868076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169010445475578 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36767893470823765 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09204004891216755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11361902579665184 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26274891570210457 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09568990208208561 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4062890075147152 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6036671586334705 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04317006096243858 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0067959558218718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08621998131275177 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17703999765217304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051419949159026146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19766995683312416 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04109996370971203 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07106992416083813 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07512001320719719 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16243988648056984 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04926021210849285 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1442090142518282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045900000259280205 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1975679080933332 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16704993322491646 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044819898903369904 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09427010081708431 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36927987821400166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09116996079683304 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36800000816583633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909601803869009 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11207000352442265 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26107998564839363 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09517907164990902 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4046990070492029 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5996259171515703 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05083996802568436 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.039925893768668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2611991949379444 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3906188067048788 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.585588170215487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23698899894952774 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39629917591810226 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10727997869253159 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19919918850064278 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04134001210331917 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07211999036371708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09292992763221264 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1820798497647047 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0496390275657177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016219913959503174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09073014371097088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047110021114349365 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.444136956706643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09119999594986439 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18074898980557919 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03979005850851536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09414018131792545 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6783330142498016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09964918717741966 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.685612929984927 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09349011816084385 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13861991465091705 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3766501322388649 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07276888936758041 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4977190401405096 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.334733080118895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036949990317225456 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.744632126763463 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1756199635565281 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29179896228015423 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07883994840085506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14509004540741444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04145014099776745 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07138005457818508 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09183981455862522 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1799499150365591 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04941015504300594 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016069971024990082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09141000919044018 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04293886013329029 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2518670409917831 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1809599343687296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039350008592009544 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09419000707566738 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6891321651637554 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09460980072617531 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.683781949803233 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09309989400207996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13845995999872684 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37645897828042507 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07335003465414047 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.500058988109231 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.34201299585402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036899931728839874 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.751583052799106 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1282698940485716 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24484912864863873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07890001870691776 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14459993690252304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040519051253795624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07125991396605968 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0915799755603075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1794800627976656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04937988705933094 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01580989919602871 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09097880683839321 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04197005182504654 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1976768728345633 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09476998820900917 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18488988280296326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09409897029399872 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6875929217785597 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09454996325075626 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6846420262008905 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09193993173539639 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13900897465646267 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3768990281969309 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07300986908376217 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49794907681643963 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.336542872712016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03718002699315548 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.750042878091335 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 30.381247866898775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09483005851507187 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026219990104436874 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.12832996435463428 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 97.65274287201464 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.2037098165601492 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05113985389471054 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04169996827840805 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.0256500206887722 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03757001832127571 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25343988090753555 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07316982373595238 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06926897913217545 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15833904035389423 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050159869715571404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09025982581079006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04072999581694603 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07152999751269817 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09136996231973171 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17934897914528847 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04945998080074787 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016399892047047615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09139999747276306 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9803080465644598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06648898124694824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15551899559795856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038869911804795265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09408011101186275 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6877619568258524 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0940500758588314 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.689263015985489 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09388895705342293 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1390799880027771 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3773199860006571 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07247901521623135 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4963488318026066 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.339082822203636 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036899931728839874 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.740141987800598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09103980846703053 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18070009537041187 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936894401907921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08892989717423916 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07087015546858311 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09109987877309322 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17910986207425594 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04916009493172169 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015730038285255432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09067007340490818 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9820880368351936 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08985982276499271 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17827004194259644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03844010643661022 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09394995868206024 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.683143062517047 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0930598471313715 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.684262977913022 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0934000127017498 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13842899352312088 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37558889016509056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07229018956422806 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49393903464078903 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.32338398322463 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03660912625491619 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.726962143555284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08585001341998577 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11263997294008732 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.64832004159689 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0774699728935957 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17098011448979378 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04983879625797272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08748983964323997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040350016206502914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07166992872953415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16211997717618942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04904996603727341 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015789875760674477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1438299659639597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033319927752017975 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0602581314742565 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07380987517535686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16153906472027302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0441698357462883 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0931199174374342 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36693899892270565 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909601803869009 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.366369029507041 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09139999747276306 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11181901209056377 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25916891172528267 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09486009366810322 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40085893124341965 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.589257037267089 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04191999323666096 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9855562131851912 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0760199036449194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16360916197299957 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04951003938913345 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14262995682656765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04066992551088333 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07088901475071907 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498986087739468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16165990382432938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06273994222283363 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1445189118385315 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0715571697801352 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426016964018345 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616699155420065 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04425994120538235 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09168987162411213 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36939908750355244 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09173993021249771 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36649894900619984 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071012027561665 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1120399683713913 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2590799704194069 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09551993571221828 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4004589281976223 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5904169995337725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04234910011291504 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9870458636432886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1395398285239935 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16622012481093407 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.375226890668273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07614004425704479 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16674003563821316 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04954892210662365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14153006486594677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040740007534623146 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07233978249132633 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07557007484138012 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16331905499100685 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049519818276166916 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25246012955904007 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039490172639489174 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.227597938850522 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485994137823582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16280007548630238 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056159915402531624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09320001117885113 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22082915529608727 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09150011464953423 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2195888664573431 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069009684026241 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10998011566698551 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21711993031203747 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14381902292370796 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40934886783361435 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3024869840592146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05433009937405586 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7322758212685585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07606996223330498 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16564014367759228 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050170114263892174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2503001596778631 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04024989902973175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07138005457818508 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581990212202072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16295001842081547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04943995736539364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015669967979192734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25210902094841003 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.278006937354803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487996481359005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634999644011259 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056029995903372765 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09307893924415112 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22083008661866188 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090339919552207 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21727895364165306 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012012742459774 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10533980093896389 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2128190826624632 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13481988571584225 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39404910057783127 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2848570477217436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05230004899203777 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7052169423550367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23026904091238976 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2567288465797901 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.30420702509582 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07697008550167084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1690201461315155 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05077989771962166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23229909129440784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04020007327198982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06888015195727348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07615913636982441 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16340892761945724 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050140079110860825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015980098396539688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23351004347205162 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2435580138117075 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16182893887162209 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049900030717253685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23177987895905972 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04045991227030754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06856000982224941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0756201334297657 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624501310288906 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050430186092853546 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01543993130326271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23314892314374447 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2341670226305723 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5219048839062452 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747600570321083 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1640799455344677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050320057198405266 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23215892724692822 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04032021388411522 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07285992614924908 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07560895755887032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633288338780403 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05010003224015236 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015639932826161385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23392005823552608 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2417479883879423 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07569999434053898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16607996076345444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05388003773987293 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09323004633188248 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19607902504503727 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218999184668064 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19228900782763958 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917899888008833 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09782984852790833 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20464998669922352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13525900430977345 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38621900603175163 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2319970410317183 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05150004290044308 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6528870910406113 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0765498261898756 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1663400325924158 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05083903670310974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2327000256627798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040479935705661774 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0689199659973383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0760799739509821 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1642298884689808 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049999915063381195 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016110017895698547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23378897458314896 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2438071426004171 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.23517101444304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07660011760890484 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1658101100474596 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05024997517466545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43372903019189835 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04085991531610489 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06883894093334675 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07609999738633633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16397004947066307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04989001899957657 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23416918702423573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07081008516252041 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.533077098429203 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07738987915217876 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16909977421164513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049239955842494965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43276906944811344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04002009518444538 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06917910650372505 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481011562049389 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624799333512783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05005998536944389 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015599885955452919 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2337489277124405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07050996646285057 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5322568360716105 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07508997805416584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16323011368513107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048848800361156464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43299002572894096 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039458973333239555 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06889994256198406 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502012886106968 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1617299858480692 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05056988447904587 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23442902602255344 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07179984822869301 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5264370013028383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.230519101023674 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31349901109933853 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.0614201463758945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07569999434053898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16468996182084084 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04945998080074787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43386989273130894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040599144995212555 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06973999552428722 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483991794288158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1617399975657463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04902016371488571 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01576007343828678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23404904641211033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07154000923037529 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5269070863723755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483013905584812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16354909166693687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053480034694075584 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09373994544148445 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20499899983406067 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09024003520607948 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20342995412647724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09143911302089691 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10389997623860836 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20956015214323997 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1341700553894043 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39091985672712326 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2553581036627293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05198991857469082 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.674136146903038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0763600692152977 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16599008813500404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05025998689234257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4342789761722088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04065013490617275 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06912020035088062 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0764802098274231 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16406015492975712 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04966999404132366 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23394892923533916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07010996341705322 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5351870097219944 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07495004683732986 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.163649907335639 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05307001993060112 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09290920570492744 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2012099139392376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906200148165226 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1992490142583847 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082001633942127 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10255002416670322 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20882999524474144 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13122893869876862 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3856089897453785 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2400769628584385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05157012492418289 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6566170379519463 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07702014409005642 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17024902626872063 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04971981979906559 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32629002816975117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039850128814578056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07694889791309834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754999928176403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16336003318428993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910002462565899 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015780096873641014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.229179160669446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05965004675090313 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4214471448212862 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07464992813766003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16236002556979656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052579911425709724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09295018389821053 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20113005302846432 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09265006519854069 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1989291049540043 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09165983647108078 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1034601591527462 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2093091607093811 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1325700432062149 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3885289188474417 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2444981839507818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05091005004942417 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6603770200163126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23024901747703552 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3157290630042553 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.015120031312108 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13161008246243 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22316910326480865 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.054060015827417374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3261400852352381 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04135980270802975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06809900514781475 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07474981248378754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16214000061154366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015730038285255432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13171904720366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060519902035593987 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3725070748478174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465016096830368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16249995678663254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04225992597639561 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.093469163402915 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34715980291366577 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09165890514850616 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3449700307101011 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09208009578287601 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10397005826234818 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24583004415035248 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0897690188139677 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3818690311163664 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5320561360567808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04019984044134617 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.926095923408866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0923799816519022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18154014833271503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049579888582229614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22828998044133186 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04027015529572964 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06790016777813435 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438892498612404 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16076886095106602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0495400745421648 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015929806977510452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13189995661377907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04877988249063492 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2138080783188343 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412000559270382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16120006330311298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04243990406394005 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09287986904382706 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3476189449429512 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167008101940155 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34653907641768456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09108008816838264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10363897308707237 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24478905834257603 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08985004387795925 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3802790306508541 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5310770832002163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040289945900440216 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9238968379795551 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0814199447631836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17092889174818993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05124998278915882 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1793599221855402 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018982872366905 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06803893484175205 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481011562049389 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613700296729803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04932982847094536 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016070203855633736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13161893002688885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04320009611546993 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.148506999015808 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416005246341228 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16170996241271496 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042380066588521004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09265984408557415 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34797005355358124 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09074993431568146 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34619891084730625 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09131012484431267 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10409997776150703 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24780910462141037 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.090339919552207 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38569909520447254 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5367069281637669 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03985920920968056 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9307259935885668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23951008915901184 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3592199645936489 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.090939933434129 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22106990218162537 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37489994429051876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10442896746098995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1805000938475132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040929997339844704 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06865011528134346 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08463999256491661 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17286883667111397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05034985952079296 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016009900718927383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08282996714115143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043899985030293465 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3720379211008549 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08468003943562508 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17396011389791965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03674998879432678 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09394995868206024 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.577742027118802 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09372993372380733 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5718828439712524 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09355996735394001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1260989811271429 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3522690385580063 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06849016062915325 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.467248959466815 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.078953949734569 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034899916499853134 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.471652865409851 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16229995526373386 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2747001126408577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07498892955482006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1304401084780693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04107016138732433 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06769993342459202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08307909592986107 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17033913172781467 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04961993545293808 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015750061720609665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08260994218289852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038950005546212196 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1969278566539288 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08446886204183102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17410889267921448 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03711017780005932 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09267008863389492 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.577922936528921 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09388988837599754 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5818920005112886 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09284005500376225 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1260389108210802 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35299896262586117 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06783008575439453 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4668289329856634 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.085103938356042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03457977436482906 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.479472948238254 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11821999214589596 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23122993297874928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0752501655369997 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13027898967266083 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04082988016307354 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06780005060136318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08367002010345459 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17209001816809177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049419933930039406 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0822900328785181 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039800070226192474 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1455579660832882 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.084729865193367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17368001863360405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03709993325173855 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0935590360313654 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.578613046556711 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09393994696438313 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5803320351988077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09307987056672573 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12693018652498722 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35411911085247993 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06791995838284492 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46828901395201683 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.088982896879315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03472995012998581 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.482133038341999 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.377350118011236 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0893299002200365 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025640008971095085 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10821898467838764 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 94.82933790422976 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.2058800309896469 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04623015411198139 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04078890196979046 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026319874450564384 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03688992001116276 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2466889563947916 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06804009899497032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06693997420370579 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1554698683321476 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06566895171999931 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0841999426484108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04150997847318649 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08496991358697414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1732190139591694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04951003938913345 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016560079529881477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08366000838577747 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9691379964351654 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06320001557469368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1509899739176035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03702007234096527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0930901151150465 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5858629271388054 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09428896009922028 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5972329787909985 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09753997437655926 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12837001122534275 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3574490547180176 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06839982233941555 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4721088334918022 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.128753863275051 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034899916499853134 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.516242960467935 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0848288182169199 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17449911683797836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08121016435325146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040190061554312706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653997115790844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08493894711136818 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1716290134936571 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015059951692819595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08303998038172722 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9391980711370707 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08312007412314415 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17046905122697353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03703008405864239 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09215995669364929 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5802829079329967 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10283896699547768 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5866929683834314 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09445985779166222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12804986909031868 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3617089241743088 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07843994535505772 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4921089857816696 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.151524001732469 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03472017124295235 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.540862938389182 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07812888361513615 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10394910350441933 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.17361095547676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07758988067507744 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1701698638498783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048479996621608734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08026999421417713 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039668986573815346 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736000068485737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413001731038094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15867012552917004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13119890354573727 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031880103051662445 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0352081153541803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07500010542571545 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624098513275385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041930004954338074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104912169277668 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3467998467385769 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078904986381531 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.346579821780324 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08939881809055805 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1030901912599802 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24459999985992908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08866004645824432 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3782890271395445 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.521446043625474 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040079932659864426 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9123160745948553 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609299797564745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12974883429706097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039850128814578056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712018512189388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738298986107111 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15802006237208843 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047558918595314026 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13123988173902035 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0099380742758512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0729698222130537 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15777000226080418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042038969695568085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169988334178925 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34711905755102634 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08934992365539074 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3467290662229061 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08919998072087765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10271999053657055 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2435001078993082 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08932896889746189 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3766189329326153 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5161270275712013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03971997648477554 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9003760535269976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12668990530073643 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15250989235937595 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.113598123192787 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07620896212756634 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16546901315450668 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04864996299147606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12945989146828651 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915000706911087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06862008012831211 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15968899242579937 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015090219676494598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22871000692248344 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037539051845669746 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1704668868333101 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384992204606533 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16048992983996868 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05295989103615284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09196996688842773 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2005689311772585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006005711853504 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19991002045571804 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08906004950404167 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1022901851683855 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2063398715108633 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13288995251059532 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3837989643216133 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2295169290155172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05063996650278568 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.639016903936863 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07696007378399372 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16994006000459194 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04924996756017208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22688903845846653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06798002868890762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456913590431213 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15966896899044514 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0487098004668951 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22841989994049072 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.220368081703782 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07381988689303398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15994906425476074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053179915994405746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09203003719449043 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20152912475168705 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08940999396145344 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.199079979211092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915799755603075 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10324991308152676 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.207170145586133 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13131904415786266 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3831989597529173 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.233916962519288 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05116011016070843 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6418159939348698 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2251099795103073 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2512300852686167 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.023807916790247 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07661012932658195 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16826996579766273 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04951003938913345 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22678892128169537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039910199120640755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06717001087963581 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07448997348546982 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601900439709425 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05164998583495617 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015559839084744453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22887997329235077 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2327381409704685 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396982982754707 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15892996452748775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854891449213028 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22718985565006733 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03977981396019459 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0676899217069149 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07470906712114811 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15977909788489342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04966999404132366 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014960067346692085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22799987345933914 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2069379445165396 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4818950332701206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506995461881161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1627600286155939 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048729823902249336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2267090603709221 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963988274335861 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06746011786162853 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15911017544567585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015050172805786133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22814888507127762 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2074278201907873 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738899689167738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16256002709269524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05293893627822399 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0922598410397768 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1961500383913517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09003002196550369 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19073905423283577 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08990010246634483 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09719002991914749 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2026190049946308 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13415003195405006 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3880290314555168 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2240780051797628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050309812650084496 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.634777057915926 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0763691496104002 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16400893218815327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968000575900078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2272299025207758 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03936979919672012 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06713997572660446 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465016096830368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16022007912397385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04831003025174141 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22795889526605606 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2112569529563189 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.147812025621533 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502990774810314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16197003424167633 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755915142595768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4234600346535444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06873998790979385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15891995280981064 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014839926734566689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2284690272063017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06891018711030483 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4912369661033154 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0765589065849781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16322918236255646 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4264591261744499 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040079932659864426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06807013414800167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385993376374245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15912996605038643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048359157517552376 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015009893104434013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23016007617115974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06908993236720562 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.49787706322968 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07316889241337776 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598089002072811 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4261988215148449 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04005990922451019 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06892997771501541 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07409020327031612 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587399747222662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048008980229496956 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014979857951402664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22946996614336967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0691101886332035 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4920569956302643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22680917754769325 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30710920691490173 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.9410799983888865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07476983591914177 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16125012189149857 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047618988901376724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42661000043153763 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012905992567539 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06834999658167362 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15886989422142506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047759851440787315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23104902356863022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07150997407734394 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4981769490987062 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455004379153252 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16324990428984165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05259993486106396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923799816519022 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2020199317485094 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969893679022789 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20001991651952267 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08955993689596653 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10295910760760307 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20753894932568073 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13304012827575207 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3856089897453785 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2365079019218683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05233893170952797 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6611660830676556 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09227008558809757 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18728012219071388 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05228002555668354 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42736902832984924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04045991227030754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06906897760927677 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0763898715376854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16296003013849258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04853983409702778 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610000617802143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23054913617670536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07038982585072517 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.549507025629282 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420009933412075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16199005767703056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05318899638950825 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1065500546246767 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2061298582702875 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09151897393167019 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2000399399548769 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09137997403740883 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10345899499952793 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20972895435988903 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1325700432062149 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3886190243065357 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2806081213057041 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050770118832588196 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6943670343607664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07750000804662704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16738916747272015 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3283591940999031 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040560029447078705 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06884988397359848 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07559987716376781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16289995983242989 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04933006130158901 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015669967979192734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2299800980836153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05937996320426464 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4101369306445122 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494003511965275 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16266899183392525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05237990990281105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09272992610931396 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2025100402534008 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09128893725574017 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.199850182980299 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09224005043506622 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10443897917866707 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21512899547815323 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.134230125695467 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39539881981909275 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2536880094558 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051140086725354195 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6694560181349516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23046019487082958 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31664990819990635 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.027900105342269 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13101985678076744 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2230689860880375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05330005660653114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3284490667283535 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041109975427389145 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06892997771501541 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609788741916418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015930039808154106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13247993774712086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06050989031791687 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3730369973927736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398915477097034 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16126898117363453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04242011345922947 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09439000859856606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34810882061719894 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0916400458663702 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3471788950264454 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09166006930172443 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10420894250273705 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2466188743710518 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09045982733368874 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.384968938305974 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5408270992338657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041040126234292984 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9364859908819199 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09177997708320618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1863099168986082 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499701127409935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23012002930045128 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041090184822678566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06845989264547825 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494981400668621 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16255001537501812 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04845019429922104 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015990110114216805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13252906501293182 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0495698768645525 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.225857064127922 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738298986107111 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16116001643240452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042090192437171936 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09368918836116791 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.349970068782568 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09157904423773289 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3475199919193983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09159906767308712 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1037998590618372 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24598021991550922 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09039021097123623 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3847498446702957 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5411071944981813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04041008651256561 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9326359033584595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08189002983272076 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17135008238255978 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05110003985464573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1803790219128132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040929997339844704 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07092999294400215 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07630977779626846 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16330997459590435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04847906529903412 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015629921108484268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13195001520216465 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04337984137237072 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1680680327117443 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416005246341228 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16205012798309326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042389146983623505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09337998926639557 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3473889082670212 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909899827092886 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3469390794634819 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082001633942127 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10300008580088615 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24543004110455513 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08914899080991745 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38044899702072144 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5296570491045713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040140002965927124 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9235960207879543 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2398989163339138 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35974918864667416 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.138998972252011 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22170995362102985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3748789895325899 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10458007454872131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1808700617402792 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04126993007957935 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680901575833559 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08748983964323997 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1924200914800167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04937010817229748 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016150064766407013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08267001248896122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044119078665971756 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3921970967203379 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08461996912956238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17273984849452972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037079909816384315 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09344005957245827 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.589241998270154 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09544007480144501 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5928930155932903 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09829900227487087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12712995521724224 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35482016392052174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866897456347942 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47022895887494087 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.126372937113047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03442005254328251 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.518042974174023 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16354001127183437 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2753790467977524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13131997548043728 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040659913793206215 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06711995229125023 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08545001037418842 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17213006503880024 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04937010817229748 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01640990376472473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08324999362230301 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03924011252820492 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1875070631504059 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08468981832265854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17477991059422493 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036800047382712364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09379000402987003 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5974818747490644 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09459001012146473 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5959628876298666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09526894427835941 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12714997865259647 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3540799953043461 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06877887062728405 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46866899356245995 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.134812815114856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03479002043604851 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.531962987035513 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11974992230534554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23413891904056072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07551000453531742 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13055000454187393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04120008088648319 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736000068485737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08506001904606819 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.171988969668746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04963995888829231 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01574005000293255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08301995694637299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039239879697561264 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.14659802056849 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08495990186929703 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17424998804926872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03767991438508034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134993888437748 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5872329026460648 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10166899301111698 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5830428823828697 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072991088032722 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12662005610764027 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35620899870991707 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06910995580255985 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4714289680123329 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.110383991152048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03430992364883423 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.50124191492796 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.5205800794065 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09059999138116837 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024620210751891136 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11490005999803543 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.75708107836545 ms for forwarding
--------------------
No. 2
<class 'diffusers.models.embeddings.Timesteps'> take 0.24110893718898296 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05421997047960758 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04442990757524967 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03460003063082695 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03809994086623192 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2750600688159466 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.13029901310801506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08915993385016918 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2001600805670023 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05222996696829796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09681005030870438 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04094000905752182 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07662014104425907 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08707004599273205 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17811008729040623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049579888582229614 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01803017221391201 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08667889051139355 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0483178775757551 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747600570321083 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16543990932404995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039460137486457825 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09708898141980171 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.606912912800908 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09754998609423637 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.596212947741151 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11474988423287868 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13841013424098492 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.38351910188794136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06980006583034992 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.516749219968915 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.249012986198068 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035739969462156296 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.672202937304974 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08741882629692554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17811893485486507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0835200771689415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04001986235380173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06924010813236237 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08761906065046787 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1756290439516306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08514989167451859 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9619880001991987 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08586910553276539 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17421902157366276 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03805011510848999 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09369989857077599 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.587832907214761 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09404891170561314 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5965528804808855 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09801005944609642 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13086898252367973 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3623089287430048 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0702801626175642 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4804490599781275 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.14665388315916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03592995926737785 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.54506203904748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08114008232951164 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10874983854591846 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.4530610460788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07920991629362106 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17356988973915577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049519818276166916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08217897266149521 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039490172639489174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07452000863850117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08907984010875225 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18450990319252014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047449953854084015 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015729805454611778 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1327299978584051 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03290013410151005 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0838380549103022 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07259007543325424 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16068993136286736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04228902980685234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09810994379222393 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36072893999516964 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069009684026241 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34749903716146946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0924400519579649 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10319985449314117 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24648010730743408 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09010918438434601 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38678920827805996 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.565277110785246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04144012928009033 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.960646128281951 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502990774810314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637099776417017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048108864575624466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13161008246243 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04021008498966694 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06857002153992653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426994852721691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613488420844078 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703993909060955 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13288995251059532 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.028458122164011 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16250880435109138 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042869942262768745 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09955000132322311 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3611990250647068 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913301482796669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3488990478217602 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10512000881135464 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10866904631257057 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25715911760926247 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09107985533773899 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3988489042967558 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5954370610415936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040770042687654495 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.993515994399786 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12975884601473808 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15775905922055244 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.33249687962234 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0765800941735506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17111003398895264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936009645462036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311791129410267 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903987817466259 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07335003465414047 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07626996375620365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637700479477644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04764879122376442 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23155007511377335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03881985321640968 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1978079564869404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467903196811676 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1625390723347664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05394988693296909 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923199113458395 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20393985323607922 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094993583858013 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.203409930691123 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1085200347006321 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10343990288674831 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2105699386447668 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1342699397355318 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3933089319616556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.284108031541109 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05186884663999081 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7044558189809322 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07601012475788593 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16859988681972027 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05825003609061241 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2329589333385229 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903987817466259 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07007014937698841 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482990622520447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1619898248463869 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01549883745610714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2313100267201662 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2585679069161415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444014772772789 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16261893324553967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05425000563263893 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09293993934988976 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2049801405519247 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071012027561665 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2008399460464716 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972990326583385 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10399986058473587 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2091098576784134 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13411999680101871 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3904190380126238 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2523580808192492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05141901783645153 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.66926602832973 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22739986889064312 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25700987316668034 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.197607843205333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09207893162965775 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19238912500441074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22990000434219837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03912998363375664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06981007754802704 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075930031016469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1638699322938919 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2315291203558445 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2614370789378881 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234000213444233 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843901842832565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.230009900406003 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04030996933579445 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06887014023959637 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518008351325989 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234884969890118 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873005673289299 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23169000633060932 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.227326923981309 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5378239806741476 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546995766460896 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16656005755066872 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499701127409935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23043900728225708 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04027015529572964 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06966991350054741 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07528997957706451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16440008766949177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049480004236102104 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01599895767867565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2312101423740387 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2432180810719728 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09164982475340366 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19861897453665733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05795015022158623 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09429990313947201 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19983993843197823 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09291013702750206 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19434001296758652 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202002547681332 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09669899009168148 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20455894991755486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13345014303922653 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3868688363581896 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2461480218917131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05113985389471054 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.710586016997695 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09291991591453552 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18480001017451286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05122995935380459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23111887276172638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040789833292365074 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06908015348017216 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07615005597472191 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16470998525619507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049898866564035416 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016059959307312965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2316199243068695 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2664080131798983 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.328822018578649 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0770490150898695 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16900897026062012 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42547890916466713 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399099662899971 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0703099649399519 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07526995614171028 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1638690009713173 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04975008778274059 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015759840607643127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22996007464826107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07035909220576286 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5259059146046638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548998109996319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16497005708515644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894984886050224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4244691226631403 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06859982386231422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07595005445182323 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16504991799592972 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04994985647499561 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015470199286937714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23638899438083172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07086992263793945 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.527976943179965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07514981552958488 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16470998525619507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04940014332532883 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4243990406394005 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04021008498966694 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06921892054378986 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480988278985023 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16307015903294086 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04995986819267273 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015619909390807152 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2291589044034481 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07025990635156631 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5181570779532194 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22649881429970264 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3236092161387205 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.058539099991322 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07643015123903751 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1666999887675047 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04921993240714073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42528891935944557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040300190448760986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07048016414046288 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07537007331848145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1647390890866518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894984886050224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583993434906006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2294299192726612 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0715798232704401 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5226269606500864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08369004353880882 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1734299585223198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05382997915148735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09450898505747318 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2022299449890852 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09267986752092838 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20032888278365135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09187008254230022 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10320008732378483 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2106688916683197 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1320699229836464 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3935790155082941 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2597569730132818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05194009281694889 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.691877143457532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0767088495194912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1666489988565445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04983996041119099 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4253790248185396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048179877921938896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0763298012316227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07584900595247746 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16363896429538727 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04861992783844471 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015759840607643127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2296001184731722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07163896225392818 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5443561132997274 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08960999548435211 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19400985911488533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053690047934651375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09375903755426407 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.202300027012825 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09166006930172443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21315901540219784 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10222988203167915 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20955991931259632 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1315898261964321 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38984883576631546 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2653071898967028 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0521601177752018 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7169469501823187 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07751979865133762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16822898760437965 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05204998888075352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3306199796497822 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040588900446891785 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06906990893185139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07651001214981079 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16478006727993488 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049530062824487686 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01576007343828678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2301689237356186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06002001464366913 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4291268307715654 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07577985525131226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1652499195188284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05328888073563576 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0927802175283432 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2017600927501917 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09164889343082905 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19993004389107227 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134993888437748 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10320008732378483 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21058903075754642 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1328501384705305 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39177900180220604 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2529578525573015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05958019755780697 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6834968701004982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2312101423740387 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31804898753762245 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.130809852853417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1315490808337927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22369902580976486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0528399832546711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3272600006312132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040329061448574066 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06915000267326832 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07553002797067165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16335002146661282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863017238676548 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016110017895698547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13238913379609585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06086984649300575 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3799469452351332 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385015487670898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16256002709269524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043040141463279724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10980921797454357 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.35838992334902287 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253015741705894 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3470799420028925 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.109479995444417 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10379008017480373 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24743005633354187 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0898800790309906 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38827909156680107 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6039460897445679 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041579827666282654 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0043961703777313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09335996583104134 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2011200413107872 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04952889867126942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22952980361878872 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04105991683900356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866990588605404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07526995614171028 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16292883083224297 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048629939556121826 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015949830412864685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1326201017946005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050179893150925636 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2432669755071402 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357983849942684 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16265013255178928 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04316004924476147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09384006261825562 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34749903716146946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09275996126234531 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3480489831417799 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09159999899566174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10532001033425331 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24879002012312412 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0896600540727377 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3860390279442072 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.543886959552765 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04139915108680725 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.944956136867404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0928700901567936 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1932100858539343 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050659989938139915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17961906269192696 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04058005288243294 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06844988092780113 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07383991032838821 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16996008343994617 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928978160023689 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016110017895698547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1326599158346653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043659936636686325 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1892179027199745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1639390829950571 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04337006248533726 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09360001422464848 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3482790198177099 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09196996688842773 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3459989093244076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167985990643501 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10322011075913906 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24647009558975697 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0895787961781025 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38670911453664303 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.549737062305212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044640153646469116 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.970726065337658 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23978902027010918 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.36584888584911823 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.326649062335491 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22182893007993698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3780790138989687 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10672002099454403 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18239999189972878 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04031904973089695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06935000419616699 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08826004341244698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17844978719949722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05134986713528633 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01604994758963585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08385907858610153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04566996358335018 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.409557182341814 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08508004248142242 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17553986981511116 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038040103390812874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0950091052800417 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5778330639004707 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09609013795852661 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.583823097869754 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09459001012146473 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12727989815175533 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3553291317075491 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06806990131735802 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47149905003607273 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.10645311139524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03515998832881451 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.50654300302267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16509904526174068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2796091139316559 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07537007331848145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13696006499230862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.043138861656188965 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850995123386383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0841100700199604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1737398561090231 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05003996193408966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016200123354792595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0833489466458559 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03944011405110359 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2162269558757544 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08475990034639835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1747598871588707 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03730994649231434 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09487895295023918 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.58442310243845 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10302010923624039 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.603382036089897 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09353994391858578 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.127479899674654 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.36205886863172054 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06903009489178658 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4806090146303177 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.150542853400111 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03545987419784069 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.548473007977009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12119906023144722 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23641902953386307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07470999844372272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1318901777267456 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04288996569812298 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06794999353587627 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08383998647332191 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17169001512229443 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016249949112534523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08406885899603367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03969995304942131 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1621469166129827 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.085500068962574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17499993555247784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0380899291485548 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09499909356236458 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5991629119962454 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11692987754940987 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5993820056319237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09342003613710403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1269201748073101 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3621091600507498 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07255980744957924 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4837089218199253 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.173282956704497 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03557000309228897 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.571782847866416 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.648509807884693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0863000750541687 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026518944650888443 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15089986845850945 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 95.40924709290266 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20109000615775585 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05301996134221554 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.042300205677747726 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026110094040632248 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03782007843255997 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25628996081650257 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07172999903559685 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07088901475071907 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1713989768177271 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05962001159787178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08694012649357319 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04158006049692631 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850995123386383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08493009954690933 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1739689614623785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050350092351436615 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016619917005300522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0830900389701128 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9889982175081968 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06794906221330166 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16696914099156857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190998151898384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09454996325075626 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.587221959605813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09439000859856606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5861728247255087 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09411992505192757 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1278489362448454 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35743904300034046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0689199659973383 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4735388793051243 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.116014068946242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0349399633705616 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.534792112186551 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08609984070062637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17652008682489395 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049629947170615196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14044996351003647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04283897578716278 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06827991455793381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08633011020720005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17410004511475563 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049110036343336105 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08307886309921741 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.020967960357666 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08471985347568989 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17256010323762894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03690994344651699 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09387009777128696 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.57439205981791 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09393994696438313 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5785729996860027 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09315903298556805 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12777000665664673 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35473983734846115 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802007555961609 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46923011541366577 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.086042944341898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034739961847662926 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.480913005769253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07872004061937332 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10541989468038082 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.236561143770814 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0773801002651453 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1701901201158762 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05013006739318371 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0806800089776516 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040319981053471565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06813020445406437 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07493002340197563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161309028044343 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015689991414546967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13205991126596928 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032400013878941536 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0385778732597828 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482990622520447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16239890828728676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04241010174155235 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09277998469769955 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34786900505423546 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920100137591362 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3465288318693638 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149010293185711 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10378006845712662 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24627987295389175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09011896327137947 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38331886753439903 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5357369557023048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04175002686679363 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9339059945195913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07500010542571545 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16434001736342907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048759160563349724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13115000911056995 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04088995046913624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06796000525355339 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548904977738857 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16221916303038597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13195001520216465 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0307678021490574 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07451907731592655 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234908252954483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04263990558683872 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09242002852261066 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3483891487121582 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0964500941336155 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34721894189715385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09083980694413185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10358006693422794 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2459490206092596 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08918996900320053 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3810189664363861 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5380268450826406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040040118619799614 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9314561504870653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1277888659387827 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15413900837302208 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.1935370322316885 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07682992145419121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16826996579766273 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049999915063381195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13078912161290646 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05526002496480942 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07066014222800732 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07614982314407825 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16384990885853767 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04953984171152115 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558009535074234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23144995793700218 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03806012682616711 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2126679066568613 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406994700431824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16212882474064827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05365000106394291 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09365007281303406 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20205997861921787 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09208987466990948 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2013097982853651 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09103002957999706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10274909436702728 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2098088152706623 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13391999527812004 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38944906555116177 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2498178984969854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05110003985464573 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6654559876769781 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07710978388786316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1669200137257576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05043996497988701 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22881897166371346 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04118005745112896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850017234683037 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579987868666649 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16323011368513107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050029950216412544 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23026904091238976 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2355581857264042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747600570321083 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1641400158405304 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05347002297639847 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09570014663040638 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20294985733926296 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09087915532290936 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2001600805670023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153992868959904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10352907702326775 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20904908888041973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13271020725369453 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3881391603499651 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2502078898251057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05175010301172733 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.676016952842474 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22742990404367447 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25428994558751583 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.148978136479855 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0782401766628027 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17115892842411995 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0506499782204628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22931001149117947 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04078005440533161 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06884988397359848 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07575005292892456 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16330997459590435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04996010102331638 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2301391214132309 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2400969862937927 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07464014925062656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16225012950599194 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05073985084891319 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22932887077331543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040629878640174866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0685201957821846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1641188282519579 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04971004091203213 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015390105545520782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2308699768036604 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2316580396145582 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.515394939109683 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07609999738633633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1665288582444191 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05011982284486294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22895983420312405 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040699029341340065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843009032309055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498008199036121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16256002709269524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04997989162802696 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015689991414546967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23014890030026436 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2343269772827625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518008351325989 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1655900850892067 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053530093282461166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0940198078751564 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19809999503195286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09264983236789703 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19360892474651337 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0919201411306858 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09591993875801563 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20248908549547195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13347994536161423 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3821288701146841 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.235818024724722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05050003528594971 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6561869997531176 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07757986895740032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16743899323046207 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05003996193408966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22903992794454098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04090997390449047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08965004235506058 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17860997468233109 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04983996041119099 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2309789415448904 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2519771698862314 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.240131005644798 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07645995356142521 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16520987264811993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4272491205483675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.042380066588521004 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687700230628252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757100060582161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630098558962345 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05000992678105831 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610000617802143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23061898536980152 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06989995017647743 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5232469886541367 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482990622520447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.163388904184103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04868977703154087 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42631919495761395 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040250131860375404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06841006688773632 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465016096830368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16152998432517052 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05019921809434891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23227999918162823 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06989017128944397 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5141970943659544 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444992661476135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16355887055397034 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04899012856185436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42700907215476036 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04170997999608517 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06867013871669769 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07522013038396835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1625490840524435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499398447573185 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22974004969000816 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0699101947247982 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5154369175434113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22577005438506603 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30729989521205425 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.015650065615773 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07598893716931343 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16610906459391117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04836008884012699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4240290727466345 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04080985672771931 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06876979023218155 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07511000148952007 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16219005919992924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049720052629709244 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01563900150358677 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22932002320885658 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06995000876486301 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5121770557016134 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07743900641798973 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16890885308384895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052480027079582214 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09371992200613022 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20085996948182583 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09132991544902325 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19928999245166779 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126006625592709 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10575982742011547 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21349010057747364 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13192999176681042 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3927289508283138 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2501580640673637 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051249051466584206 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6724360175430775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07621990516781807 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16538985073566437 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049789901822805405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42394897900521755 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04118005745112896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06886990740895271 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07542013190686703 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628398895263672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04940992221236229 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0156499445438385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22907904349267483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07039005868136883 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5186769887804985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07470999844372272 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16258005052804947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05276012234389782 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0937699805945158 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20106998272240162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09129918180406094 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1995200291275978 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169010445475578 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10275887325406075 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2090290654450655 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13168994337320328 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38685882464051247 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2433179654181004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05194987170398235 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6609271988272667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07765018381178379 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677300315350294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873005673289299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32605906017124653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040350016206502914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06832997314631939 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07543014362454414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16158982180058956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04935893230140209 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015609897673130035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22897007875144482 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05944003351032734 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4040081296116114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489905692636967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16292906366288662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052339863032102585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09386008605360985 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2009700983762741 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09215017780661583 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19925017841160297 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09175017476081848 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10295002721250057 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2084991429001093 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13224012218415737 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3866488113999367 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2425279710441828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05104998126626015 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6584359109401703 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2300997730344534 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3162999637424946 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.959229966625571 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13148901052773 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2227691002190113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05400017835199833 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32690889202058315 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.042250147089362144 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820005364716053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16236002556979656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949886351823807 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015930039808154106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1318801660090685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060140155255794525 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.376367174088955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747989397495985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623891294002533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04216982051730156 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0941101461648941 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3456489648669958 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0927201472222805 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34576887264847755 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09331991896033287 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10302988812327385 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2446791622787714 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08950009942054749 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3803391009569168 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5373569913208485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040389830246567726 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.933956053107977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09030895307660103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18099904991686344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04945998080074787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22797007113695145 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04120916128158569 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06823008880019188 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599099487066269 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1328890211880207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050449976697564125 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.213017152622342 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0795100349932909 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16819010488688946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04222989082336426 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09298999793827534 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3468201030045748 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09121885523200035 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34726993180811405 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112898260354996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10384991765022278 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24616997689008713 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0899098813533783 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3816599491983652 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5301569364964962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04008994437754154 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9299660343676805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08110981434583664 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17096009105443954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050720060244202614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17943908460438251 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04103011451661587 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06819004192948341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07520010694861412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16341009177267551 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04965905100107193 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015719793736934662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13119983486831188 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04323013126850128 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1550281196832657 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07507996633648872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624098513275385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042279018089175224 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09258999489247799 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3457488492131233 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09215995669364929 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3456489648669958 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122979827225208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10308017954230309 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24581002071499825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0892390962690115 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38076890632510185 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5288570430129766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040079932659864426 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9209960009902716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23917900398373604 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3586588427424431 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.10350906290114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22068992257118225 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37510902620851994 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10413001291453838 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1804698258638382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04051998257637024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765010766685009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08610007353127003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17464999109506607 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04994007758796215 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015840167179703712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0824101734906435 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043899985030293465 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3722279109060764 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08573988452553749 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17746002413332462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0363299623131752 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09453995153307915 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.584102028980851 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0971001572906971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.580672899261117 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0910500530153513 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12631993740797043 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35220012068748474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06766989827156067 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46511879190802574 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.099674014374614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03436999395489693 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.494033012539148 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16400008462369442 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27709989808499813 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.079768942669034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13221986591815948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06758002564311028 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08350005373358727 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16852887347340584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048789894208312035 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08308002725243568 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0385199673473835 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1817580088973045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08509913459420204 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17153890803456306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03603007644414902 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09208987466990948 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.578663105145097 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0916698481887579 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.588993102312088 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0951599795371294 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12706895358860493 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3544690553098917 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802985444664955 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46706898137927055 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.097403915598989 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03471993841230869 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.48301313817501 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12091011740267277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23370003327727318 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07441989146173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13067899271845818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03988994285464287 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06774021312594414 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08394988253712654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16959011554718018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770886152982712 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08225999772548676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03940984606742859 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.129798125475645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08414988406002522 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16978010535240173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03627990372478962 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09139999747276306 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.593832952901721 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09182002395391464 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5905318800359964 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10952004231512547 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1308999489992857 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3606791142374277 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067500164732337 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4752089735120535 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.140074089169502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0338901299983263 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.524113101884723 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.41113105043769 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08739903569221497 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02511986531317234 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11403998360037804 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 94.01758993044496 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20209886133670807 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04855985753238201 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03996002487838268 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02496992237865925 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03605009987950325 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2423301339149475 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06876001134514809 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06631016731262207 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15372014604508877 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04895986057817936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08307001553475857 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978004679083824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06714998744428158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08475990034639835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1701698638498783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015829922631382942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08277012966573238 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9313379414379597 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06264913827180862 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.149149214848876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03686011768877506 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09285006672143936 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.593832952901721 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09303889237344265 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5903227981179953 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09498000144958496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12788898311555386 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3559091128408909 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06825011223554611 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4690689966082573 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.119303965941072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03362004645168781 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.504402125254273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0841398723423481 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17194007523357868 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08108001202344894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03968016244471073 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759911775588989 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08458993397653103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17040991224348545 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04799012094736099 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08224998600780964 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.937469070777297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08312007412314415 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16900897026062012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036349985748529434 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09272992610931396 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5715620033442974 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09220000356435776 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5715328995138407 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09105983190238476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1263690646737814 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3533188719302416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802985444664955 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4654990043491125 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.063373854383826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03403006121516228 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.447241969406605 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07845996879041195 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10342011228203773 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.024891080334783 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07770000956952572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16930000856518745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08071889169514179 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04129018634557724 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06900005973875523 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414980791509151 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15977001748979092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04753982648253441 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015409896150231361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311290543526411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03151991404592991 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.027467893436551 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0728401355445385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15829992480576038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041839899495244026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09147892706096172 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3467099741101265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912920020520687 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34446013160049915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08857902139425278 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10221987031400204 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24423981085419655 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0890498049557209 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3772799391299486 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.512656919658184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03935885615646839 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8980458844453096 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074049923568964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.160320196300745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047620153054594994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12986990623176098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039378879591822624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760004907846451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07761991582810879 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16196002252399921 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04684994928538799 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830147847533226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13190903700888157 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0123380925506353 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07218006066977978 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15681004151701927 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041590072214603424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10582013055682182 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3477588761597872 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09068986400961876 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34587900154292583 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911010809242725 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1023199874907732 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24317996576428413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08888007141649723 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3762999549508095 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5321369282901287 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03957911394536495 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9151659216731787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12727011926472187 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15307008288800716 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.106257904320955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07563992403447628 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16443990170955658 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900014027953148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12953998520970345 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039010075852274895 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843009032309055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481896318495274 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609688624739647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840991459786892 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22934004664421082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03706011921167374 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1708468664437532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407995872199535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16011996194720268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052579911425709724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912700779736042 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2012890763580799 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016995318233967 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21060905419290066 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09412015788257122 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10310020297765732 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20749005489051342 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13295887038111687 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3849088679999113 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2628771364688873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05085999146103859 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.672287005931139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07588998414576054 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16391021199524403 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04935893230140209 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2287400420755148 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04503014497458935 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0792602077126503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07404899224638939 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15980913303792477 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854006692767143 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22859987802803516 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2404380831867456 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15940004959702492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0527501106262207 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09158998727798462 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19984901882708073 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028986096382141 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19873003475368023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08966890163719654 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10338006541132927 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20707002840936184 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1322000753134489 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3846399486064911 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2288480065762997 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050579896196722984 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.636005938053131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2258701715618372 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2509199548512697 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.070307921618223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08388888090848923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17490889877080917 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968000575900078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22754003293812275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03920006565749645 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06788899190723896 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517985068261623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16205990687012672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049659982323646545 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22850907407701015 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2266070116311312 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07440010085701942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15992997214198112 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04897010512650013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22691907361149788 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039449892938137054 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728013977408409 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459917105734348 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16057887114584446 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014939811080694199 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22869999520480633 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.207438064739108 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.476415131241083 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07439008913934231 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628801692277193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22704899311065674 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06739003583788872 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739898532629013 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15928992070257664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871003329753876 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014679040759801865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22799009457230568 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2090879026800394 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07448997348546982 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633490901440382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05310005508363247 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10982993990182877 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1979998778551817 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915199052542448 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1913399901241064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893501564860344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09625894017517567 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20074890926480293 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13267993927001953 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37785922177135944 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2352578341960907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05061994306743145 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6498870681971312 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566018030047417 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16240007244050503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04939991049468517 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22702990099787712 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039668986573815346 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06756000220775604 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363990880548954 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15861005522310734 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794984124600887 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22871908731758595 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2073870748281479 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.160481970757246 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227993182837963 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048320041969418526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4234400112181902 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03926991485059261 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06862892769277096 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742301344871521 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15980005264282227 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04833005368709564 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01488998532295227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2285689115524292 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06964989006519318 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5000170096755028 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07721991278231144 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16599008813500404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767882637679577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42302999645471573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039109960198402405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750994361937046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07300986908376217 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1573499757796526 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048190122470259666 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01488998532295227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22832886315882206 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06867991760373116 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4876569621264935 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393001578748226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15909899957478046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047289999201893806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4230090416967869 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906991332769394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06752018816769123 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073309987783432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15833997167646885 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0485291238874197 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014749821275472641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22804015316069126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06890017539262772 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.48342689499259 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22480986081063747 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3052989486604929 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.928250098600984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517985068261623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16160891391336918 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04818011075258255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42447890155017376 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06900983862578869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746001023799181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15978002920746803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806998185813427 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014859950169920921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22860011085867882 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0698501244187355 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4958169776946306 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07397006265819073 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1603888813406229 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05240016616880894 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09261001832783222 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20105019211769104 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998904377222061 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1986201386898756 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0887999776750803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.103008933365345 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20701903849840164 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13144989497959614 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3830788191407919 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2281779199838638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05047000013291836 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6356168780475855 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07551000453531742 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16267015598714352 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840991459786892 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4234591033309698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918004222214222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06767013110220432 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15906919725239277 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015020137652754784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22934004664421082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07117004133760929 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.493627903982997 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386994548141956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067991964519024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05260016769170761 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09170989505946636 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2010988537222147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998997509479523 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1995700877159834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09356997907161713 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10230997577309608 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20628003403544426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13194908387959003 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3820790443569422 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2340769171714783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050789909437298775 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6425671055912971 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07588998414576054 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16307015903294086 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048069050535559654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32550981268286705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038640107959508896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678489450365305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1590498723089695 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2287889365106821 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05884002894163132 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3825071509927511 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16014999710023403 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05246000364422798 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122001938521862 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20104018040001392 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09706011041998863 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19880011677742004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08937995880842209 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10255002416670322 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20686909556388855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13210996985435486 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38414914160966873 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2344680726528168 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050980132073163986 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6427668742835522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2294389996677637 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3137590829282999 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.81490989215672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13048015534877777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22036908194422722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052159884944558144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32610981725156307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03998982720077038 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06811902858316898 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09540002793073654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19301008433103561 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.054800184443593025 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13159913942217827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05943002179265022 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3971570879220963 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07389998063445091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587499864399433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04191999323666096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09176903404295444 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34690997563302517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08985912427306175 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3463400062173605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09184889495372772 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10361010208725929 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2443299163132906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08866004645824432 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3764401189982891 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.519456971436739 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040560029447078705 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9065758679062128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0865499023348093 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1746700145304203 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22848905064165592 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03957003355026245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742984987795353 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357005961239338 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15887897461652756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742015153169632 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1316999550908804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04843994975090027 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1904779821634293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07260916754603386 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15817885287106037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0417300034314394 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09088008664548397 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34734909422695637 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089969951659441 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34734909422695637 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986983448266983 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10313000530004501 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2444989513605833 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0887999776750803 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37724897265434265 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5163070056587458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04003988578915596 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.899956027045846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1339591108262539 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22254884243011475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1790798269212246 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03924989141523838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728991866111755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327995263040066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15767989680171013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477801077067852 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13110903091728687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042419880628585815 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1820169165730476 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07328996434807777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15868013724684715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04234001971781254 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125005453824997 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34706899896264076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927006274461746 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34517887979745865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08939998224377632 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10268902406096458 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24350895546376705 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08867005817592144 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37611881271004677 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5128168743103743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04000007174909115 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8982267938554287 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23968005552887917 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35809888504445553 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.042500216513872 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2208699006587267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37572882138192654 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10538985952734947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18122908659279346 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040830112993717194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680300872772932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08482998237013817 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17212890088558197 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04851003177464008 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014570076018571854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08242996409535408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043289968743920326 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.363046932965517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08493009954690933 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17216894775629044 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036499928683042526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0931799877434969 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5749231465160847 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177904576063156 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5789930261671543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09298999793827534 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1269499771296978 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3528089728206396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06814999505877495 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46575884334743023 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.076494093984365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03474997356534004 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.46439297311008 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16372906975448132 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2762989606708288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13111019507050514 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903987817466259 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06741913966834545 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08329004049301147 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16874982975423336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04766997881233692 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015550060197710991 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08298992179334164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038768863305449486 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.173157012090087 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08396012708544731 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17024017870426178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036739977076649666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09211990982294083 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5877220798283815 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09328010492026806 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5899130161851645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.091959023848176 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12581981718540192 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3528099041432142 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06785010918974876 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4651700146496296 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.095653029158711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03403006121516228 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.47955304197967 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12005981989204884 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23378990590572357 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07469905540347099 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13049994595348835 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03976002335548401 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06723008118569851 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08402997627854347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1685891766101122 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05580997094511986 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015609897673130035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08255010470747948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039010075852274895 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1394878383725882 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08535012602806091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17078896053135395 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03647990524768829 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5835029557347298 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254994802176952 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.587563056498766 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09093992412090302 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12654997408390045 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.353069044649601 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06776000373065472 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46503893099725246 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.096863981336355 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03432994708418846 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.481812896206975 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.324971139431 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0878798309713602 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02505001612007618 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11004996486008167 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.12768210656941 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19026990048587322 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.0459989532828331 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04008016549050808 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02550007775425911 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03653997555375099 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24009915068745613 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06932998076081276 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06583891808986664 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15635881572961807 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04861992783844471 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08257990702986717 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03986014053225517 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06778002716600895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.084639061242342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1701689325273037 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04750979132950306 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015500001609325409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08324999362230301 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.93218800611794 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06191898137331009 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14811893925070763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03627012483775616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09230989962816238 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6008330062031746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09203003719449043 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.597291884943843 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1277988776564598 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35485904663801193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757001392543316 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4672589711844921 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.12081410549581 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03430992364883423 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.499681949615479 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08425000123679638 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17261016182601452 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488299410790205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08148001506924629 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04652002826333046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06825895980000496 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08490984328091145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17131981439888477 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900014027953148 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015049939975142479 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08277990855276585 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9466488845646381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08343998342752457 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16914005391299725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036659883335232735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09139999747276306 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.58742312528193 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09317882359027863 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5892529413104057 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112991392612457 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12689991854131222 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35340897738933563 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06792019121348858 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.465228920802474 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.096733829006553 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03419979475438595 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.480033138766885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07858010940253735 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10438007302582264 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.063411047682166 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07900013588368893 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17091981135308743 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048919813707470894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08056010119616985 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039929989725351334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06740004755556583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07403991185128689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588999293744564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04784995689988136 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015339814126491547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13143010437488556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031029805541038513 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.025248086079955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372000254690647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15909993089735508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04175002686679363 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09086006321012974 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3478399012237787 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08965004235506058 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34509995020926 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08984003216028214 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10357983410358429 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24485005997121334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08959020487964153 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3920900635421276 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.532156951725483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03978004679083824 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9185261335223913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07915985770523548 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16494980081915855 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13052881695330143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03967992961406708 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749015301465988 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743598211556673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15866989269852638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736892879009247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014990102499723434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13199006207287312 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0164380073547363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344991900026798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585299614816904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04178914241492748 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09211013093590736 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.35033910535275936 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08999998681247234 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34583895467221737 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968007750809193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10301987640559673 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24416018277406693 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08891918696463108 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3771588671952486 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5211771242320538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039850128814578056 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9062059000134468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12761005200445652 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1528800930827856 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.120537873357534 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0759901013225317 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656298991292715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04918011836707592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12989994138479233 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03875000402331352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06885896436870098 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0817000400274992 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1673600636422634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0484599731862545 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522991806268692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23025902919471264 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03724009729921818 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1807868722826242 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422990165650845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16025989316403866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05270005203783512 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09174901060760021 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2024101559072733 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048986248672009 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20029908046126366 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006005711853504 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10284991003572941 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2069298643618822 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13393000699579716 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38486882112920284 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2343667913228273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05084997974336147 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6421971376985312 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07551000453531742 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16271998174488544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22871000692248344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040079932659864426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06764009594917297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907897613942623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04777987487614155 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015360070392489433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22971001453697681 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2129668612033129 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07399008609354496 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15979004092514515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05273986607789993 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09227986447513103 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20343903452157974 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904998742043972 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2002499531954527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0898588914424181 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10322011075913906 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2062399871647358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13221008703112602 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3823700826615095 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2340769171714783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050980132073163986 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6425370704382658 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22537005133926868 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2541998401284218 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.032357923686504 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0769288744777441 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16758916899561882 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049110036343336105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22872001864016056 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039590056985616684 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06845896132290363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07440987974405289 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16036001034080982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922994412481785 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23003900423645973 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.221287064254284 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402012124657631 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15927990898489952 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04877988249063492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22855889983475208 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963988274335861 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06764987483620644 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15963008627295494 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477891881018877 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014789868146181107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22995006293058395 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2055181432515383 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.468314953148365 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08798902854323387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17746887169778347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04909001290798187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2291800919920206 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03964989446103573 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06828899495303631 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581990212202072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16065011732280254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04929979331791401 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014990102499723434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22982899099588394 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2320568785071373 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07496005855500698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1632198691368103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05268002860248089 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09186007082462311 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19693979993462563 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09222980588674545 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1910300925374031 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08966983295977116 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09651994332671165 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20083901472389698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13258005492389202 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37752906791865826 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2148478999733925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0499701127409935 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.624837052077055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07572909817099571 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16256910748779774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049690017476677895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22666994482278824 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398589763790369 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06720004603266716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387017831206322 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15887990593910217 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048430170863866806 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22864900529384613 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2162269558757544 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.164600977674127 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517007179558277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16169995069503784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048459041863679886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4237999673932791 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395490787923336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06812019273638725 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07405015639960766 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15993998385965824 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04891003482043743 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014940043911337852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22842898033559322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06883987225592136 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4921771362423897 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436005398631096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16044988296926022 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047699082642793655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4229398909956217 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899015635252 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06762007251381874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15850993804633617 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22830883972346783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06874999962747097 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4845470432192087 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498986087739468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16509019769728184 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04832888953387737 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42338017374277115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395490787923336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06788014434278011 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16028014943003654 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22803922183811665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0688598956912756 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5018470585346222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22487901151180267 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30422909185290337 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.935769829899073 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390999235212803 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15987013466656208 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785996861755848 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.423019053414464 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906991332769394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06808014586567879 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15867012552917004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803994670510292 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999881386756897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2289090771228075 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06945990025997162 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.486056949943304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07328996434807777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596000511199236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052179908379912376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09200000204145908 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20087999291718006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08945888839662075 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1991400495171547 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08924002759158611 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10242895223200321 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20649912767112255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13177981600165367 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3843889571726322 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2309681624174118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05079992115497589 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6383458860218525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541989907622337 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16161007806658745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04763994365930557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42319903150200844 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03979005850851536 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06791995838284492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445993833243847 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16426900401711464 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736008122563362 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015070196241140366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22829999215900898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06953999400138855 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4938770327717066 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0737698283046484 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16029994003474712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05218992009758949 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09213993325829506 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20080897957086563 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08835992775857449 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1991400495171547 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0899189617484808 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10246015153825283 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20612007938325405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13160984963178635 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38218009285628796 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.228128094226122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05050003528594971 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6342469025403261 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07588998414576054 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16409996896982193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048129819333553314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32586907036602497 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04066992551088333 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760982796549797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0790890771895647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16415910795331 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22789998911321163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05888892337679863 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.390696968883276 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16065011732280254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052609946578741074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09292992763221264 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2003200352191925 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08952012285590172 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19898894242942333 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0899401493370533 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10206992737948895 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2051698975265026 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13184896670281887 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3809090703725815 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2259369250386953 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05067000165581703 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6347768250852823 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.229518860578537 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31418888829648495 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.80074005201459 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13070995919406414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22074999287724495 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052789924666285515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32622995786368847 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399700365960598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06801006384193897 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377984002232552 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1580298412591219 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0478099100291729 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.028590206056833267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.132089015096426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05948985926806927 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3663370627909899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354002445936203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159549992531538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04217005334794521 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214901365339756 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34630997106432915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09014899842441082 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34599006175994873 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910894393920898 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10280008427798748 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24361000396311283 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08867005817592144 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37605990655720234 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5144369099289179 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03986898809671402 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8997660372406244 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08609984070062637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1733400858938694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22790906950831413 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039190053939819336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06732018664479256 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741397961974144 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15827990137040615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04727020859718323 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13071997091174126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04863995127379894 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1850879527628422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15951995737850666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04205992445349693 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09209010750055313 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34702010452747345 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09053992107510567 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3467488568276167 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986005559563637 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10307994671165943 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24297996424138546 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0885890331119299 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3753488417714834 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5155468136072159 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03991997800767422 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9017159938812256 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07959012873470783 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1680899877101183 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04891911521553993 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1786800567060709 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395399983972311 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06727990694344044 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741488765925169 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593991182744503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015200115740299225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1310498919337988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04226993769407272 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1288679670542479 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07324991747736931 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15938910655677319 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04187994636595249 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09130989201366901 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3475088160485029 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08993991650640965 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3463688772171736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944002911448479 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10359915904700756 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24540908634662628 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08925003930926323 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37873885594308376 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5216069296002388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03978004679083824 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.906825928017497 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23974990472197533 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35807909443974495 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.955020155757666 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2207891084253788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.375258969143033 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10339007712900639 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18002907745540142 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963010385632515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760004907846451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08421996608376503 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17085997387766838 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015250174328684807 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08223997429013252 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04345993511378765 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3556568883359432 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08468003943562508 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17114006914198399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03657001070678234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09220000356435776 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.591973101720214 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09286007843911648 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.591942135244608 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112991392612457 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12634997256100178 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35172910429537296 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06725010462105274 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4635290242731571 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.101432817056775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03429013304412365 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.486662991344929 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16362895257771015 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2750291023403406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13144011609256268 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06746896542608738 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08526002056896687 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17051002942025661 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048720045015215874 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0825000461190939 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03846990875899792 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1724180076271296 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08451985195279121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17186999320983887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036319950595498085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149988181889057 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5887020640075207 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09224982932209969 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.59726301394403 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202910587191582 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12687989510595798 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3546799998730421 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06947992369532585 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4707400221377611 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.119123056530952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03447011113166809 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.513703010976315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12062001042068005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23399991914629936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07421895861625671 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13062008656561375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04003988578915596 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06732996553182602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08445000275969505 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16941898502409458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015129800885915756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08224020712077618 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03899005241692066 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.130308024585247 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08418015204370022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.170030165463686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035850098356604576 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09234994649887085 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5840929485857487 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09469990618526936 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.590062027797103 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214016608893871 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12659002095460892 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3520490135997534 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06787991151213646 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4638989921659231 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.098643971607089 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03395997919142246 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.481292985379696 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.36441102065146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09215879254043102 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025129877030849457 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10766997002065182 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.05304195731878 ms for forwarding
--------------------
No. 3
<class 'diffusers.models.embeddings.Timesteps'> take 0.24062907323241234 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05204998888075352 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04473002627491951 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.035250093787908554 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0377800315618515 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2729799598455429 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1285090111196041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0891098752617836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19811000674962997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05219015292823315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09727897122502327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04118005745112896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08377013728022575 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09202002547681332 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18823007121682167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04975893534719944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01831003464758396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08639995940029621 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0763478931039572 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749099999666214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17900997772812843 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03949902020394802 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10711001232266426 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6360728554427624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10453001596033573 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6039319820702076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09725010022521019 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13637891970574856 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3786589950323105 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07751001976430416 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5141589790582657 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.27295402996242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03579910844564438 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.70359200052917 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08771987631917 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1778199803084135 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979013465344906 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08357991464436054 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03927014768123627 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06992882117629051 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08877995423972607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17680996097624302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04813005216419697 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01543993130326271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08605001494288445 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9670080617070198 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08653011173009872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17501995898783207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03818003460764885 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09462004527449608 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.604631870985031 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09863008745014668 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5778728779405355 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09790901094675064 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12748991139233112 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35689002834260464 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0688689760863781 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4744590260088444 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.1521428655833 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03493018448352814 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.549362886697054 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07912982255220413 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10742992162704468 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.52068111859262 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07806997746229172 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.172728905454278 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04989001899957657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08155987598001957 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03901985473930836 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0721900723874569 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07542921230196953 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628489699214697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1335900742560625 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03581005148589611 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0623980779200792 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08131912909448147 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1752988900989294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042289961129426956 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09284005500376225 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34877914004027843 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10119006037712097 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3563491627573967 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051989763975143 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10375911369919777 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2479488030076027 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09038997814059258 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38549909368157387 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5618870966136456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04077982157468796 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9758359994739294 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483013905584812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1638690009713173 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04827999509871006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13131997548043728 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039400067180395126 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820890121161938 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348996587097645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16019982285797596 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047220150008797646 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13247900642454624 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.024747034534812 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07299007847905159 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16053998842835426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04303990863263607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917299184948206 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3494389820843935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10059983469545841 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.358430203050375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006890468299389 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10406994260847569 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24787010625004768 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09126984514296055 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3874900285154581 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5658170450478792 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045239925384521484 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.980806002393365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12922007590532303 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1563399564474821 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.320517044514418 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07557007484138012 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16722921282052994 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900991916656494 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1309700310230255 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903009928762913 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07347995415329933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07531908340752125 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1635691151022911 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2316401805728674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03837002441287041 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1959669645875692 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456005550920963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162909971550107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05338992923498154 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09172013960778713 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20355894230306149 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980999700725079 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20199990831315517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09050010703504086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10376004502177238 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2106800675392151 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13592001050710678 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3975299187004566 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2615269515663385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05501997657120228 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6959269996732473 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07643993012607098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16616005450487137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23075914941728115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0405898317694664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06977003067731857 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08839881047606468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19070901907980442 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875008016824722 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01535983756184578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23096008226275444 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2663679663091898 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07424992509186268 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16211019828915596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05418993532657623 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09359000250697136 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2034089993685484 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08958997204899788 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20144903101027012 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09115994907915592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1041698269546032 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2113699447363615 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13355910778045654 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3937189467251301 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2567469384521246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05174009129405022 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6758469864726067 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22693886421620846 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2546589821577072 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.199297029525042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07807998917996883 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17296010628342628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050609931349754333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23006880655884743 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03970996476709843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07009017281234264 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07683993317186832 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.167118851095438 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0498499721288681 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016249949112534523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23150001652538776 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2527280487120152 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07599894888699055 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667090691626072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05073007196187973 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22968999110162258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04077982157468796 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06883987225592136 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07560988888144493 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16486016102135181 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05006999708712101 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01543993130326271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2311889547854662 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2463969178497791 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5468149688094854 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07599988020956516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16774004325270653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050269998610019684 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23039896041154861 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04045991227030754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06947014480829239 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07651885971426964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16541918739676476 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050309812650084496 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015619909390807152 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2309300471097231 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2453680392354727 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07613003253936768 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16754004172980785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05407980643212795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09453995153307915 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2170188818126917 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10283011943101883 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19397004507482052 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09285006672143936 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09766989387571812 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2219600137323141 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13444991782307625 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4055288154631853 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3029768597334623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05184998735785484 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7301258631050587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07712002843618393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16767997294664383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05097012035548687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2300189808011055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04050019197165966 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843986921012402 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07567997090518475 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16521895304322243 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968000575900078 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015960074961185455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2294899895787239 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2410981580615044 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.324892070144415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07719011045992374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16624992713332176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04924018867313862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4247790202498436 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040109967812895775 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0707102008163929 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07628998719155788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16463897190988064 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05000992678105831 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558009535074234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298599574714899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07011997513473034 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5238879714161158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535004988312721 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1653099898248911 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04958990029990673 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42442907579243183 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04144012928009033 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06924988701939583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08977903053164482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19681896083056927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05155988037586212 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015570083633065224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2296101301908493 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07017003372311592 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5642470680177212 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07565016858279705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16550999134778976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04883995279669762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4278291016817093 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04032999277114868 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06946898065507412 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566995918750763 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1635600347071886 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0502800103276968 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015799887478351593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2293391153216362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07040007039904594 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.526947133243084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22631906904280186 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32284902408719063 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.103089148178697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07624994032084942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16739009879529476 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0492599792778492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4262900911271572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025897942483425 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06999983452260494 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546995766460896 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16337004490196705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06534997373819351 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01556985080242157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23047905415296555 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0735200010240078 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.54830701649189 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506995461881161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656289678066969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05350005812942982 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09368988685309887 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20209001377224922 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09155902080237865 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1990899909287691 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09154994040727615 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10283896699547768 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2091890200972557 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13296003453433514 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3918891306966543 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.25322793610394 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05202018655836582 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6776269767433405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07648998871445656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16689999029040337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049110036343336105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42447890155017376 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040140002965927124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07022987119853497 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07659918628633022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17413916066288948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.058420002460479736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0189801212400198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23344997316598892 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07122894749045372 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5613469295203686 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16469997353851795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06074015982449055 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09406008757650852 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20246999338269234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09117997251451015 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1997789368033409 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09083980694413185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10302010923624039 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20906003192067146 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13296888209879398 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3907990176230669 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2513569090515375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0517901498824358 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.680747140198946 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07814005948603153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16910908743739128 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049449969083070755 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32677012495696545 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039999838918447495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06940006278455257 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0765498261898756 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1662098802626133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049380119889974594 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01587020233273506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2303288783878088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06019999273121357 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4197870623320341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07512909360229969 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16548903658986092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05303998477756977 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11289003305137157 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20631891675293446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09165005758404732 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19932002760469913 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11064903810620308 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10356004349887371 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22485991939902306 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1328401267528534 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4094101022928953 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3222871348261833 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050919828936457634 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7487658187747002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23077009245753288 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31659984961152077 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.178040014579892 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13125012628734112 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22389995865523815 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05346001125872135 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3274190239608288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04054000601172447 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06945012137293816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415888831019402 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16209902241826057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048999907448887825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13227015733718872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06120000034570694 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3789879158139229 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430906407535076 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16366899944841862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04276982508599758 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0948500819504261 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34912885166704655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1090499572455883 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3473989199846983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153014980256557 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10981992818415165 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25417888537049294 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09710993617773056 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4061588551849127 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5979569870978594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04051998257637024 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.000225940719247 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0923199113458395 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18332898616790771 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049040187150239944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22873911075294018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03982987254858017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06960984319448471 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07340009324252605 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16073998995125294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725996404886246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015339814126491547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13206014409661293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048969872295856476 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2130469549447298 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733397901058197 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16120006330311298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04270882345736027 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09258999489247799 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34749903716146946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3474790137261152 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09103002957999706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10301009751856327 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24569896049797535 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09036995470523834 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3833291120827198 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5339269302785397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04075001925230026 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9278558902442455 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08041015826165676 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17037894576787949 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05589006468653679 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18049008212983608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039350008592009544 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363012991845608 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15967898070812225 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047659967094659805 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1330999657511711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04505994729697704 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1534178629517555 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07375003769993782 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16197003424167633 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04324992187321186 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09362003766000271 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.347689026966691 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09093014523386955 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3452589735388756 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0894798431545496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10322988964617252 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2466491423547268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09034993126988411 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3850390203297138 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5332570765167475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040640123188495636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9279359839856625 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24033011868596077 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.36234897561371326 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.185210034251213 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2205690834671259 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37931883707642555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10475004091858864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18302910029888153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04187994636595249 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06989995017647743 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08775992318987846 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17742998898029327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822993651032448 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08328002877533436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04412000998854637 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.393317012116313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08440995588898659 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17289910465478897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03745011053979397 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09366008453071117 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5828130785375834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09365915320813656 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.585672937333584 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09326986037194729 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12713996693491936 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.355008989572525 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06853998638689518 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47011906281113625 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.101233979687095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03479002043604851 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.511523017659783 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17281994223594666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29497011564671993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07500988431274891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1334089320152998 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03927992656826973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850017234683037 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08362019434571266 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1710799988359213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048318877816200256 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01535983756184578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08295988664031029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03990018740296364 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2209778651595116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08642999455332756 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1831690315157175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038670143112540245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09401002898812294 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5809429828077555 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09336997754871845 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.591042011976242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09269011206924915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12696022167801857 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35618897527456284 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06832997314631939 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4728790372610092 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.10995395295322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0350000336766243 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.515232941135764 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.122779980301857 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23701018653810024 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13189902529120445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04000007174909115 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681800302118063 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08359015919268131 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18429988995194435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05002901889383793 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08409004658460617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03939005546271801 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.163118053227663 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0845401082187891 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17301994375884533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03801984712481499 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09346986189484596 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.60003300011158 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09345007129013538 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5987228620797396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09292014874517918 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12699002400040627 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3552099224179983 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06839982233941555 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47037890180945396 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.133203955367208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035139964893460274 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.526532910764217 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.5802999753505 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08615991100668907 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024999957531690598 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15215901657938957 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 95.34791694022715 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20201015286147594 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.051320064812898636 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.041380058974027634 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.029529910534620285 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03686011768877506 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25494908913969994 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07136003114283085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0702200923115015 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15883916057646275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049830181524157524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08322997018694878 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040770042687654495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06961985491216183 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08542905561625957 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17311889678239822 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04932982847094536 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016619917005300522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08386978879570961 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9549979586154222 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06507011130452156 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15337904915213585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03718002699315548 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09302003309130669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5806328523904085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09390013292431831 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5848519764840603 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09332993067800999 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12791994959115982 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35619898699223995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06856000982224941 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4722690209746361 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.100124076008797 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034880125895142555 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.490722859278321 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08559017442166805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18276995979249477 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04964997060596943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08195987902581692 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04088995046913624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06754905916750431 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08533010259270668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17235008999705315 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979991354048252 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015520025044679642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08364999666810036 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9639288764446974 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08465000428259373 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17265905626118183 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03720005042850971 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0943599734455347 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.578132949769497 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09426893666386604 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5724430344998837 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0932400580495596 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1267390325665474 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35551912151277065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06806012243032455 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4698890261352062 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.084903936833143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034569064155220985 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.479912066832185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07891980931162834 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10552979074418545 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.100380828604102 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07731001824140549 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17021014355123043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04958990029990673 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08053891360759735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040430109947919846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06852997466921806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07594004273414612 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16403011977672577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049759866669774055 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01581897959113121 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.131980050355196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032060081139206886 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0407879017293453 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473002187907696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16294000670313835 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04254002124071121 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09816000238060951 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3475998528301716 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09194016456604004 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3454901743680239 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10187993757426739 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10406016372144222 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24663982912898064 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08924887515604496 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3830990754067898 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5531969256699085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040400074794888496 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.951316138729453 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07652002386748791 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16545993275940418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13065896928310394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04036002792418003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06857002153992653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08948007598519325 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1831590197980404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04776008427143097 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01546996645629406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13187993317842484 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0506079997867346 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07326016202569008 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15863892622292042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04200986586511135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0926000066101551 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3483290784060955 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030988439917564 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34632906317710876 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909986354410648 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10394002310931683 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24619908072054386 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08963001891970634 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3805288579314947 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5224069356918335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04021008498966694 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9088461995124817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1278489362448454 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1538088545203209 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.208817008882761 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07604993879795074 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17961999401450157 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499398447573185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13086991384625435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039218924939632416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0697900541126728 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07575005292892456 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16128993593156338 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0150301493704319 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2308790571987629 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03744009882211685 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1922079138457775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376004941761494 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16085011884570122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05317898467183113 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917299184948206 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20240992307662964 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904700718820095 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20846910774707794 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904700718820095 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10319985449314117 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20777899771928787 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13327994383871555 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3848690539598465 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2439980637282133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05074008367955685 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.65255693718791 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07540895603597164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628291793167591 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048690009862184525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22854004055261612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038929982110857964 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681891106069088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07381988689303398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585299614816904 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849978722631931 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23003900423645973 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2119368184357882 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0813901424407959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677300315350294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05319993942975998 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09161909110844135 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20302017219364643 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983002044260502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2004189882427454 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08825003169476986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10368996299803257 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20819995552301407 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.132629182189703 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38644904270768166 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2355369981378317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05094008520245552 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6519369091838598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22641895338892937 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2519791014492512 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.061188178136945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07737986743450165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1684599556028843 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049380119889974594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22887904196977615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396601390093565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843009032309055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746001023799181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15940912999212742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860991612076759 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22984016686677933 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2195580638945103 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07521919906139374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16499892808496952 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04864996299147606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22863992489874363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03964919596910477 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06839004345238209 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07403013296425343 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16000005416572094 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049720052629709244 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014810124412178993 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22982899099588394 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2259171344339848 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.487604971975088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07496005855500698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16236980445683002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048320041969418526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22855005227029324 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039380043745040894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06821006536483765 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07474888116121292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15972903929650784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04893983714282513 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22988999262452126 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2128781527280807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07508997805416584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16417005099356174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05349982529878616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09234994649887085 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1963090617209673 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09113992564380169 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1948499120771885 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08993898518383503 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0963299535214901 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20255986601114273 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13323011808097363 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37982012145221233 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.222657971084118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050609931349754333 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6380359884351492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07616984657943249 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1629700418561697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04939991049468517 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22864900529384613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918982110917568 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0677499920129776 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0759901013225317 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16347994096577168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873890429735184 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016999896615743637 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23017008788883686 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2352780904620886 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.181351978331804 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07499894127249718 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630790065973997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4269690252840519 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03999006003141403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06924988701939583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437006570398808 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15988899394869804 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854006692767143 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501990482211113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22849999368190765 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06878003478050232 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.499176025390625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07425015792250633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16107014380395412 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047179870307445526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42297900654375553 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039670150727033615 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06791995838284492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348996587097645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15801913104951382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048320041969418526 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014669960364699364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22843992337584496 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06932998076081276 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4858869835734367 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346994243562222 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16005011275410652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871981218457222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4239690024405718 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040449900552630424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850995123386383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390999235212803 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15950994566082954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866020753979683 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014639925211668015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2287188544869423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06906990893185139 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4928369782865047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22502988576889038 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30476017855107784 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.934579832479358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486995309591293 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16196002252399921 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047789886593818665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4242390859872103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03948993980884552 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06834999658167362 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455004379153252 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15972880646586418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014859950169920921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.228530028834939 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06956001743674278 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4918979723006487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362989708781242 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16440008766949177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052519841119647026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09273993782699108 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2019088715314865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034993126988411 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2064201980829239 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09020906873047352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10269996710121632 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20757014863193035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1321102026849985 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38606999441981316 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2439270503818989 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0506499782204628 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6567471902817488 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07634004577994347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16371998935937881 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048339832574129105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42342906817793846 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039430102333426476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06809993647038937 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16050995327532291 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015129800885915756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22895913571119308 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0698799267411232 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4980470295995474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387995719909668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15924987383186817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052960123866796494 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09261909872293472 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20147999748587608 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968007750809193 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19864900968968868 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900999091565609 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1023199874907732 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20657014101743698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13170880265533924 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3829691559076309 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.228666864335537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05089002661406994 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6366869676858187 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07562991231679916 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633400097489357 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04859990440309048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32563996501266956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039479928091168404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759888492524624 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485994137823582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16023986972868443 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22829906083643436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05886005237698555 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.386307179927826 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377006113529205 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602000556886196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05260901525616646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214016608893871 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20034005865454674 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08961907587945461 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19868998788297176 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995016105473042 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10253908112645149 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20610913634300232 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13236980885267258 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38249907083809376 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.226198161020875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05079992115497589 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6360972076654434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23011001758277416 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3152999561280012 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.83664090745151 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13071903958916664 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22005918435752392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05263998173177242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.327069079503417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.042289961129426956 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06762007251381874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07399008609354496 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588999293744564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04747905768454075 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13077002950012684 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05952012725174427 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3545569963753223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15927990898489952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041999854147434235 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920100137591362 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34712906926870346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972012437880039 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3453991375863552 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911988697946072 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10350998491048813 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2443299163132906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08888891898095608 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3766990266740322 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5146869700402021 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04078005440533161 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9018861930817366 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09107007645070553 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17858995124697685 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04850002005696297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22841920144855976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03976980224251747 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06788992322981358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477984763681889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600491814315319 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475000124424696 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014839926734566689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13137003406882286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04830001853406429 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2005879543721676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429905235767365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608789898455143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04191999323666096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09152013808488846 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34577888436615467 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08990010246634483 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3469788935035467 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08928007446229458 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1029700506478548 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24358904920518398 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08936994709074497 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3771891351789236 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5168669633567333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03981008194386959 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9049560651183128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07948908023536205 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16703899018466473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0492599792778492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17936993390321732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040049897506833076 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06723892875015736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15818001702427864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046999892219901085 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014849938452243805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13085897080600262 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04260987043380737 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1288670357316732 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735800713300705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15868991613388062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04226015880703926 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917389988899231 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34717004746198654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978904224932194 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3533600829541683 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0888691283762455 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10331021621823311 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24412991479039192 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08809985592961311 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37644989788532257 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5203170478343964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039739999920129776 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9040158949792385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23919017985463142 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3586001694202423 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.964609984308481 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22000004537403584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3736100625246763 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10459916666150093 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1801301259547472 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040239887312054634 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0669797882437706 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08556898683309555 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17227907665073872 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048419926315546036 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08240994065999985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043659936636686325 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.355938147753477 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08455896750092506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1710092183202505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03638002090156078 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09295996278524399 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5796628799289465 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09321002289652824 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5825332161039114 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153992868959904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12635882012546062 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3522890619933605 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067870132625103 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46595907770097256 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.085464127361774 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034670112654566765 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.471183013170958 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16425014473497868 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2778000198304653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07527018897235394 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13194908387959003 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04096003249287605 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08045998401939869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08465000428259373 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17613009549677372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900991916656494 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016049016267061234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08260016329586506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039190053939819336 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2062168680131435 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0841100700199604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1719500869512558 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03699003718793392 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09320001117885113 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5807727836072445 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09402004070580006 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.587413113564253 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09443005546927452 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1270999200642109 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35491911694407463 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687700230628252 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47026900574564934 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.101583924144506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03421003930270672 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.4915931802243 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12239906936883926 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2362290397286415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07499009370803833 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13107992708683014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04134001210331917 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749993190169334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08447002619504929 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17083995044231415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015780096873641014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08262903429567814 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040400074794888496 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1467670556157827 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08407002314925194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17191004008054733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03719981759786606 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0942691694945097 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5886429250240326 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09422004222869873 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5908229183405638 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0930300448089838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1270999200642109 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35485904663801193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06856978870928288 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47010905109345913 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.112322771921754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03511016257107258 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.503653109073639 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.40355008468032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08723000064492226 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025429995730519295 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11068908497691154 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.36235094815493 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19837007857859135 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05281902849674225 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.039640115574002266 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02559996210038662 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.037029851227998734 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25146896950900555 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06849016062915325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06721983663737774 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15594903379678726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04989001899957657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08257990702986717 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04118005745112896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06829015910625458 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08454895578324795 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1720490399748087 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049040187150239944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016479985788464546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08311006240546703 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9488679934293032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06301980465650558 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15213992446660995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037359073758125305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0955299474298954 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5955028142780066 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09405985474586487 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5976129584014416 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0968689564615488 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12803985737264156 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35614892840385437 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820005364716053 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.471348874270916 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.14131391234696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03475998528301716 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.53328313678503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.084359897300601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17441902309656143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05061016418039799 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0823501031845808 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04090019501745701 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0674098264425993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08534896187484264 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17209909856319427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04995008930563927 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016110017895698547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08295988664031029 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9553181007504463 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0838900450617075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17149909399449825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037049874663352966 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09327009320259094 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.574252827093005 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09304005652666092 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.574602073058486 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09267986752092838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12638000771403313 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35377894528210163 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06851996295154095 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46801893040537834 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.080274099484086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033910153433680534 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.471322944387794 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07854891009628773 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.12230896390974522 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.13955109193921 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10945997200906277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21791993640363216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050530070438981056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0817989930510521 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04036002792418003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06937002763152122 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07555005140602589 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16223988495767117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922994412481785 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016219913959503174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13129995204508305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03182003274559975 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.099838176742196 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16508996486663818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04757894203066826 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.108370091766119 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34870882518589497 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09066984057426453 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3454890102148056 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08882000111043453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10266993194818497 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24376017972826958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08894992060959339 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3773188218474388 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.536486903205514 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03927992656826973 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9446059595793486 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1615199726074934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048029934987425804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13027898967266083 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963010385632515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06727012805640697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346994243562222 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15851017087697983 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04777987487614155 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1316999550908804 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0142079554498196 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402990013360977 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159319955855608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04199007526040077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10469905100762844 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34944014623761177 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0899189617484808 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3462000750005245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08877995423972607 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10309997014701366 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24378998205065727 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08930009789764881 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3771290648728609 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5335259959101677 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039499951526522636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.918876077979803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12740003876388073 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15250989235937595 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.233318010345101 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07558893412351608 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16566901467740536 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900991916656494 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12952997349202633 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039830105379223824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687700230628252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467996329069138 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16098003834486008 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843994975090027 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830147847533226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22872909903526306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03750994801521301 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1740569025278091 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612999476492405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05295989103615284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09090895764529705 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20132982172071934 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08939998224377632 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19980897195637226 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09679002687335014 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10245013982057571 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21151895634829998 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1328799407929182 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3889189101755619 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2409170158207417 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0506499782204628 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.649426994845271 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07598986849188805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16400893218815327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22744014859199524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03982009366154671 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06771995685994625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07439008913934231 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15993998385965824 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048350077122449875 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015220139175653458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2282590139657259 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2121768668293953 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07359986193478107 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600098330527544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05310005508363247 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09262003004550934 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20345882512629032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09058997966349125 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1993998885154724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08922000415623188 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10335003025829792 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20798994228243828 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.132350018247962 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3852788358926773 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2379670515656471 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05068001337349415 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.645467011258006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22544991225004196 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25070016272366047 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.032917881384492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07651001214981079 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18076901324093342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052209943532943726 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22780010476708412 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03967992961406708 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06852997466921806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541012018918991 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16218004748225212 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04940992221236229 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015470199286937714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22866902872920036 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2400969862937927 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07462012581527233 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16146991401910782 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04934915341436863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2270198892802 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03850017674267292 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750994361937046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07558008655905724 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16020890325307846 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22863899357616901 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2086271308362484 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.491234103217721 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16202987171709538 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04857010208070278 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22718915715813637 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039209844544529915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06785010918974876 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423991337418556 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16004987992346287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0491489190608263 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014729797840118408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22865994833409786 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.21055799536407 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07447903044521809 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16362895257771015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05343998782336712 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09297998622059822 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19731908105313778 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09063980542123318 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19147014245390892 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0900391023606062 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09637000039219856 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20117009989917278 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13302010484039783 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3785199951380491 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2235380709171295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049958936870098114 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6370860394090414 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07668999023735523 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645099837332964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910002462565899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22732908837497234 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915000706911087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06780005060136318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15963008627295494 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04906998947262764 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014659948647022247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22873911075294018 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2133980635553598 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.154091933742166 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07528997957706451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16249902546405792 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048819929361343384 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42351009324193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03902008756995201 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06806012243032455 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07493002340197563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16006012447178364 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22874888963997364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06967992521822453 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4958970714360476 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07401988841593266 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608789898455143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04760012961924076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4228190518915653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03937003202736378 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820005364716053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07379986345767975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15889015048742294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04905997775495052 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2280299086123705 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06911996752023697 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4871570747345686 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406901568174362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215909272432327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4233790095895529 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03875000402331352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06773998029530048 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414980791509151 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15994906425476074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2286299131810665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06918003782629967 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4911061152815819 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22542988881468773 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3057799767702818 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.932040115818381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07504899986088276 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623290590941906 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4242989234626293 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04103989340364933 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06926991045475006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07479009218513966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16102008521556854 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22883014753460884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0700601376593113 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4996870886534452 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07319892756640911 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15989900566637516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05268002860248089 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09191990830004215 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20068883895874023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905003778636456 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1990299206227064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08930894546210766 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10253000073134899 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20718015730381012 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1316398847848177 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3833700902760029 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2290379963815212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05089910700917244 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.63966603577137 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546995766460896 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16171997413039207 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871981218457222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42328890413045883 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039499951526522636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06796000525355339 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438892498612404 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1580489333719015 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04746997728943825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2297600731253624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07066898979246616 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.492546871304512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07858010940253735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16517005860805511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052930088713765144 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09188009425997734 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2010599710047245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09004003368318081 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19907904788851738 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918996900320053 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10227994062006474 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20643998868763447 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13205897994339466 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3827789332717657 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2313369661569595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050380127504467964 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6438669990748167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07575005292892456 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631688792258501 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048129819333553314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3254201728850603 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039618927985429764 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06764987483620644 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407017983496189 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16080006025731564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843994975090027 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015009893104434013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22857892327010632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059159938246011734 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3881470076739788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738000962883234 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15966896899044514 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052830204367637634 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09246007539331913 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20092003978788853 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10349880903959274 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1987498253583908 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08992012590169907 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303896851837635 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20611914806067944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13328995555639267 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3832089714705944 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2430979404598475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05102017894387245 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.652186969295144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22962014190852642 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31405012123286724 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.83872008509934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1307888887822628 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21979911252856255 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05191005766391754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32562902197241783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939983434975147 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06722006946802139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735800713300705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15812995843589306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015059951692819595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13103894889354706 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06031990051269531 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3463168870657682 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730298925191164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15831016935408115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042209867388010025 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09244983084499836 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34689996391534805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08959905244410038 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3444801550358534 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037903510034084 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10331999510526657 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24421000853180885 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08909008465707302 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3773798234760761 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5251671429723501 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040590064600110054 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9126359838992357 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1781198661774397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22791908122599125 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03976980224251747 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06812997162342072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07340009324252605 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15832995995879173 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792911931872368 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01482013612985611 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13149995356798172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04830001853406429 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1930379550904036 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345015183091164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15929993242025375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04215002991259098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034015238285065 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3458589781075716 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08936994709074497 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3449090290814638 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08882000111043453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10285014286637306 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24266005493700504 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08929893374443054 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.376138836145401 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.507677137851715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04003988578915596 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8935860134661198 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08015986531972885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18203002400696278 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04978012293577194 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17938902601599693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396000687032938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06773998029530048 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07405015639960766 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1578188966959715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047389883548021317 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13128994032740593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042220111936330795 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1411779560148716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07313909009099007 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15882891602814198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0424098689109087 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917899888008833 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34654908813536167 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09008008055388927 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3449989017099142 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08827005513012409 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10267994366586208 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24372898042201996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08927984163165092 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37701893597841263 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5112869441509247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039400067180395126 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8964360933750868 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23886002600193024 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35758898593485355 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.949279949069023 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2207891084253788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3741190303117037 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10507996194064617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18123909831047058 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0407099723815918 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06764009594917297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08459994569420815 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17117895185947418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0481500755995512 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015590107068419456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08217990398406982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04350999370217323 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.358266919851303 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08564000017940998 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17257989384233952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03681913949549198 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09293993934988976 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.579242853447795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.574982052668929 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069009684026241 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12668990530073643 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3524790517985821 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670398585498333 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46429899521172047 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.073454024270177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03407010808587074 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.46080295741558 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1627590972930193 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27406890876591206 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.131240114569664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03899005241692066 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06681913509964943 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08283997885882854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16795005649328232 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04831980913877487 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014950055629014969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08307001553475857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03844895400106907 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1679371818900108 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08380995132029057 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17060991376638412 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03690994344651699 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09156996384263039 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.585512051358819 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09301002137362957 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.586292965337634 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205914102494717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1271700020879507 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3540900070220232 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06845011375844479 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4676699172705412 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.097883081063628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034529948607087135 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.483963087201118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11848006397485733 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23243995383381844 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07507903501391411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.130870146676898 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039479928091168404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06686011329293251 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08414010517299175 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1698690466582775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08342997170984745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04003988578915596 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1360179632902145 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08410005830228329 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17104903236031532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03650994040071964 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149988181889057 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.589601954445243 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09022001177072525 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.586953040212393 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970010094344616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12762006372213364 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35475892946124077 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4678990226238966 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.102604188024998 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03421003930270672 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.487823186442256 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.318030923604965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08751987479627132 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024929875507950783 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11081015691161156 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.26244215480983 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19186991266906261 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.046040164306759834 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04009995609521866 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02498016692698002 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.036770012229681015 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24011917412281036 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06873998790979385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06603891961276531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15225913375616074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483999028801918 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08473009802401066 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039929989725351334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06842007860541344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.084269093349576 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1696390099823475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785996861755848 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015789875760674477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08311006240546703 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9331780020147562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06308895535767078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1497690100222826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036289915442466736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0925001222640276 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.592642955482006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09108008816838264 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.602551994845271 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09059999138116837 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12678909115493298 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3533889539539814 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06787991151213646 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4674589727073908 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.11380404047668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03381003625690937 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.4936220664531 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08425000123679638 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1713898964226246 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049819936975836754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08125999011099339 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03982987254858017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06672902964055538 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08458993397653103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1708699855953455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04766997881233692 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.017029931768774986 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08248002268373966 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9480479639023542 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08283997885882854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1681600697338581 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03629014827311039 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09182002395391464 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.582822857424617 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917389988899231 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.584852907806635 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065982885658741 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12650992721319199 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3525889478623867 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06772996857762337 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4641590639948845 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.08143406175077 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034049851819872856 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.46369401551783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07795006968080997 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10291999205946922 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.042490981519222 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07639010436832905 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1669200137257576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048479996621608734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08049001917243004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395399983972311 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757909432053566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158810056746006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13095885515213013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03137998282909393 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0144878178834915 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346994243562222 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922007150948048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04123011603951454 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0908789224922657 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.346610089763999 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010895155370235 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34470995888113976 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08945888839662075 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10277004912495613 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24332990869879723 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08875993080437183 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37597003392875195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5123470220714808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03968016244471073 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8969259690493345 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515004836022854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624799333512783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04820013418793678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13014907017350197 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039529986679553986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759003736078739 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741397961974144 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1573499757796526 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755984991788864 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014969846233725548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13126898556947708 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0105478577315807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07287017069756985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1572000328451395 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04229997284710407 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067915380001068 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3478899598121643 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0899098813533783 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3443800378590822 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970987983047962 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10260986164212227 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2436989452689886 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08848006837069988 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3756391815841198 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5136569272726774 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039670150727033615 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8978570587933064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12700003571808338 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15209009870886803 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.085227942094207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07574004121124744 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16460008919239044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12902915477752686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03896001726388931 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06806990131735802 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16036001034080982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047419918701052666 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.02447003498673439 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.229269964620471 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03881985321640968 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1801680084317923 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357914000749588 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1595888752490282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05213008262217045 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110989049077034 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2017691731452942 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980999700725079 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1995200291275978 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891790259629488 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10295002721250057 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20619016140699387 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1324999611824751 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3822499420493841 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2276980560272932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050579896196722984 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6333458479493856 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0756201334297657 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628100872039795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880991764366627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2292189747095108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039499951526522636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06763008423149586 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482990622520447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16019889153540134 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22855005227029324 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2142679188400507 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07379916496574879 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15981891192495823 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05284999497234821 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09146006777882576 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20148907788097858 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900999091565609 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21279999054968357 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08929893374443054 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1038599293678999 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20756013691425323 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13188994489610195 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3834501840174198 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2403980363160372 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050579896196722984 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.647275872528553 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22468017414212227 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.253249891102314 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.028507836163044 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08083879947662354 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.171249033883214 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04952005110681057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22768997587263584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03924989141523838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06785895675420761 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472001016139984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15959003940224648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892004653811455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830147847533226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22850907407701015 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2184570077806711 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343990728259087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15839003026485443 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04813005216419697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22703898139297962 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039109960198402405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0675697810947895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423991337418556 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1597600057721138 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04942994564771652 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014659948647022247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2282499335706234 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2023579329252243 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.462774980813265 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406016811728477 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16190903261303902 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048919813707470894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22708997130393982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07244991138577461 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15916000120341778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871981218457222 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015010125935077667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2288490068167448 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.214567106217146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07501011714339256 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16423012129962444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05254009738564491 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0930088572204113 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19559008069336414 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09079999290406704 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19116909243166447 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08992012590169907 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09592017158865929 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2001700922846794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13248994946479797 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37681893445551395 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2124169152230024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050179893150925636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.62503682076931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07536006160080433 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16153906472027302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048800138756632805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2264899667352438 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039090169593691826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06737909279763699 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16152998432517052 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797987639904022 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014919787645339966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2280091866850853 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2102269101887941 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.142730962485075 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08008000440895557 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1672001089900732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4235389642417431 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03999006003141403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06856885738670826 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15957001596689224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048629939556121826 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015020137652754784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2285391092300415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06892997771501541 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4971268828958273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481989450752735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16131997108459473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779989831149578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231589846313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039430102333426476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06798910908401012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591700129210949 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04833005368709564 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014839926734566689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22839894518256187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06901007145643234 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4917668886482716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067014075815678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231790080666542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039239879697561264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06795884110033512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473002187907696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594899222254753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05139992572367191 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2285290975123644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.068980036303401 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5054370742291212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22536912001669407 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3040891606360674 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.950630012899637 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07511000148952007 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1614599023014307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42368914000689983 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03937981091439724 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680088996887207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455982267856598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15928992070257664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734005779027939 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014859950169920921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22808881476521492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07098005153238773 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4886870048940182 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446995005011559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16079004853963852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05207001231610775 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256997145712376 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20051002502441406 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08893897756934166 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19862991757690907 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08895993232727051 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10212883353233337 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20686909556388855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13092998415231705 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38222898729145527 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2286680284887552 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050480011850595474 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.646576914936304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07614004425704479 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16328995116055012 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04844018258154392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42328890413045883 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04048994742333889 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486017420887947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15998887829482555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04818011075258255 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22876006551086903 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06987014785408974 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4950670301914215 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594389323145151 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052029965445399284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09198999032378197 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2007591538131237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09004003368318081 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1993600744754076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08962908759713173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10228995233774185 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2062399871647358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13125990517437458 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3809700720012188 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2268680147826672 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05048885941505432 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6333358362317085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076299998909235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16397004947066307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3254590556025505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975001163780689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06761006079614162 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07577892392873764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16156909987330437 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477498397231102 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014960067346692085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22841012105345726 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05902000702917576 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4003671240061522 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07403991185128689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16066990792751312 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0527799129486084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20118895918130875 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089599983766675 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19838917069137096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09027007035911083 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1023800577968359 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2060199622064829 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13174908235669136 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38109906017780304 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2266668491065502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05012005567550659 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6339768189936876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22990908473730087 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3140191547572613 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.81835019774735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13039004988968372 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21908991038799286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0523589551448822 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32590003684163094 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396601390093565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750901229679585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407995872199535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587499864399433 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479010097682476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13121915981173515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05922000855207443 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.34689686819911 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07397006265819073 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15921005979180336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04197983071208 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909899827092886 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34627015702426434 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953013457357883 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34508900716900826 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089969951659441 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10316888801753521 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24354900233447552 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08899997919797897 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37606898695230484 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5116671565920115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0396000687032938 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8964570481330156 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08627981878817081 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17383997328579426 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2280490007251501 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395399983972311 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06756000220775604 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453002035617828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15860982239246368 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725996404886246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014960067346692085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13088900595903397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04852982237935066 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1867079883813858 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07331999950110912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15833997167646885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041640130802989006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09103887714445591 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34740008413791656 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030895307660103 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3449500072747469 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891790259629488 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10331999510526657 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24413992650806904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08964003063738346 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3777099773287773 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.522565959021449 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039629871025681496 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.905336044728756 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08085998706519604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16915006563067436 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049250200390815735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17868890427052975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03937003202736378 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06777001544833183 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372000254690647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.157548813149333 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311800442636013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042400090023875237 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1265280190855265 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07299915887415409 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15807896852493286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04182010889053345 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202002547681332 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3462389577180147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913991041481495 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34612882882356644 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0898200087249279 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10331999510526657 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2435091882944107 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0896600540727377 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37792883813381195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.518497010692954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040040118619799614 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.903595868498087 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23999903351068497 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3585892263799906 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.933349909260869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22046896629035473 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37404894828796387 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10401010513305664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1799489837139845 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03891997039318085 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742007099092007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.083989929407835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16980990767478943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797009751200676 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522991806268692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08282903581857681 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04333001561462879 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3520270586013794 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08481997065246105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1709400676190853 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03674998879432678 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09120884351432323 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5932231694459915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09772996418178082 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5985729191452265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09370897896587849 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12757000513374805 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.355039956048131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789900362491608 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4700289573520422 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.132213028147817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034910161048173904 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.520952891558409 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16583013348281384 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27878908440470695 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07486017420887947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13171997852623463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040579820051789284 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742007099092007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0858998391777277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1728590577840805 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016479985788464546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08333008736371994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039289938285946846 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1921280529350042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08542998693883419 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19150995649397373 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04155002534389496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09542005136609077 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.593093017116189 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0992391724139452 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.602513112127781 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09219883941113949 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12662983499467373 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3559899050742388 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07168902084231377 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.476958928629756 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.13946290872991 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03451993688941002 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.560701971873641 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12065982446074486 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.234140083193779 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07537892088294029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311700325459242 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04120008088648319 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06799004040658474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08409889414906502 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16847881488502026 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047740060836076736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08283997885882854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03882008604705334 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1340780183672905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08442020043730736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17111911438405514 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036850105971097946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0928100198507309 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.577922936528921 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09302003309130669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.589342813938856 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902798492461443 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12732995674014091 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35413913428783417 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06865989416837692 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4664191510528326 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.087174035608768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034640077501535416 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.473973022773862 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.46106088347733 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09054900147020817 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024619977921247482 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11201994493603706 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.074772041291 ms for forwarding
--------------------
No. 4
<class 'diffusers.models.embeddings.Timesteps'> take 0.2391489688307047 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05329982377588749 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.044869957491755486 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03514997661113739 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.037899939343333244 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2730500418692827 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.12650899589061737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08967006579041481 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.200960086658597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05108979530632496 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09810901246964931 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04071998409926891 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07616006769239902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0869198702275753 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1773899421095848 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018059974536299706 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08726981468498707 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.046977937221527 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07704994641244411 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16737007535994053 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039109960198402405 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09683007374405861 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.649723017588258 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11977995745837688 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.61390202306211 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09721983224153519 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13321894221007824 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3742289263755083 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07094000466167927 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4965991247445345 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.299062959849834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03849901258945465 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.73740203678608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09663007222115993 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18749991431832314 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04975008778274059 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08489983156323433 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03956886939704418 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07068994455039501 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08955015800893307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17770007252693176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797009751200676 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015339814126491547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08606887422502041 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9778279345482588 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0855601392686367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18011010251939297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03969017416238785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09999983012676239 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5800919868052006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09300978854298592 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5875930916517973 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09593903087079525 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12900982983410358 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3601501230150461 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07015909068286419 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47919899225234985 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.125002961605787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03587012179195881 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.534823078662157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07973005995154381 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10945997200906277 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.523230846971273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07922900840640068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17477897927165031 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050320057198405266 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08228002116084099 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398498959839344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07278984412550926 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07521896623075008 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16295886598527431 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016189878806471825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13381009921431541 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03236997872591019 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0595680214464664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742690172046423 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16332906670868397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042530009523034096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09363004937767982 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3494990523904562 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09180000051856041 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3505290951579809 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09147007949650288 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10761991143226624 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2596189733594656 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0904998742043972 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3980989567935467 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5835468657314777 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04098983481526375 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9820260349661112 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494911551475525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17639901489019394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05365000106394291 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1328799407929182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039380043745040894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06929994560778141 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16178889200091362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04759989678859711 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01524016261100769 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1335800625383854 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0537381749600172 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345899939537048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16021891497075558 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04309997893869877 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09304005652666092 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3620090428739786 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09014992974698544 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3492089454084635 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010988287627697 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11099991388618946 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2636290155351162 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0909299124032259 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4031891003251076 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5697469934821129 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04060007631778717 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9631560426205397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12986897490918636 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15712901949882507 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.325176917016506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07759989239275455 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1860000193119049 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04930002614855766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13279891572892666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039460137486457825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07334002293646336 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477006874978542 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1626899465918541 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015338882803916931 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23378990590572357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03869016654789448 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2152979616075754 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08819880895316601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1925190445035696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05397992208600044 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09361002594232559 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20444905385375023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20337989553809166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10976893827319145 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10371999815106392 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21020998246967793 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13564899563789368 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3929990343749523 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.28531688824296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052060000598430634 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7399860080331564 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07589999586343765 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1648899633437395 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04891003482043743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23133005015552044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039319973438978195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06963987834751606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442920468747616 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16196910291910172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015280209481716156 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23320014588534832 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2334669008851051 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486017420887947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16717985272407532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05470006726682186 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233993478119373 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20514894276857376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20121014676988125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08931988850235939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10376982390880585 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20786002278327942 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13311998918652534 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3885999321937561 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2494870461523533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05109002813696861 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.672507030889392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2263900823891163 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25253999046981335 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.221048068255186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07772981189191341 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17051887698471546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04909001290798187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2297000028192997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03899889998137951 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0703700352460146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07539987564086914 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16236002556979656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047930050641298294 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2309088595211506 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2336568906903267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074820127338171 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16387016512453556 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049480004236102104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22938894107937813 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05473988130688667 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06922008469700813 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07521896623075008 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16379891894757748 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05021016113460064 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015570083633065224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2310900017619133 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2519580777734518 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.531625097617507 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07551000453531742 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16806996427476406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04988000728189945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23002899251878262 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039739999920129776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06965990178287029 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581012323498726 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16344012692570686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05010911263525486 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016039935871958733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23077009245753288 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2427279725670815 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07534888572990894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16623898409307003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054080039262771606 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0946300569921732 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20173913799226284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09350012987852097 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19348016940057278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0930391252040863 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10496005415916443 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2344499807804823 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13406993821263313 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42138900607824326 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2836670503020287 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052110059186816216 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.709755975753069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076669966802001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16643991693854332 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05140993744134903 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23049907758831978 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04624994471669197 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06862008012831211 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518893107771873 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16348902136087418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049690017476677895 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0156499445438385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23130979388952255 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2598680332303047 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.3232920579612255 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07562991231679916 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16650999896228313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42789895087480545 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039899954572319984 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07110997103154659 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0760289840400219 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16669905744493008 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06046006456017494 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018730061128735542 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2349198330193758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07130904123187065 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5614470466971397 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07574004121124744 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1654098741710186 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05082995630800724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42773899622261524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04059984348714352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06930995732545853 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524993270635605 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16335910186171532 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049560097977519035 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23178011178970337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07136003114283085 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5295471530407667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07557985372841358 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1652499195188284 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04931981675326824 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4278190899640322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04035979509353638 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06992905400693417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16314000822603703 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05001993849873543 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583993434906006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2308790571987629 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07057003676891327 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.527457032352686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22772885859012604 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3255091141909361 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.107969976961613 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07628998719155788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16701011918485165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04847906529903412 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42799999937415123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03986898809671402 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07095001637935638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538986392319202 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628200989216566 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049530062824487686 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015399884432554245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23089908063411713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07225992158055305 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5274770557880402 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18003908917307854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05425000563263893 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09825988672673702 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20258896984159946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09230012074112892 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20172004587948322 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0910500530153513 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10417005978524685 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21195015870034695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1336799468845129 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3942891489714384 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.263618003576994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05185883492231369 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7030059825628996 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07680989801883698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1674701925367117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05000992678105831 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4271289799362421 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398498959839344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07066014222800732 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07567903958261013 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16394886188209057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049519818276166916 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2309300471097231 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07146899588406086 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5299872029572725 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07606018334627151 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1649500336498022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05470006726682186 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09431014768779278 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2031298354268074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09085983037948608 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20086904987692833 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09156996384263039 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10307994671165943 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2100400160998106 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13323896564543247 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39166887290775776 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2566070072352886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0521000474691391 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6799070872366428 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07699988782405853 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16861897893249989 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049659982323646545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32831006683409214 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04031904973089695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06961007602512836 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07625017315149307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1647300086915493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015750061720609665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2295591402798891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059789977967739105 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.41969695687294 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0758699607104063 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16635889187455177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053209951147437096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09345985017716885 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20174914970993996 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0925001222640276 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20097987726330757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09949901141226292 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1037099864333868 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21100020967423916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13317004777491093 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39323000237345695 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2727179564535618 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0513899140059948 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7104558646678925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2313000150024891 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31920988112688065 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.11557993479073 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13197981752455235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22500986233353615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05284999497234821 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32697897404432297 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04058005288243294 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0692000612616539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07540895603597164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16399892047047615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0498499721288681 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015520025044679642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13189995661377907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06094016134738922 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3812680263072252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472909055650234 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643691211938858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04276004619896412 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10122009553015232 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.36054919473826885 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09293993934988976 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3467791248112917 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1046301331371069 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10929000563919544 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2550890203565359 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09000999853014946 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39609894156455994 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5961469616740942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0407099723815918 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9973160233348608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09174901060760021 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18367893062531948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04998990334570408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22891000844538212 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04053907468914986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06896979175508022 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459009066224098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16255001537501812 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015659956261515617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13241893611848354 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04963995888829231 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2253369204699993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748801976442337 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.164759811013937 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04296004772186279 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09409012272953987 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34640892408788204 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09227008558809757 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34597888588905334 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09116996079683304 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1046301331371069 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2475499641150236 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08998904377222061 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3850189968943596 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5384771395474672 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04105991683900356 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9388159271329641 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08098012767732143 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17265998758375645 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051110051572322845 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17969985492527485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03996002487838268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687099527567625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444014772772789 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234884969890118 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05509005859494209 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015879981219768524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13248994946479797 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04388997331261635 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1663869954645634 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494003511965275 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16406015492975712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04296004772186279 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0951599795371294 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34845899790525436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3464990295469761 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09242002852261066 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10292907245457172 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2463089767843485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0891699455678463 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38403901271522045 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5424469020217657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0413600355386734 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9482269417494535 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2426300197839737 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3696091007441282 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.253830114379525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22138911299407482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37827901542186737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10438007302582264 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18525891937315464 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04069996066391468 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06938003934919834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08738995529711246 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17747003585100174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04995008930563927 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08542998693883419 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051579903811216354 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.420147018507123 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.086240004748106 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17867912538349628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03802007995545864 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09536999277770519 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5805520601570606 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09440979920327663 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.576352959498763 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10123010724782944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1279700081795454 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3625790122896433 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07006991654634476 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.48313895240426064 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.11702385544777 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035529956221580505 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.52075289003551 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16403011977672577 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2786389086395502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07474003359675407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.132458982989192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04120008088648319 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06915000267326832 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08396990597248077 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1730700023472309 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0492301769554615 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08294917643070221 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039929989725351334 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.200736965984106 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08459994569420815 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17535011284053326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03809994086623192 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09474903345108032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.587762825191021 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09541003964841366 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5887020640075207 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09470991790294647 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12704008258879185 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35657896660268307 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07548998109996319 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4874791484326124 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.14273301512003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035010045394301414 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.551162900403142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12305914424359798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23870891891419888 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13180007226765156 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040220096707344055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06786896847188473 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08713989518582821 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1783398911356926 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04909001290798187 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016089994460344315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.083989929407835 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039350008592009544 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1640270240604877 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08426979184150696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17375010065734386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.046309782192111015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09447010233998299 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.596631810069084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09330990724265575 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6159330047667027 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248917922377586 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12652995064854622 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35419012419879436 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0692990142852068 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4714189562946558 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.144403109326959 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036899931728839874 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.550423197448254 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.639129992574453 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08674990385770798 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02553989179432392 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1584789715707302 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 95.4280870500952 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.2072700299322605 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05789007991552353 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04271999932825565 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026380177587270737 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03760983236134052 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.261110020801425 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07115001790225506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0708091538399458 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16004894860088825 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05021016113460064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08495012298226357 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040800077840685844 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843009032309055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08475012145936489 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17220992594957352 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049140071496367455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016369856894016266 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0835498794913292 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9555078577250242 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0661101657897234 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1547390129417181 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037190038710832596 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09466009214520454 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5815529990941286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09366893209517002 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.580572782084346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09228009730577469 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12731901369988918 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.355319119989872 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06806012243032455 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4705991595983505 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.096564095467329 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034488970413804054 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.48658219911158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08423998951911926 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1735500991344452 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05004997365176678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08163019083440304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04153908230364323 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712996400892735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08562998846173286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17428002320230007 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04915008321404457 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01767999492585659 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0831889919936657 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9636881295591593 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08406979031860828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17348001711070538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03681983798742294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09348010644316673 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5666818730533123 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09429012425243855 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5716029815375805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09332899935543537 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12610014528036118 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35343016497790813 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06783008575439453 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4677390679717064 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.06932314299047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03428012132644653 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.46364302560687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07855985313653946 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1048799604177475 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.079350866377354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07776007987558842 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17098896205425262 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050339847803115845 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08057989180088043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04124012775719166 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802007555961609 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518986240029335 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16169901937246323 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04916987381875515 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015780096873641014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13123010285198689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03198999911546707 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.03996810503304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07452000863850117 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215885989367962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04195980727672577 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09326008148491383 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3477588761597872 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09242002852261066 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3449288196861744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09053992107510567 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10279007256031036 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24514994584023952 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08932896889746189 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38078916259109974 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5290069859474897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052029965445399284 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9362359307706356 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07616984657943249 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16455980949103832 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050168950110673904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13037980534136295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040789833292365074 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673199538141489 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07525901310145855 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16140914522111416 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015560071915388107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1314601395279169 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0294481180608273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738091766834259 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16535911709070206 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041950028389692307 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09350012987852097 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3488590009510517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09193993173539639 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34638913348317146 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917899888008833 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10256003588438034 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24428917095065117 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08968007750809193 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3801190759986639 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5324871055781841 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040220096707344055 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9288561306893826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1276389230042696 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15425914898514748 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.1943368054926395 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07692980580031872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16868999227881432 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049770111218094826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12993905693292618 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040700193494558334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06992998532950878 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075930031016469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17498992383480072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05001900717616081 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015730038285255432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.229639932513237 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03777979873120785 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2063579633831978 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07514888420701027 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16393885016441345 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05328003317117691 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09304983541369438 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2023798879235983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090370187535882 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1997698564082384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09186007082462311 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1027591060847044 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2094090450555086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13349996879696846 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3886190243065357 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2446080800145864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05092984065413475 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6613160260021687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0772599596530199 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16661989502608776 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05013006739318371 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22737914696335793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040770042687654495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06847013719379902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16324990428984165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049900030717253685 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01574982888996601 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2284690272063017 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2306380085647106 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749099999666214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16315001994371414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05292007699608803 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09535998106002808 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2012199256569147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09123003110289574 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20016892813146114 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032012894749641 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10320008732378483 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20861905068159103 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13174000196158886 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3870690707117319 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2441680300980806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051380135118961334 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6588270664215088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22506900131702423 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2513490617275238 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.1127981171011925 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07790001109242439 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17037009820342064 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05139014683663845 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2275488805025816 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040190061554312706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06857002153992653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075930031016469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.165638979524374 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050870003178715706 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2289600670337677 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2410080526024103 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07509905844926834 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16239890828728676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04998990334570408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22749998606741428 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040980055928230286 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06838911212980747 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498986087739468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16341987065970898 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05012005567550659 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015639932826161385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22857892327010632 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2257869821041822 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.510594902560115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07855985313653946 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18142000772058964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05021016113460064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22789905779063702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040770042687654495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06817001849412918 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07561896927654743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1632289495319128 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04940992221236229 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015659956261515617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.229269964620471 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2465780600905418 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07521011866629124 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16563991084694862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05282997153699398 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09370013140141964 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19809906370937824 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09238999336957932 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19369903020560741 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110989049077034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09657000191509724 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2021400723606348 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13226899318397045 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.379669014364481 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.231377013027668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050830189138650894 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.649267040193081 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07681990973651409 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1649400219321251 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05076918751001358 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22745993919670582 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040319981053471565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06826990284025669 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538986392319202 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16308901831507683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979013465344906 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2296192105859518 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2309469748288393 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.2246009688824415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07682992145419121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667100004851818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42416900396347046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04051998257637024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06937980651855469 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07573980838060379 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16271905042231083 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04958990029990673 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22885994985699654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06933021359145641 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5148571692407131 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16440986655652523 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936009645462036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4239189438521862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04037003964185715 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06900005973875523 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07514003664255142 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16214977949857712 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050320057198405266 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015350058674812317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22821896709501743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06943009793758392 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5101870521903038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07454981096088886 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16283011063933372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04955986514687538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4236989188939333 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04150997847318649 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684000551700592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07978011853992939 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16678008250892162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05047000013291836 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2285391092300415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06936001591384411 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5133668202906847 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22611883468925953 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30778907239437103 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.003969883546233 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07602991536259651 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1649500336498022 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049190130084753036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42430893518030643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04061986692249775 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0689290463924408 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07529999129474163 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16228994354605675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913005977869034 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015549827367067337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22900919429957867 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07057003676891327 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5115269925445318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07512001320719719 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16440986655652523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05286885425448418 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09501981548964977 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20045996643602848 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09174901060760021 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1997698564082384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09115994907915592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10455888696014881 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21311896853148937 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1321001909673214 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39341882802546024 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2549280654639006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05116988904774189 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.67199713177979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076299998909235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1653099898248911 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0492599792778492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4235990345478058 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04097982309758663 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06860005669295788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16243895515799522 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015570083633065224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22906996309757233 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07007992826402187 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5136378351598978 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483991794288158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16307015903294086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052919844165444374 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09298999793827534 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20094914361834526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09019998833537102 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1988699659705162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09199907071888447 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10246015153825283 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20963000133633614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1315500121563673 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3873598761856556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.240248093381524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051159877330064774 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6549061983823776 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07646996527910233 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664001028984785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04970002919435501 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3266790881752968 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04091998562216759 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680300872772932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07551000453531742 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16262894496321678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015829922631382942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22857892327010632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05965004675090313 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4135469682514668 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436005398631096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16284012235701084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05295989103615284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09359908290207386 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2002499531954527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912700779736042 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19870907999575138 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09114015847444534 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10246993042528629 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20768912509083748 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13296003453433514 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.386448809877038 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2393970973789692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05092006176710129 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6544971149414778 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23010908626019955 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31604920513927937 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.951140033081174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13160891830921173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22249901667237282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053969910368323326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3261489327996969 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04076980985701084 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07395981810986996 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076669966802001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16272999346256256 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13159890659153461 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06007007323205471 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3763271272182465 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444992661476135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16248994506895542 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04189019091427326 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0934298150241375 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34740008413791656 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09217998012900352 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3463400062173605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106891229748726 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1039199996739626 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24557998403906822 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08913013152778149 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3810501657426357 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5315869823098183 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040289945900440216 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9248458556830883 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09149988181889057 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18155016005039215 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05029002204537392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22823899053037167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04029995761811733 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06832997314631939 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548998109996319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16166898421943188 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048770103603601456 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0156499445438385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1314298715442419 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048690009862184525 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2120979372411966 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07561990059912205 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17720903269946575 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04297005943953991 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09349011816084385 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34667900763452053 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09133992716670036 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3455991391092539 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169010445475578 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10574003681540489 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24959910660982132 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08942000567913055 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3851188812404871 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5336668584495783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0402799341827631 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9425160717219114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0801389105618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1701789442449808 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05116988904774189 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1792600378394127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039006307721138 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06751879118382931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07559009827673435 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622599083930254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015500001609325409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13135885819792747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043419888243079185 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.148447161540389 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07495004683732986 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16170996241271496 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04263012669980526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09565893560647964 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34818006679415703 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122886694967747 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3470799420028925 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905089545994997 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10277004912495613 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24530012160539627 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08884980343282223 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3798399120569229 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5322368126362562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04030996933579445 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9246959127485752 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23881997913122177 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35800994373857975 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.102380067110062 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22169016301631927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3762200940400362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10447902604937553 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18076994456350803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040929997339844704 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843986921012402 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08616992272436619 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17603905871510506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050029950216412544 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015629921108484268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08285022340714931 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04386017099022865 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3808479998260736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0848590862005949 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1732790842652321 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03677979111671448 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09403005242347717 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.57661210000515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09366986341774464 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5884229000657797 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09213993325829506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12792902998626232 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35464903339743614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06743008270859718 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4684790037572384 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.0958039034158 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03458978608250618 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.488731924444437 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1645800657570362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27688988484442234 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1320289447903633 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040839891880750656 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07366994395852089 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08473009802401066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17725001089274883 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04937010817229748 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08298992179334164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039239879697561264 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.201817998662591 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08489005267620087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17349887639284134 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03672996535897255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09448989294469357 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5855930764228106 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09489920921623707 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5844328813254833 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0932198017835617 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12663006782531738 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3544790670275688 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802007555961609 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4691688809543848 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.103743894025683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03471993841230869 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.495012996718287 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12318883091211319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23895897902548313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07557007484138012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.131209846585989 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04078005440533161 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06729899905622005 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08392008021473885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17135008238255978 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050239963456988335 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015730038285255432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08328980766236782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03986898809671402 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1538472026586533 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08437014184892178 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.173620181158185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037029851227998734 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09344983845949173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5879621282219887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09395997039973736 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.592423163354397 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125890210270882 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266698818653822 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35399990156292915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06810901686549187 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46740914694964886 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.107563015073538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034289900213479996 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.499572984874249 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.449091060087085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08791987784206867 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02498994581401348 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11636991985142231 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.83476106449962 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20430982112884521 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.06582005880773067 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04249997437000275 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02550915814936161 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0362698920071125 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2680090256035328 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06876001134514809 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06642984226346016 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.153339933604002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048590125516057014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08242903277277946 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03929995000362396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712996400892735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08477992378175259 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16993004828691483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04815892316401005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016180099919438362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08314987644553185 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.931598013266921 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06273994222283363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14935992658138275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036980025470256805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09265006519854069 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.6001030821353197 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1050001010298729 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.598651848733425 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09436998516321182 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12866011820733547 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3574790898710489 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06831996142864227 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4707188345491886 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.15530400723219 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0342600978910923 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.54083290323615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08517899550497532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17245905473828316 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08167000487446785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0392301008105278 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673199538141489 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08480995893478394 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1702990848571062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04820013418793678 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015029916539788246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08313986472785473 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9349680040031672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08417898789048195 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16976892948150635 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036510173231363297 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09196996688842773 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.579032840207219 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09285891428589821 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5825129598379135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102001786231995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12803892605006695 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3574790898710489 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06944010965526104 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47186901792883873 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.087604073807597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03386917524039745 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.480991935357451 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07815007120370865 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10421010665595531 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.095581024885178 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0774699728935957 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1689500641077757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04903995431959629 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08097896352410316 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039090169593691826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068240100517869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07443991489708424 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16133999451994896 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053619034588336945 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.021529849618673325 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13781990855932236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03205006942152977 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0786280035972595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07499987259507179 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16249995678663254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190998151898384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09132991544902325 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3479388542473316 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017996490001678 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34487899392843246 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08864002302289009 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303989984095097 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24527986533939838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08877902291715145 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37869904190301895 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5180669724941254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03915000706911087 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9069360569119453 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487996481359005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16187992878258228 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0482101459056139 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13061892241239548 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03853999078273773 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06744987331330776 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346016354858875 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15767989680171013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04666997119784355 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015330035239458084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1321299932897091 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0103981476277113 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07336982525885105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591998152434826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04203012213110924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09175902232527733 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3481297753751278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08892896585166454 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3465202171355486 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08888007141649723 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10345992632210255 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24419999681413174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08942908607423306 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3779290709644556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5232258010655642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03976002335548401 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9088760018348694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12771994806826115 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15304004773497581 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.160098128020763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07556891068816185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1660690177232027 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04859990440309048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1302498858422041 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0387697946280241 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06881006993353367 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743598211556673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16143987886607647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048010144382715225 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014970079064369202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23086904548108578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037689926102757454 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.176017103716731 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07389998063445091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16028992831707 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05258992314338684 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09074900299310684 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20286999642848969 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08903001435101032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20060897804796696 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09047985076904297 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10351999662816525 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21033897064626217 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13361009769141674 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3883491735905409 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2354468926787376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05031982436776161 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6433570999652147 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483991794288158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1627090387046337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842015914618969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2284301444888115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038929982110857964 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0685499981045723 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426994852721691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15947013162076473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014869961887598038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298590261489153 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2104068882763386 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07309997454285622 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592598855495453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05310983397066593 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09246007539331913 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20308885723352432 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957996033132076 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20119990222156048 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08836016058921814 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10357005521655083 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20795012824237347 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13304012827575207 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3867389168590307 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2369169853627682 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05128001794219017 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6488470137119293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2264000941067934 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2519199624657631 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.03087805211544 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07740012370049953 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16950909048318863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0485200434923172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2288299147039652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04043895751237869 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843986921012402 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524015381932259 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16197003424167633 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048419926315546036 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015219906345009804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23020897060632706 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.224806997925043 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412000559270382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598200760781765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04839897155761719 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22852979600429535 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03827991895377636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06729993037879467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16060913912951946 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014839926734566689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2299200277775526 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2083668261766434 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.475464018061757 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07876986637711525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16697007231414318 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2287188544869423 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915000706911087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06821984425187111 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498986087739468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601900439709425 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860898479819298 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015149824321269989 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23004994727671146 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2182279024273157 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433886639773846 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16287900507450104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05293986760079861 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0933399423956871 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20542903803288937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09154016152024269 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19536982290446758 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09055901318788528 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09616999886929989 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.1993600744754076 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13248994946479797 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37618004716932774 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.229338115081191 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0505989883095026 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6412260010838509 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07572001777589321 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16250996850430965 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2290690317749977 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038869911804795265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06733997724950314 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422990165650845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15983986668288708 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501990482211113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302199136465788 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2132979463785887 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.167721839621663 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0750301405787468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616189256310463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4270190838724375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03881007432937622 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068610068410635 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465993985533714 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15978002920746803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795985296368599 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01498986966907978 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298988401889801 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06975000724196434 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4953070785850286 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07714005187153816 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17754919826984406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0473798718303442 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42678904719650745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03948016092181206 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06828992627561092 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418007589876652 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15917979180812836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04839012399315834 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01446995884180069 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2282888162881136 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0688300933688879 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5069169458001852 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385993376374245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15910901129245758 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047350069507956505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4232088103890419 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03912998363375664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06816000677645206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07337983697652817 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15866011381149292 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049029942601919174 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014769844710826874 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22856914438307285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06884988397359848 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4836671762168407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22518890909850597 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3045988269150257 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.944059997797012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16129016876220703 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04819990135729313 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42394897900521755 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04057004116475582 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06948900409042835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480988278985023 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16013020649552345 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047770095989108086 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014549819752573967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2283791545778513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06960984319448471 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.493537100031972 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0732101034373045 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15897001139819622 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05231890827417374 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09257020428776741 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20150002092123032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08966890163719654 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1986201386898756 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08894992060959339 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10289903730154037 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2076688688248396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13264990411698818 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3856488037854433 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2321581598371267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05029980093240738 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6381870955228806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07656984962522984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16381009481847286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742993041872978 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42348913848400116 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879005089402199 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06787991151213646 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07529905997216702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596990041434765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015009893104434013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2288601826876402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07021892815828323 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4921070542186499 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394980639219284 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15986012294888496 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052419956773519516 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149988181889057 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20068883895874023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08966983295977116 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1993090845644474 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08908985182642937 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10257004760205746 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20714011043310165 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13178889639675617 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3835591487586498 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.230867113918066 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05076988600194454 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6371170058846474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07601012475788593 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16480009071528912 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048369867727160454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3256190102547407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03861007280647755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789015606045723 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459009066224098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16062892973423004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047449953854084015 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2283800859004259 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05914899520576 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3849071692675352 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422990165650845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15968014486134052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052480027079582214 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09203003719449043 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20073889754712582 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08990010246634483 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20728912204504013 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09073992259800434 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10280986316502094 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20635989494621754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.131858978420496 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3817090764641762 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2359069660305977 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0502800103276968 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6443668864667416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2292189747095108 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31393906101584435 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.813779965043068 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13089017011225224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21964986808598042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05230889655649662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32569002360105515 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03968016244471073 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06744894199073315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1577800139784813 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046969857066869736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13072905130684376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05974993109703064 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3456568121910095 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07479009218513966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16026990488171577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041830120608210564 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09131897240877151 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3469299990683794 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11471891775727272 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3670100122690201 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11092983186244965 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10446994565427303 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2464901190251112 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08918903768062592 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3802089486271143 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6077060718089342 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04030996933579445 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9956561736762524 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09097997099161148 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17895991913974285 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04835985600948334 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22786902263760567 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950996324419975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06860005669295788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07270881906151772 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15734904445707798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015059951692819595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13100006617605686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048479996621608734 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1925080325454473 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333909161388874 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15898910351097584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041640130802989006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09183003567159176 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34845899790525436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08930987678468227 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34493906423449516 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08854991756379604 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1029700506478548 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24357996881008148 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08886982686817646 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37625920958817005 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.514767063781619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03958004526793957 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8989259842783213 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08017010986804962 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16760011203587055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06942893378436565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18159998580813408 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03970996476709843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06692996248602867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15887897461652756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04823016934096813 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13129995204508305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042629893869161606 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1538979597389698 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07246993482112885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15788013115525246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042579835280776024 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104004129767418 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34698890522122383 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968985639512539 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34487899392843246 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08977996185421944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10276888497173786 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24352897889912128 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08858018554747105 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37580891512334347 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5118170995265245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039659906178712845 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8962160684168339 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23935013450682163 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35771005786955357 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.051360120996833 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22042985074222088 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37356000393629074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10444899089634418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18239999189972878 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05032983608543873 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07347017526626587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08439901284873486 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17174892127513885 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015380093827843666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08243019692599773 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04316004924476147 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3882380444556475 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08454895578324795 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17051887698471546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036800047382712364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09379000402987003 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.57627309858799 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09301886893808842 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5811529960483313 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0929099041968584 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12678885832428932 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3540290053933859 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46643917448818684 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.087083930149674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03489013761281967 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.471952984109521 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16277004033327103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2749599516391754 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07422990165650845 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1312789972871542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03971997648477554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06732996553182602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08332985453307629 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16812002286314964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477801077067852 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459023416042328 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0833701342344284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03866013139486313 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1706780642271042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08422997780144215 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16976986080408096 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03582984209060669 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09183911606669426 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.581642871722579 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09329989552497864 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5918918438255787 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09329011663794518 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.126739963889122 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.352699076756835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802007555961609 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46523893252015114 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.098332909867167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03423006273806095 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.481412893161178 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11969986371695995 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23373891599476337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07467018440365791 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13072998262941837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039250124245882034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067130196839571 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08371006697416306 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16859895549714565 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04801992326974869 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014990102499723434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08359015919268131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04164990969002247 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1399080976843834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08490006439387798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1745489425957203 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036659883335232735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09277998469769955 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.584172111004591 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102001786231995 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.585333004593849 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037996642291546 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12873904779553413 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3562688361853361 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765988655388355 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46930904500186443 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.104023989289999 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03419886343181133 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.493792032822967 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.370859963819385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08797994814813137 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024789944291114807 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11239992454648018 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 93.30641198903322 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19112997688353062 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.046140048652887344 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.039980048313736916 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024999957531690598 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03600004129111767 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23984001018106937 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06937002763152122 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0652889721095562 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15197903849184513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048530055209994316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08255988359451294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040049897506833076 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749993190169334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08437898941338062 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1693291123956442 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048220157623291016 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08249003440141678 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9284780826419592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.061690108850598335 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14762021601200104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03629992716014385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254016913473606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5938629880547523 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09206891991198063 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.596533089876175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0924400519579649 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12692995369434357 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3533889539539814 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06859982386231422 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46755908988416195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.11474397778511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03366987220942974 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.491713088005781 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08406885899603367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17138919793069363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795007407665253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0817298423498869 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039739999920129776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06674998439848423 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08501997217535973 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17077894881367683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014939811080694199 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08201994933187962 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9393980726599693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08335895836353302 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16968906857073307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036770012229681015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09155017323791981 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.58176208101213 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5857229959219694 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067007340490818 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12644892558455467 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35256892442703247 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736000068485737 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4637790843844414 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.081213803961873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03374996595084667 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.465582970529795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07758988067507744 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10251998901367188 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 19.027691101655364 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07713004015386105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1676301471889019 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07995916530489922 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039220089092850685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06729015149176121 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366016507148743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1570400781929493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734005779027939 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1308999489992857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03144005313515663 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0129280854016542 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159549992531538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041209859773516655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09103980846703053 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3466191701591015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978997357189655 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34399889409542084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08885003626346588 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1022599171847105 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24375994689762592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08851895108819008 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37661916576325893 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5130171086639166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03927992656826973 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8969359807670116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747600570321083 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607700251042843 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047980109229683876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1294498797506094 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03947014920413494 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06722006946802139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07375003769993782 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15733996406197548 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04663015715777874 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014740042388439178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13143010437488556 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0069480631500483 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731500331312418 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15747989527881145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042289961129426956 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09074993431568146 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.346840126439929 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909008465707302 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.344360014423728 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911988697946072 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10274001397192478 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24259998463094234 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08911010809242725 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37546013481914997 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5158068854361773 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03930996172130108 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.902185846120119 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12667011469602585 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15181978233158588 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.078208098188043 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07616006769239902 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16561895608901978 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04813005216419697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12978003360331059 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879005089402199 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680300872772932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742089468985796 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15977886505424976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04746997728943825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01488998532295227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298201434314251 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03700004890561104 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1831270530819893 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426994852721691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16085011884570122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052560120820999146 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09178020991384983 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20108907483518124 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902798492461443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19921897910535336 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895101111382246 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10226992890238762 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20584999583661556 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.132458982989192 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38177892565727234 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.228007022291422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05015009082853794 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6375768464058638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541989907622337 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1617399975657463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048099085688591 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22936007007956505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039280159398913383 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06771995685994625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442990317940712 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16025896184146404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014750054106116295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22839894518256187 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2095770798623562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07331999950110912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15977001748979092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0524700153619051 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202002547681332 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20131911151111126 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944002911448479 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2058190293610096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08895993232727051 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10282988660037518 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20657014101743698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13122917152941227 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3829591441899538 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2344869319349527 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050870003178715706 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6424369532614946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22459006868302822 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2537500113248825 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.026546936482191 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07811002433300018 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17005996778607368 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04899012856185436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2266489900648594 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950018435716629 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06772996857762337 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502990774810314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16081985086202621 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04859990440309048 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014919787645339966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22843899205327034 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2173180002719164 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596398651599884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22667879238724709 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03893999382853508 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06792997010052204 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735200010240078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15903892926871777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04859990440309048 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01482013612985611 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22816983982920647 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2034678366035223 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.463206183165312 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07443013601005077 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1627691090106964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789978265762329 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22760010324418545 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04009902477264404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06900005973875523 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07425015792250633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159720191732049 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894006997346878 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22871908731758595 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.212986884638667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07452000863850117 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16260985285043716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05257898010313511 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09275996126234531 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19605993293225765 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09070895612239838 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1927199773490429 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09606895036995411 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19984901882708073 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.131980050355196 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3757891245186329 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2142681516706944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05009002052247524 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.62387709133327 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07572979666292667 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16265898011624813 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879990592598915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22658007219433784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03927014768123627 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06778002716600895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475982420146465 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16399985179305077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04853983409702778 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22787903435528278 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2139470782130957 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.144191043451428 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08695991709828377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17601903527975082 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0485200434923172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.423758989199996 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592598855495453 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014980090782046318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22818916477262974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06845989264547825 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5046070329844952 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16060913912951946 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4226590972393751 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03863009624183178 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06808992475271225 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16181007958948612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047479989007115364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0145698431879282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.228038989007473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06873998790979385 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4906770084053278 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07383013144135475 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588691957294941 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04727998748421669 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4225189331918955 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03785989247262478 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06748014129698277 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585900317877531 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05442998372018337 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22759893909096718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06848014891147614 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4854068867862225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2251090481877327 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3046090714633465 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.937980091199279 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07447018288075924 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604999415576458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04707882180809975 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4233899526298046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918888978660107 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06869994103908539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15937001444399357 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04697008989751339 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22823922336101532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06952998228371143 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4856469351798296 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320987060666084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907897613942623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052449991926550865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09249988943338394 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20120013505220413 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08931988850235939 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1985500566661358 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08971989154815674 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10302010923624039 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20708004012703896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13099005445837975 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3825689200311899 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.228878041729331 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05036895163357258 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6345358453691006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0762399286031723 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16374001279473305 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04764995537698269 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.423249090090394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038880156353116035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06751995533704758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749099999666214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15854905359447002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015039928257465363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2283100038766861 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06915885023772717 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4894569758325815 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07306993938982487 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15862006694078445 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05237990990281105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09215995669364929 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20047882571816444 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09210011921823025 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19877892918884754 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08955993689596653 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10174000635743141 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20558014512062073 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13139890506863594 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38134888745844364 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2303970288485289 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05061016418039799 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6363868489861488 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075560063123703 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622901763767004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047930050641298294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3256888594478369 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039289938285946846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06786989979445934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515004836022854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16154907643795013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725996404886246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015249941498041153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23115985095500946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0588900875300169 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3937370385974646 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073309987783432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592300832271576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05255010910332203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110011160373688 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20050001330673695 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045004844665527 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19824900664389133 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08890987373888493 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10287994518876076 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2069601323455572 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13174908235669136 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3825691528618336 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2249869760125875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05038990639150143 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6305269673466682 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22976892068982124 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3148589748889208 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.811339899897575 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13099005445837975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.220439862459898 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05246000364422798 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32572890631854534 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03989017568528652 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06790994666516781 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400894537568092 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15875883400440216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04740990698337555 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159836038947105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1307399943470955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0668899156153202 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3580180238932371 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346994243562222 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15909899957478046 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042109983041882515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09245006367564201 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34614885225892067 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029009379446507 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34594908356666565 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016995318233967 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10327901691198349 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24391897022724152 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0891401432454586 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37658889777958393 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5160369221121073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0395399983972311 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9001159816980362 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0901601742953062 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1771699171513319 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048629939556121826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22793910466134548 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03921007737517357 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06766989827156067 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07381988689303398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15759002417325974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13070995919406414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04818011075258255 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1902269907295704 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07356004789471626 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15903008170425892 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04146900027990341 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091990068554878 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.34626899287104607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09651994332671165 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3463390748947859 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08943001739680767 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10276003740727901 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24243001826107502 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08899904787540436 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.375199131667614 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5183370560407639 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03957003355026245 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9011960830539465 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07945997640490532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677200198173523 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050629954785108566 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17882883548736572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03990018740296364 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06719003431499004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407995872199535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15841983258724213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047239940613508224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014980090782046318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13086991384625435 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04225992597639561 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.129247946664691 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07279007695615292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15795906074345112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041590072214603424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09088008664548397 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3465190529823303 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983002044260502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.3454890102148056 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08890987373888493 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10288995690643787 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24398998357355595 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08908892050385475 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3774091601371765 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5137370210140944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039950013160705566 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8961960449814796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2388390712440014 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35758898593485355 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.94214997626841 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22145896218717098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3758289385586977 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10366016067564487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1798500306904316 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03940891474485397 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06763986311852932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08397991769015789 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1696101389825344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522991806268692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08240900933742523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04307017661631107 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3538568746298552 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08332985453307629 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16958988271653652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036590034142136574 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09213900193572044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5881130024790764 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0926898792386055 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.587262937799096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09050918743014336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12640003114938736 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35265996120870113 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067500164732337 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46494905836880207 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.093363139778376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03435020335018635 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.478203089907765 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1627288293093443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2748989500105381 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07470999844372272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13072998262941837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039629871025681496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06727012805640697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08481997065246105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16974005848169327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04850002005696297 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015380093827843666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08238013833761215 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03865011967718601 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1692771222442389 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08377991616725922 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.169999897480011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03614998422563076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09490014053881168 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.5926431883126497 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.597632981836796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906588975340128 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12618000619113445 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35266997292637825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06813905201852322 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46668900176882744 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.10885289683938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034190015867352486 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.506462909281254 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12000999413430691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23241015151143074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07506902329623699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13036001473665237 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03958004526793957 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06741005927324295 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08447002619504929 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1699291169643402 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0489500816911459 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015059951692819595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0822299625724554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0385800376534462 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.129027921706438 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08427002467215061 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17021899111568928 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036499928683042526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09176996536552906 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.587852930650115 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09191990830004215 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 3.584412159398198 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09116018190979958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12757000513374805 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3542490303516388 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06771995685994625 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4663490690290928 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 8.087974041700363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034079886972904205 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 8.472072891891003 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 29.333061072975397 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08974899537861347 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02455012872815132 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10594003833830357 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 92.90793188847601 ms for forwarding
