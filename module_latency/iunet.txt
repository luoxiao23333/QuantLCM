--------------------
No. 1
<class 'diffusers.models.embeddings.Timesteps'> take 0.6088879890739918 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.13384001795202494 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.11126906611025333 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.14114996884018183 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.058199046179652214 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.6203979719430208 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 1026.2212050147355 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.3636379260569811 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.663977931253612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.08117908146232367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1979890512302518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04622992128133774 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.2115190727636218 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10208890307694674 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1964089460670948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051059992983937263 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.04434899892657995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.10223989374935627 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 2.028393093496561 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09006005711853504 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19185896962881088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04758010618388653 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.13985997065901756 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.976016977801919 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0975889852270484 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22472906857728958 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09343994315713644 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.30342908576130867 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07843005005270243 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45627797953784466 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 2.2943520452827215 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03797002136707306 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.82001995947212 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09495997801423073 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21803996060043573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05105894524604082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09714998304843903 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04221999552100897 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07288902997970581 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09375996887683868 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1831400441005826 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052660005167126656 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016819918528199196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09127997327595949 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.06321694329381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09283993858844042 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18357892986387014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0495400745421648 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09524007327854633 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6173980655148625 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.092328991740942 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21366996224969625 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10936998296529055 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27914904057979584 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06930006202310324 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3977589076384902 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7130529740825295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036770012229681015 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1398620447143912 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09655894245952368 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.13765902258455753 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 8.431819034740329 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08316000457853079 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18713902682065964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05113007500767708 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09782996494323015 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04198902752250433 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08230993989855051 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07560895755887032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16428902745246887 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05012005567550659 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016849953681230545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1534890616312623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03539002500474453 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1366559192538261 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07510895375162363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16497902106493711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04518008790910244 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1039489870890975 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24892902001738548 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.092889997176826 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19817904103547335 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09041000157594681 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2031500916928053 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10660907719284296 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.36000797990709543 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2846550671383739 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04342000465840101 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6948740230873227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07662002462893724 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1673089573159814 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04962005186825991 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.15386904124170542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04287995398044586 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08058897219598293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07583003025501966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1719699939712882 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04952994640916586 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01646007876843214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.15423994045704603 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1323550716042519 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467996329069138 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16375002451241016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04542910028249025 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09306997526437044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21756894420832396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10055000893771648 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20557001698762178 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2035689540207386 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0934000127017498 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34646899439394474 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2580259935930371 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04340906161814928 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6651839250698686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.27493899688124657 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.30340906232595444 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.051627919077873 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.078729004599154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1743989996612072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05154998507350683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14274893328547478 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040779937990009785 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07318996358662844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07803004700690508 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1773190451785922 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04938000347465277 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01634994987398386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26182993315160275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04091998562216759 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2764949351549149 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07840001489967108 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17009908333420753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06569002289324999 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09635998867452145 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2622390165925026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09653903543949127 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2283990615978837 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923000043258071 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.15549908857792616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14838005881756544 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.35907793790102005 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3248350005596876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05542999133467674 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7730740364640951 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08533906657248735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17777900211513042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05231995601207018 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25257898960262537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04104000981897116 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08812895976006985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0753400381654501 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16475992742925882 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050149974413216114 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016239937394857407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2530389465391636 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3264149893075228 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07533002644777298 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1646790187805891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059089972637593746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09356997907161713 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21848897449672222 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11168897617608309 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21790899336338043 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09097007568925619 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1466990215703845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14101993292570114 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.33681897912174463 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.262604957446456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05528004840016365 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6951339785009623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26167905889451504 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2903289860114455 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.481867050752044 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07735996041446924 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1775000710040331 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05077000241726637 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2514799125492573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04004000220447779 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08922989945858717 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07614900823682547 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16670895274728537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05022005643695593 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01643004361540079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2530489582568407 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3248859904706478 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07616996299475431 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664099982008338 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049869995564222336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25173905305564404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04050903953611851 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07239996921271086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545901462435722 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16326899640262127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936009645462036 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015810015611350536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527990145608783 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2871649814769626 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.677759970538318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07649010512977839 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17089000903069973 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05043903365731239 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25255896616727114 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04032999277114868 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08586002513766289 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07658905815333128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16582896932959557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04994997289031744 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016210018657147884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526988973841071 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3127559795975685 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07761898450553417 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18317904323339462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05847995635122061 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1032890286296606 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24049903731793165 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1013799337670207 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21799898240715265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10147993452847004 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13554003089666367 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14047895092517138 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3250989830121398 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.28218496683985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05528994370251894 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7349340487271547 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0779799884185195 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1702290028333664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05001993849873543 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2524490701034665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041680061258375645 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07464992813766003 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07544993422925472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.163018936291337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048990012146532536 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016539939679205418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25337899569422007 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3082650257274508 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.7101330710574985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07906998507678509 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.172480009496212 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04980992525815964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4715679679065943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04071998409926891 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07362000178545713 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07627892773598433 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16582896932959557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04972994793206453 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015949946828186512 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2530190395191312 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08628901559859514 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6276739770546556 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07648009341210127 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16887905076146126 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049329944886267185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47094805631786585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04066003020852804 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07307995110750198 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751490006223321 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16281893476843834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049110036343336105 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015540048480033875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25330891367048025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07722899317741394 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6068239929154515 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07589010056108236 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1669389894232154 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47013896983116865 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978994209319353 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07266004104167223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07462897337973118 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631489722058177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015780096873641014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2531090285629034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07579999510198832 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5975539572536945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521589631214738 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.5445580463856459 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.612078937701881 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07714005187153816 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17949892207980156 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05021004471927881 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4712279187515378 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041140010580420494 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07588998414576054 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07723888847976923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664890442043543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04975008778274059 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016339938156306744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2536990214139223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07840897887945175 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6333539970219135 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07512001320719719 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16532896552234888 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05691999103873968 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09568000677973032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22341904696077108 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11215906124562025 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21697906777262688 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09166006930172443 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1467089168727398 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14102004934102297 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3381690476089716 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2683449313044548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05468004383146763 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7010039882734418 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566902786493301 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1658290857449174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05000003147870302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47030800487846136 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04045001696795225 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07308903150260448 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07588008884340525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16598997171968222 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04926894325762987 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016239937394857407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25341904256492853 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07706996984779835 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6086139949038625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07501896470785141 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.165638979524374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057279947213828564 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11061900295317173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21014909725636244 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09312003385275602 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21361897233873606 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09231001604348421 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1483600353822112 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14161900617182255 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.35081896930933 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2590050464496017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055329990573227406 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6925940290093422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07688009645789862 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1697099069133401 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05101901479065418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3615790046751499 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08356897160410881 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1658900873735547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05013903137296438 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016669975593686104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2530489582568407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06502994801849127 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5035850228741765 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532897870987654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16545900143682957 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0575199956074357 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09496905840933323 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21855905652046204 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09668001439422369 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21394900977611542 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09991996921598911 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.15152001287788153 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14010898303240538 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34136802423745394 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2559850001707673 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05491997580975294 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6875839792191982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2539190463721752 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34604803659021854 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.40312193799764 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14201889280229807 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23632904049009085 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05721999332308769 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3715880447998643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04181999247521162 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09349989704787731 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07611897308379412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17130898777395487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04916999023407698 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016110017895698547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14534906949847937 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06571004632860422 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4949650503695011 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07539999205619097 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1646089367568493 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045569962821900845 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0941490288823843 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20446896087378263 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09183993097394705 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19665900617837906 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09143003262579441 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2034190110862255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09345007129013538 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34617807250469923 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2313449988141656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045569962821900845 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6514440067112446 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09986991062760353 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19163894467055798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05101005081087351 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521390561014414 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04076899494975805 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07314002141356468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07558008655905724 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16309903003275394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04955998156219721 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016489997506141663 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14431902673095465 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05276000592857599 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2796049704775214 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0812190119177103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17719902098178864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04504004027694464 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09361898992210627 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20081002730876207 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917200231924653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1949290744960308 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012001100927591 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20473904442042112 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09307905565947294 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.36054791416972876 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2189450208097696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04320009611546993 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.643613912165165 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08606002666056156 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1879889750853181 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05000003147870302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19837904255837202 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039890059269964695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07193908095359802 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07730000652372837 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18680898938328028 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.054480042308568954 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01607008744031191 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14468992594629526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055530108511447906 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.254764967598021 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751300249248743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16403896734118462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04503992386162281 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09471992962062359 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20142900757491589 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09148905519396067 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.199079979211092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.12015900574624538 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20419899374246597 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09271001908928156 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3458489663898945 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2440159916877747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04244898445904255 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6585239209234715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26117905508726835 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3887490602210164 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.60078602656722 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2391589805483818 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.40521903429180384 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10683899745345116 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19947998225688934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04035898018628359 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07285003084689379 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09577011223882437 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18423900473862886 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04985998384654522 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016240053810179234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09145005606114864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05089992191642523 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4808150008320808 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10309903882443905 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19438902381807566 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03988994285464287 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10048900730907917 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6037179846316576 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09247008711099625 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22180890664458275 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09198894258588552 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2780290087684989 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06930995732545853 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3962090704590082 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6928940312936902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03697001375257969 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1279819775372744 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17452903557568789 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29079895466566086 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07833994459360838 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.15423900913447142 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04257005639374256 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07433898281306028 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09202002547681332 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18142000772058964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05001993849873543 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016440055333077908 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09094993583858013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04366005305200815 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2750349706038833 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09227998089045286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19196001812815666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03967992961406708 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09590003173798323 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6126180524006486 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167997632175684 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1939989160746336 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09052990935742855 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2789089921861887 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07476890459656715 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4177980590611696 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.694964012131095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03678002394735813 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1219529444351792 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13086001854389906 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24764996487647295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0792299397289753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14498003292828798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040758983232080936 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07196003571152687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09123899508267641 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1910589635372162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0515299616381526 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016549951396882534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0914689153432846 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042860046960413456 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2222359655424953 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09159999899566174 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18166902009397745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039380043745040894 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0941200414672494 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6072670221328735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09273004252463579 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19561906810849905 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091000538319349 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2761790528893471 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06941996980458498 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39290799759328365 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.666793948970735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03679003566503525 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0872929599136114 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.586881078779697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1017299946397543 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026850029826164246 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 4.3483939953148365 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 1097.7518169675022 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.28683897107839584 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.09751005563884974 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.05694897845387459 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.033279997296631336 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04488998092710972 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.3563089994713664 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.09198999032378197 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08394999895244837 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1815400319173932 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053709023632109165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09858992416411638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04115002229809761 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08014892227947712 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09548000525683165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18472992815077305 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05005905404686928 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018650083802640438 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09151000995188951 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.045736949890852 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07231999188661575 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17176906112581491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044109998270869255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09825907181948423 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6231180159375072 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09194901213049889 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1963799586519599 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915090786293149 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.280059059150517 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06951997056603432 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3986889496445656 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6851340187713504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036989920772612095 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1152020199224353 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09217998012900352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18270895816385746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05068001337349415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08998997509479523 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07280998397618532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09228894487023354 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1827289815992117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04966999404132366 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0910500530153513 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0154759511351585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0911300303414464 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18173002172261477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03998889587819576 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09333004709333181 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6069480441510677 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09166996460407972 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19369903020560741 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905599445104599 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2782390220090747 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0688689760863781 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3946979995816946 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6550440341234207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03679003566503525 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.067522960714996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08605897892266512 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11385895777493715 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.4712269231677055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07764995098114014 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17058895900845528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936999175697565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08801009971648455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07305992767214775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07522001396864653 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16216898802667856 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04921003710478544 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610000617802143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1442889915779233 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034190015867352486 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0661659762263298 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17897901125252247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04759000148624182 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09251991286873817 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.203389092348516 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09220896754413843 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19503000658005476 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901700695976615 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20155892707407475 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0928100198507309 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3423290327191353 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2011559447273612 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04317902494221926 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6320240683853626 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07909000851213932 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17353997100144625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049129012040793896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14358002226799726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040140002965927124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07290905341506004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743399141356349 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1615489600226283 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01607008744031191 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1445600064471364 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0723159648478031 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736600486561656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16043009236454964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04552002064883709 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153992868959904 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20174903329461813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012001100927591 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19408902153372765 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968997281044722 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20524999126791954 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09514903649687767 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3519980236887932 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2113559059798717 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04373001866042614 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6135150799527764 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13946904800832272 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1671789214015007 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.6623900309205055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08033891208469868 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17760891932994127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051119946874678135 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14236895367503166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039989943616092205 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07236003875732422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07563002873212099 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16284000594168901 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049119931645691395 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016000005416572094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526600146666169 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039950013160705566 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2453150702640414 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754200154915452 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16433896962553263 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05743000656366348 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09371002670377493 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21204899530857801 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09127904195338488 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2194090047851205 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09176996536552906 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14592893421649933 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1407500822097063 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3338989336043596 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2256759218871593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05507993046194315 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6532139852643013 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07991003803908825 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1712590456008911 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876998718827963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2553989179432392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04151999019086361 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07777998689562082 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07526006083935499 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16191008035093546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049080001190304756 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015850062482059002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525289310142398 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.30386499222368 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07447006646543741 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16110995784401894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05754898302257061 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09335996583104134 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20670902449637651 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23166905157268047 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0903689069673419 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14662009198218584 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14032900799065828 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3339290851727128 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2298159999772906 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05498994141817093 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6516040777787566 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24888908956199884 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2760789357125759 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.236576940864325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07717008702456951 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16851001419126987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049168942496180534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25094905868172646 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039860024116933346 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07318996358662844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467903196811676 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16085896641016006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01589988823980093 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2524390583857894 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.277585979551077 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08093996439129114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17316907178610563 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048749963752925396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2508290344849229 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03987003583461046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07185002323240042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07501000072807074 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16333896201103926 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0485200434923172 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015189987607300282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526089083403349 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2843849835917354 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.606951049529016 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07688906043767929 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1743389293551445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05040003452450037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25139900390058756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03994000144302845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07259903941303492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07321999873965979 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15945907216519117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821002949029207 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558999065309763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2524589654058218 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2847750913351774 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743489945307374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16346899792551994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05714001599699259 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252899326384068 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20107009913772345 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082991164177656 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21412898786365986 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09035994298756123 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1342189498245716 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14203903265297413 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3242379752919078 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1995650129392743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05476002115756273 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6248540487140417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07611897308379412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16542896628379822 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04905997775495052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510390477254987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07274898234754801 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07496995385736227 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162240001372993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898000042885542 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015820027329027653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25285896845161915 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2776850489899516 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.298793966881931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07703900337219238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16556901391595602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04903995431959629 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47014805022627115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03988004755228758 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0716290669515729 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07568998262286186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16270997002720833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049078953452408314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015359953977167606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525389427319169 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0757700763642788 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5907440101727843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423898205161095 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1626389566808939 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04862004425376654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4697380354627967 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04097004421055317 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07251999340951443 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746800797060132 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16199995297938585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04942994564771652 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558999065309763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527490723878145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0758500536903739 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5888640191406012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481907960027456 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16239902470260859 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0479499576613307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46978797763586044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04027003888040781 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0725799473002553 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503898814320564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16150902956724167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2529689809307456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07627904415130615 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5885840402916074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2498289104551077 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3378489054739475 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.263470928184688 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552898023277521 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16491906717419624 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05006999708712101 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4710580687969923 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025001544505358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07538998033851385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751300249248743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16314000822603703 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05039898678660393 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522002276033163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2537789987400174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07724994793534279 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6167339636012912 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414899300783873 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16206898726522923 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05755003076046705 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09485892951488495 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20985002629458904 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09297998622059822 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21538895089179277 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09353994391858578 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1464789966121316 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14051899779587984 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3341679694131017 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.224274979904294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05455000791698694 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6493239672854543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07596996147185564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16597902867943048 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913995508104563 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4699178971350193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039995837956667 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07301894947886467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466005627065897 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16189995221793652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04987895954400301 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015730038285255432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2529490739107132 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07662991993129253 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.606044010259211 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07544003892689943 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.164529075846076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05717005115002394 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09419897105544806 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2075090305879712 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09225006215274334 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21607906091958284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09133003186434507 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14661892782896757 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14106999151408672 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.334037933498621 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2188950786367059 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05495001096278429 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.647294033318758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07652898784726858 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1708389027044177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050029950216412544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.360728008672595 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039829988963902 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07265002932399511 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538904901593924 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643090508878231 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922004882246256 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2529390621930361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06459001451730728 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.483494066633284 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07496005855500698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644300064072013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0578899635002017 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09313004557043314 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2083790022879839 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09941996540874243 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2178190043196082 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09074900299310684 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1465399982407689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14094903599470854 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3356389934197068 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2309860903769732 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054570031352341175 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6593839973211288 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525890013203025 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34398899879306555 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.226442944258451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1415089936926961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.235428917221725 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05680997855961323 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3618390765041113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04071998409926891 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07416005246341228 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07596006616950035 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16169005539268255 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016009085811674595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14479993842542171 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06632006261497736 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.446484006009996 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07463002111762762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16244989819824696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04592898767441511 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09336997754871845 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19984901882708073 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09056006092578173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1936890184879303 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08990906644612551 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2025199355557561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09399896953254938 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34413894172757864 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1953959474340081 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04255003295838833 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5977650182321668 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10118004865944386 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1900800270959735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04961888771504164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25202007964253426 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07299997378140688 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07547903805971146 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16367901116609573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898000042885542 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016059959307312965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14416908379644156 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05229003727436066 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2666359543800354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516902405768633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16286899335682392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04513002932071686 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09182002395391464 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19985006656497717 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0945189967751503 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1941000809893012 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08966994937509298 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2037889789789915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09232002776116133 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34295895602554083 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.197185949422419 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04326901398599148 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.597894006408751 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08578994311392307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17824897076934576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04942005034536123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19812898244708776 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03979005850851536 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07229996845126152 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481896318495274 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612290507182479 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14424894470721483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04579999949783087 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1934159556403756 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07462897337973118 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624289434403181 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04488008562475443 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09163899812847376 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20093994680792093 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1943589886650443 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935004007071257 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2038689563050866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09271001908928156 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34351798240095377 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1965649900957942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04267995245754719 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5964640770107508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26073900517076254 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3904480254277587 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.308315929956734 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23897900246083736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39723794907331467 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10654004290699959 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1993790501728654 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039650010876357555 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07239996921271086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09287998545914888 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17902895342558622 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04883995279669762 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01611909829080105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09087997023016214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04796998109668493 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4377449406310916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09245006367564201 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1817189622670412 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03918004222214222 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09213900193572044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.603978056460619 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104888886213303 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19348005298525095 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.095619005151093 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2770090941339731 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06970996037125587 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3943289630115032 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6513039590790868 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03688992001116276 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0610219798982143 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17568899784237146 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29156904201954603 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07854902651160955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1447000540792942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040319981053471565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07167900912463665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0916400458663702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17993897199630737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05015998613089323 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01597008667886257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09088998194783926 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041738967411220074 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2461950536817312 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09158998727798462 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18018996343016624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03909005317837 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09278999641537666 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6193179870024323 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09295996278524399 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19291904754936695 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09014003444463015 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2772090956568718 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06934895645827055 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39325805846601725 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.663653994910419 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03687001299113035 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0699730375781655 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1292800297960639 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24498894345015287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07858895696699619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14494999777525663 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07128901779651642 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09104993660002947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17777003813534975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015850062482059002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09046995546668768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04227994941174984 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1906650615856051 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09126996155828238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17938995733857155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03987993113696575 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09339000098407269 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6049680523574352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09754893835633993 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19335001707077026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030010551214218 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2788490382954478 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068610068410635 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3953190753236413 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6536240000277758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03614998422563076 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0643030293285847 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.307342978194356 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09393901564180851 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02629996743053198 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.12552004773169756 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.832866980694234 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20045903511345387 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.048479996621608734 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04170997999608517 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025759916752576828 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.037980033084750175 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25057897437363863 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07363001350313425 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06988993845880032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15763891860842705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049959984607994556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09070895612239838 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018004983663559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07181998807936907 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09192002471536398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17854897305369377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979001823812723 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016069971024990082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09034993126988411 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9728060103952885 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06631005089730024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1540690427646041 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03906001802533865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09402993600815535 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6086369976401329 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09146006777882576 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1955189509317279 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09056006092578173 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2763390075415373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06853893864899874 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3904779441654682 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6497239703312516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03629003185778856 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0495429635047913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09072001557797194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17926900181919336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049650087021291256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08932908531278372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039995837956667 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07178995292633772 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09167997632175684 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17769902478903532 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04921993240714073 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016000005416572094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09007903281599283 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9776260703802109 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09021000005304813 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17800903879106045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0390299828723073 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09310990571975708 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6031569791957736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102001786231995 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19320903811603785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09089999366551638 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2917490201070905 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06909901276230812 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.406607985496521 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6545739490538836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0358800170943141 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0580230047926307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08531904313713312 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11215894483029842 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.273606908507645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07763993926346302 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17008907161653042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04919001366943121 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08761999197304249 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039359903894364834 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07144000846892595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07605005521327257 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161609030328691 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015930039808154106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14394905883818865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03345997538417578 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0518559720367193 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377995643764734 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15986908692866564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044479966163635254 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09311002213507891 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19986892584711313 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08977903053164482 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1967300195246935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900894317775965 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20277907606214285 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0928100198507309 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3410991048440337 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.18828599806875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04220893606543541 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.582204014994204 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07621000986546278 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16396003775298595 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860898479819298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14350994024425745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018004983663559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07143989205360413 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07563992403447628 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16932899598032236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049400026910007 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016170088201761246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14447898138314486 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0667459573596716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390999235212803 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16371998935937881 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045699067413806915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09183003567159176 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20099908579140902 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08994003292173147 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.195158994756639 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08832896128296852 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20322902128100395 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09188998956233263 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34114893060177565 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1874160263687372 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190008621662855 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.584884012117982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13851001858711243 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16531010624021292 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.55247999727726 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07671990897506475 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1686890609562397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968990106135607 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.141729018650949 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039770035073161125 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07260998245328665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07631001062691212 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623000716790557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015600002370774746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2517789835110307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03941997420042753 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2224450474604964 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566995918750763 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16478903125971556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057609984651207924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09344005957245827 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21345901768654585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09097892325371504 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21648895926773548 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14595896936953068 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1407400704920292 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3320389660075307 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2149460380896926 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055020092986524105 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6414439305663109 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07616996299475431 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16408006194978952 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25112892035394907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03911997191607952 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07166992872953415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437902968376875 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15954894479364157 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04964997060596943 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016279984265565872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527190372347832 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2680159416049719 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434003055095673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16019900795072317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05718995817005634 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134900756180286 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20532903727144003 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09097997099161148 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21384900901466608 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09005004540085793 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14717900194227695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14152994845062494 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.337188015691936 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2084050104022026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055999960750341415 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6404340276494622 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2487290184944868 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.275118974968791 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.150568020530045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07647997699677944 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1662799622863531 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894996527582407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25057897437363863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03956898581236601 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0717799412086606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426994852721691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596990041434765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015850062482059002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.253599020652473 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.266874955035746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738389790058136 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15877897385507822 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048959976993501186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25057897437363863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039529986679553986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0716489739716053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435993757098913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159619958139956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048888963647186756 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015060068108141422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516689710319042 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2574050342664123 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.56736995652318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07373001426458359 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16142893582582474 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803994670510292 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2506989985704422 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978004679083824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07187994197010994 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422000635415316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16257003881037235 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876998718827963 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.022748950868844986 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25213893968611956 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2741349637508392 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515004836022854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16468006651848555 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056869001127779484 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09222002699971199 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1960990484803915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0907799694687128 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2129890490323305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016902185976505 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13457995373755693 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14086905866861343 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32102898694574833 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1838460341095924 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053720083087682724 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6059749759733677 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07563002873212099 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634999644011259 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900002386420965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2506889868527651 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039738952182233334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07178995292633772 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446995005011559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922891907393932 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01576996874064207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25218899827450514 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2636460596695542 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.240154987201095 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07646903395652771 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637890236452222 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04916999023407698 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4704979946836829 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04200998228043318 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07306004408746958 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545994594693184 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16201997641474009 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521489514037967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07538998033851385 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5899839345365763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07484899833798409 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16173894982784986 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04782003816217184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46940799802541733 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03988004755228758 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07186993025243282 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749899772927165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067002434283495 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015118974260985851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2517899265512824 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07528997957706451 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5746839344501495 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457903120666742 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611689804121852 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46955805737525225 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025991074740887 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0717600341886282 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07644994184374809 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.164529075846076 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04836998414248228 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014810007996857166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525990130379796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07598893716931343 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5803140122443438 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24912902154028416 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32995897345244884 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.228040972724557 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07585994899272919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637190580368042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898989573121071 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4697679542005062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039929989725351334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07256900426000357 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751999905332923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16117002815008163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04819000605493784 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526689786463976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07628998719155788 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5823839930817485 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07460999768227339 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16251893248409033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057089957408607006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09304005652666092 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2084400039166212 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076902642846107 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21359906531870365 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09046006016433239 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14616898261010647 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14079001266509295 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.33208902459591627 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.210185931995511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054510077461600304 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6325339674949646 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07536006160080433 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17854897305369377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4706380423158407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039680046029388905 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07174001075327396 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754001084715128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1605189172551036 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873995203524828 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01597998198121786 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25257898960262537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07631897460669279 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5992539701983333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413001731038094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613890053704381 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05665002390742302 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09235006291419268 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20678900182247162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091896936297417 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21328998263925314 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08960999548435211 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14605897013098001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13940990902483463 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.33045897725969553 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.202726038172841 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05412998143583536 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6229439061135054 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07525004912167788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16476900782436132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05059991963207722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36050903145223856 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04045001696795225 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07201009429991245 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07641001138836145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17013901378959417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25232904590666294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06385007873177528 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.472164993174374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423001807183027 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177992802113295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05667004734277725 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09242992382496595 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2061990089714527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09107007645070553 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21194899454712868 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08985900785773993 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14567002654075623 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1411390257999301 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3319989191368222 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2035059044137597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05467003211379051 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6256039962172508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521489514037967 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34356897231191397 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.092203039675951 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14105893205851316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23772893473505974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05618995055556297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36094803363084793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399700365960598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07266993634402752 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07559999357908964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16120902728289366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901993088424206 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015669967979192734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14440901577472687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06535998545587063 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.436954946257174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412990089505911 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607290469110012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04488998092710972 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09321898687630892 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20355894230306149 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09031000081449747 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19473896827548742 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969998452812433 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20268908701837063 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09282003156840801 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3410290228202939 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1922650737687945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042750034481287 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5882840380072594 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10054896119982004 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18947897478938103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04992994945496321 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25176897179335356 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039890059269964695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07174897473305464 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07496995385736227 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15984999481588602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0496589345857501 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015919911675155163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1441100612282753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05200004670768976 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2572449631989002 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742500415071845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16010995022952557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04438904579728842 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09182991925626993 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1988089643418789 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08926005102694035 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2066990127786994 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057007264345884 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2028000308200717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09201897773891687 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3408690681681037 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1977460235357285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04227005410939455 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5917150303721428 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08423998951911926 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17303996719419956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1972400350496173 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039479928091168404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07138994988054037 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743999844416976 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15939900185912848 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840001929551363 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015560071915388107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14365999959409237 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045689987018704414 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.173575990833342 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345003541558981 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15953904949128628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04466995596885681 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09245006367564201 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19907893147319555 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09425892494618893 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19408995285630226 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891189556568861 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2027689479291439 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09174994193017483 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.33981900196522474 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1878659715875983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04182907287031412 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5800939872860909 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26088894810527563 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3878689603880048 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.231077041476965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2379189245402813 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3957590088248253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10582990944385529 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19906996749341488 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04079996142536402 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07139891386032104 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09122001938521862 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17743906937539577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049900030717253685 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09028008207678795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047918991185724735 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4334049774333835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09184994269162416 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18342002294957638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038458965718746185 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214994497597218 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6040980806574225 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102909825742245 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1935299951583147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896799610927701 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27652899734675884 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06885000038892031 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39019889663904905 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6365650808438659 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03612996079027653 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.043452928774059 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17429899889975786 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2895889338105917 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07862993516027927 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14546001330018044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.042190076783299446 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07096899207681417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09220000356435776 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1797099830582738 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09153992868959904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04104990512132645 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2386050075292587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09156996384263039 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17867004498839378 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03867899067699909 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6033480167388916 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986901957541704 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1925190445035696 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08949008770287037 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27741899248212576 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06868003401905298 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.390687957406044 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6329040518030524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03620004281401634 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.035462995991111 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12595998123288155 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2418590011075139 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07833901327103376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1444999361410737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07088889833539724 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09164994116872549 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17807900439947844 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049769994802773 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015849946066737175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09022001177072525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04762900061905384 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1887550354003906 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0912700779736042 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17903000116348267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038819038309156895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09278010111302137 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6060680607333779 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896890414878726 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19118899945169687 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948997128754854 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27599907480180264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06927002687007189 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3902190364897251 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.635103952139616 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03623007796704769 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0373729057610035 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.211072978563607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0957300653681159 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026619993150234222 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11096999514847994 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 60.74259092565626 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20179897546768188 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.044720014557242393 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04075001925230026 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02671999391168356 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03699003718793392 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2448689192533493 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0727999722585082 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06939005106687546 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567190047353506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049939961172640324 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 4.4071340234950185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.09335007052868605 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11070002801716328 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10932004079222679 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.20952895283699036 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05073996726423502 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.02177001442760229 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09697000496089458 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 5.484169931150973 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06729003507643938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15676009934395552 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03985001239925623 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1012999564409256 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.640818034298718 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19740907009691 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045994374901056 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.284158973954618 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06861903239041567 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4006879171356559 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7083239508792758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03681995440274477 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1205730736255646 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09182002395391464 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1824389910325408 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049749971367418766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08952897042036057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0404199818149209 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07277994882315397 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09174994193017483 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18068007193505764 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04962005186825991 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016108970157802105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09088008664548397 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9897960117086768 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09003898594528437 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1792990369722247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039860024116933346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09290000889450312 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6050780648365617 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909899827092886 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19341905135661364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102001786231995 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27593900449573994 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06848899647593498 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39098807610571384 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.643013907596469 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036400044336915016 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0527729066088796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08540891576558352 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11324905790388584 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 10.86969010066241 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07772003300487995 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17047906294465065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880002234131098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08889997843652964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039006307721138 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07488008122891188 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757899833843112 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161609030328691 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049110036343336105 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016029924154281616 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1445600064471364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034610042348504066 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0666759917512536 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426994852721691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16250007320195436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044740037992596626 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1088900025933981 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20486896391957998 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09320909157395363 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1959499204531312 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09025901090353727 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20139897242188454 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09283993858844042 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3413090016692877 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2170060072094202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04292908124625683 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6172940377146006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552991155534983 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633400097489357 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048768939450383186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14305999502539635 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04009006079286337 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07299007847905159 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07470999844372272 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16064895316958427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910991992801428 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015930039808154106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1441290369257331 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0574560146778822 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377006113529205 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598299713805318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04560896195471287 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253993630409241 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2008589217439294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09093002881854773 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19453908316791058 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08975900709629059 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2043689601123333 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09238999336957932 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3433589590713382 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.194265903905034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0425499165430665 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5918640419840813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14754000585526228 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.17462007235735655 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.61506999656558 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07721001747995615 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1701490255072713 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049789901822805405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14212902169674635 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07261999417096376 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07569999434053898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1620299881324172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049119931645691395 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016018981114029884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523399889469147 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03984000068157911 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2280249502509832 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07499998901039362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637099776417017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05728995893150568 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09291991591453552 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2179689472541213 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09137904271483421 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21795998327434063 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072897955775261 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1464799279347062 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14225998893380165 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3358289832249284 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2287460267543793 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05418900400400162 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.656843931414187 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07629010360687971 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16379996668547392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04902901127934456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25146896950900555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040169921703636646 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07275003008544445 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07441896013915539 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1606689766049385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049440073780715466 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25313906371593475 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2807759921997786 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487996481359005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634190557524562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05764001980423927 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0942290062084794 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20652904640883207 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214994497597218 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21450896747410297 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082991164177656 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14704989735037088 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14153891243040562 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3350490005686879 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.215465017594397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055119977332651615 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6400839667767286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24860992562025785 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2754590241238475 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.186967017129064 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07671001367270947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16704003792256117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048558926209807396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25080901104956865 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03958004526793957 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07299997378140688 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749389873817563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234908252954483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048569985665380955 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016279984265565872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525389427319169 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.275945920497179 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422990165650845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16055896412581205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049080001190304756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516490640118718 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041619990952312946 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07248995825648308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07504003588110209 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16083905939012766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053080031648278236 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015249941498041153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25202904362231493 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2752149486914277 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.595751080662012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386901415884495 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16286899335682392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2515889937058091 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039470032788813114 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07347005885094404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07565994746983051 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16175000928342342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04918000195175409 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526789903640747 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2712250463664532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546995766460896 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656298991292715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05784002132713795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09313004557043314 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20316895097494125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253900498151779 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21444004960358143 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08988904301077127 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13454991858452559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14233903493732214 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32421899959445 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2007560580968857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054169911891222 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6283340519294143 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07686007302254438 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17143995501101017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04923902451992035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25144906248897314 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07283990271389484 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07615901995450258 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16316899564117193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04818988963961601 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016039935871958733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527190372347832 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.282976008951664 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.302624962292612 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07663003634661436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16645994037389755 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04868896212428808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4698989214375615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04046002868562937 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07266004104167223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07560895755887032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16208889428526163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873995203524828 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015199999324977398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25256897788494825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07595994975417852 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5909939538687468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16295001842081547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04893902223557234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46915910206735134 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07263000588864088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08996902033686638 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17694896087050438 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863005597144365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015260069631040096 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523789880797267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07575890049338341 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6011539846658707 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227003652602434 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04827894736081362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4700790159404278 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04046002868562937 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07310998626053333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453910075128078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16154907643795013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898000042885542 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525005791336298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25263894349336624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07597997318953276 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5853639924898744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24995894636958838 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3418190171942115 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.2865209290757775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0755189685150981 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1648090546950698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979001823812723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46964804641902447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04006002563983202 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07440999615937471 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07605995051562786 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16300997231155634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048459041863679886 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014950055629014969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2529589692130685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07799000013619661 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5984339406713843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07501896470785141 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16390904784202576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057270051911473274 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09461899753659964 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20758004393428564 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256904013454914 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21547905635088682 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913500552996993 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14637899585068226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14131003990769386 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.33460906706750393 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2187559623271227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05504000000655651 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6462840139865875 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548904977738857 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643890282139182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922994412481785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4698879783973098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04028005059808493 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07230998016893864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458904292434454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16162905376404524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04882004577666521 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521589631214738 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0763090793043375 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5859539853408933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07501000072807074 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16294000670313835 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057108933106064796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09868002962321043 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20916899666190147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09434903040528297 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21535996347665787 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09002001024782658 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14666898641735315 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1409699907526374 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.334019074216485 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2236860347911716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05478900857269764 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6484439838677645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07644994184374809 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16583898104727268 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049480004236102104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36042893771082163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04000996705144644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07279892452061176 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07603003177791834 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16334897372871637 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873995203524828 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015660072676837444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25264895521104336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0645200489088893 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4725649962201715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346004713326693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177899669855833 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05710998084396124 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09290897287428379 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2072189236059785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09185005910694599 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21505902986973524 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09068998042494059 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.14672998804599047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14232902321964502 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3360490081831813 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2164149666205049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05442998372018337 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6385240014642477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2524489536881447 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34633802715688944 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.153612936846912 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14118896797299385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23612903896719217 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05639996379613876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36115897819399834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04037993494421244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07338996510952711 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552909664809704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16171904280781746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048209913074970245 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01597008667886257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14525908045470715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06561004556715488 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4396349433809519 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07391907274723053 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607990125194192 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04578998778015375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0928100198507309 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2006900031119585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09075901471078396 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19568996503949165 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09044003672897816 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20409899298101664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09267998393625021 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3447489580139518 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1994760716333985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04256900865584612 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5981339383870363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10119006037712097 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19113905727863312 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05055998917669058 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25190901942551136 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07275003008544445 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07529009599238634 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16241997946053743 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04891003482043743 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015779980458319187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1442700158804655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0521800247952342 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2762449914589524 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743200071156025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16105000395327806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045428983867168427 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09671994484961033 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1997689250856638 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09005994070321321 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19411894027143717 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893190735951066 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.20205997861921787 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09269989095628262 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.34236907958984375 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1946860468015075 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042860046960413456 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5948140062391758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08221995085477829 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1716099213808775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04874903243035078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19765994511544704 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0401489669457078 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07210997864603996 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412000559270382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15950901433825493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04833005368709564 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01625996083021164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14455895870923996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04691991489380598 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1828060960397124 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442000787705183 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16244896687567234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0451699597761035 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09329000022262335 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20035903435200453 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905690249055624 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19314000383019447 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08932000491768122 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2031889744102955 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09218009654432535 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3420489374548197 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.191576011478901 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04232896026223898 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5944839688017964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2613690448924899 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3888190258294344 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.294817107729614 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23836898617446423 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3967389930039644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10679999832063913 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19997893832623959 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04009006079286337 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07193908095359802 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09227998089045286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1790500245988369 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050140079110860825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015810015611350536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04803005140274763 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4371039578691125 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09183993097394705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18023990560323 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03950891550630331 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10902993381023407 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6054779514670372 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078008588403463 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19414897542446852 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067007340490818 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2774890745058656 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06956898141652346 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39715797174721956 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.66821398306638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03648991696536541 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0776730962097645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17463008407503366 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.291039003059268 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07902004290372133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14511006884276867 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04002999048680067 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07402000483125448 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09218906052410603 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18015899695456028 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049959984607994556 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583003904670477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09167904499918222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0422400189563632 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.245275023393333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09177904576063156 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17998891416937113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03921007737517357 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09292003232985735 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6031279917806387 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09013991802930832 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19333907403051853 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08984992746263742 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27719896752387285 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06960995960980654 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39505900349467993 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6445440705865622 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03612996079027653 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0502820843830705 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12703007087111473 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24234899319708347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07886998355388641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14484906569123268 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07186003495007753 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09222002699971199 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18199894111603498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050200033001601696 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610000617802143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09056006092578173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04241999704390764 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1950660264119506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09263900574296713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1803890336304903 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0393999507650733 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09353901259601116 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6047479109838605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08982990402728319 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19337993580847979 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08961907587945461 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27687905821949244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07127004209905863 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39661896880716085 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6466140514239669 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03683904651552439 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0539519609883428 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.299931978806853 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09820004925131798 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.027049914933741093 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.181559007614851 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 65.88232202921063 ms for forwarding
--------------------
No. 2
<class 'diffusers.models.embeddings.Timesteps'> take 0.24849898181855679 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.054609961807727814 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.044680084101855755 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.035190023481845856 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03787002060562372 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.27542898897081614 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.13311905786395073 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09906908962875605 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.210339087061584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05264009814709425 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.10256899986416101 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04084000829607248 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08005998097360134 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08613895624876022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1752690877765417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04989001899957657 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016750069335103035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08536898531019688 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.056385925039649 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644688891246915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038470025174319744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09574997238814831 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6048469804227352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09290000889450312 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19839906599372625 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091000538319349 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2689590910449624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06586906965821981 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38557802326977253 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6602439573034644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03550003748387098 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0727129885926843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08801906369626522 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17838901840150356 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04996010102331638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08409901056438684 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0403400044888258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06949994713068008 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08769903797656298 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17606897745281458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04945998080074787 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01628999598324299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08466897998005152 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9705560514703393 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08730997797101736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17589004710316658 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038470025174319744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09430001955479383 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5589580396190286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09230896830558777 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19491894636303186 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091000538319349 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2646490465849638 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06563903298228979 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3780180122703314 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5916840638965368 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034920056350529194 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9919530022889376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0796089880168438 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10724901221692562 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.316476967185736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07733004167675972 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17032003961503506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04945008549839258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08298992179334164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04008994437754154 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07058901246637106 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07514993194490671 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16211997717618942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048339017666876316 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016309903003275394 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13407994993031025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03330002073198557 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0472460417076945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743999844416976 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16257993411272764 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0428489875048399 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0926699722185731 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1960289664566517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10681001003831625 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1963890390470624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08975900709629059 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1925990218296647 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08758006151765585 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32712891697883606 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1928260792046785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04103989340364933 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5911939553916454 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07585994899272919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16664003487676382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04953902680426836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13349996879696846 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07040903437882662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738800736144185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599700190126896 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048839021474123 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13390998356044292 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0404569329693913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736600486561656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16090995632112026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043629901483654976 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253993630409241 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19501906353980303 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09132002014666796 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19615900237113237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0894500408321619 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19269995391368866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08717901073396206 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32850797288119793 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1785950046032667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040779937990009785 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5738039510324597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12838898692280054 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15672901645302773 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.519680096767843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07798895239830017 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17077894881367683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05129002965986729 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1304789911955595 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903999458998442 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866990588605404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07528893183916807 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16289902850985527 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049119931645691395 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015870085917413235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23039896041154861 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03787991590797901 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.202485989779234 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07531989831477404 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16460905317217112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05410006269812584 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09307009167969227 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2101990394294262 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21435902453958988 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09083002805709839 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13721000868827105 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13058900367468596 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31618797220289707 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.198805053718388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05204998888075352 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6203239792957902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07717893458902836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16607902944087982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048840069212019444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23003900423645973 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017993342131376 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07012998685240746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07568998262286186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645000884309411 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048930058255791664 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015789992175996304 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23037998471409082 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.239664969034493 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08182006422430277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17065007705241442 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05465897265821695 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09278999641537666 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20353903528302908 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905599445104599 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2135690301656723 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09060895536094904 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13743003364652395 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13057899195700884 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31672907061874866 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1924559948965907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05189003422856331 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6185740241780877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22666901350021362 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2540689893066883 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.0438079526647925 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0764400465413928 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16885995864868164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04971993621438742 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22921909112483263 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040140002965927124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06984989158809185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387890946120024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602389384061098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806008655577898 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0160199124366045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23090897593647242 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.228726003319025 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364002522081137 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16072893049567938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048749963752925396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22857903968542814 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03994000144302845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06916897837072611 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477006874978542 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16160006634891033 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06525998469442129 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23147906176745892 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2374649522826076 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5122410152107477 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427902892231941 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644589938223362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936999175697565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2300790511071682 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040019978769123554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06985990330576897 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742500415071845 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16142998356372118 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901003558188677 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01574005000293255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2315390156581998 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.23418599832803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16507005784660578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05459901876747608 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923000043258071 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19912899006158113 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901099992915988 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21243910305202007 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08989905472844839 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12604997027665377 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13072905130684376 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3049090737476945 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1727759847417474 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05129002965986729 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5931150410324335 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07564900442957878 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16494898591190577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04882004577666521 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22910907864570618 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03912998363375664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06993894930928946 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07523002568632364 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17009000293910503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04835904110223055 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2307490212842822 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2346348958089948 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.1699649300426245 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07628905586898327 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1669389894232154 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048779998905956745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42760802898555994 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018004983663559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06918003782629967 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738990493118763 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16074907034635544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0485200434923172 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23038906510919333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07030903361737728 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5179039910435677 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07575994823127985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1636900706216693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824006464332342 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42663898784667253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03937899600714445 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06940995808690786 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07474003359675407 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16111903823912144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871992859989405 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23027905263006687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0719800591468811 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.515955082140863 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393001578748226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631999621167779 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42948906775563955 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041070044972002506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06960902828723192 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518997881561518 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16189995221793652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04872900899499655 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015400000847876072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23055903147906065 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07021997589617968 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.530974986962974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22778892889618874 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32353901769965887 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.052961991168559 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757100060582161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1638599205762148 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04882004577666521 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4276280524209142 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039830105379223824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07056002505123615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07448997348546982 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604800345376134 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048530055209994316 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23105996660888195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2628790680319071 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.7078439705073833 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07373001426458359 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215990763157606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05412998143583536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09383994620293379 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21237903274595737 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09215995669364929 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21272909361869097 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912289833649993 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13655994553118944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12956897262483835 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3136990126222372 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1986859608441591 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05133997183293104 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6175650525838137 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075930031016469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16476004384458065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04946906119585037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42364897672086954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03976002335548401 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06952905096113682 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506005931645632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630500191822648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01601001713424921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22905890364199877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0712500186637044 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.516174990683794 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415900472551584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1627790043130517 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05324999801814556 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09286904241889715 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20574894733726978 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917200231924653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2098290715366602 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09083002805709839 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13661908451467752 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13111007865518332 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31600799411535263 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1910450411960483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05125999450683594 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6100440407171845 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07625902071595192 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1670890487730503 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04955998156219721 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32656907569617033 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040199956856667995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07032009307295084 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541896775364876 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16166898421943188 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488299410790205 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595995854586363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22875901777297258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05947996396571398 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.406625029630959 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418997120112181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16352999955415726 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053570023737847805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09357999078929424 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20476907957345247 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09184004738926888 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2113690134137869 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09020906873047352 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13701000716537237 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12923905160278082 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.313769094645977 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1911960318684578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05119002889841795 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6099950298666954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.230289064347744 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3177189501002431 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.007823002524674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13191904872655869 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23699901066720486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05585001781582832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32703904435038567 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040619983337819576 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07070996798574924 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07678894326090813 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633788924664259 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048410031013190746 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015659956261515617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13216899242252111 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0605899840593338 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3975549954921007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429893594235182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16188900917768478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042980071157217026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09342003613710403 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19357004202902317 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048893116414547 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19429996609687805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08976995013654232 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19035895820707083 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08628994692116976 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.324658933095634 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1714559514075518 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04040903877466917 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5673839952796698 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09242002852261066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18251000437885523 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04992994945496321 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22880907636135817 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978994209319353 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06880995351821184 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566902786493301 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616890076547861 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483799958601594 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016020028851926327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13143906835466623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048939953558146954 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.220275997184217 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743689015507698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16193906776607037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04287005867809057 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0929889502003789 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19203999545425177 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972897194325924 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19330892246216536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970999624580145 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1903190277516842 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08670997340232134 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32533903140574694 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1665060883387923 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040479935705661774 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5638039913028479 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07986905984580517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.169298960827291 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936009645462036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1810290850698948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04108005668967962 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866001058369875 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434003055095673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608789898455143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048230052925646305 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13139890506863594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043190084397792816 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1473560007289052 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433898281306028 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16179902013391256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043020001612603664 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09165902156382799 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19305991008877754 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906200148165226 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19361893646419048 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890899682417512 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18948898650705814 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08825992699712515 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3270689630880952 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1669750092551112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040109967812895775 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5615440206602216 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23898901417851448 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3587889950722456 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.038698044605553 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2229389501735568 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37657900247722864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1042389776557684 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18079997971653938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04243000876158476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06854895036667585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0883500324562192 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17560890410095453 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04896009340882301 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015510013327002525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0831700162962079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044989981688559055 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3810750097036362 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0860999571159482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17491006292402744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037539051845669746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09370001498609781 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5555680254474282 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0903900945559144 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19398005679249763 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09096902795135975 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2626989735290408 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07386994548141956 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38510910235345364 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5904050087556243 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03482901956886053 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.98728300165385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1633589854463935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27704902458935976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13177900109440088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03997993189841509 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06880005821585655 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08479994721710682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1721000298857689 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900002386420965 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01584901474416256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08386000990867615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03962009213864803 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.188296009786427 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08555897511541843 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1733590615913272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037860008887946606 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09387894533574581 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5562079604715109 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248999413102865 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19447901286184788 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09060907177627087 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2634989796206355 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06492005195468664 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3764790017157793 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5844940207898617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03458000719547272 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9789219368249178 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12390001211315393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2361589577049017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13166898861527443 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0403400044888258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06833008956164122 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0851490767672658 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1724290195852518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04998000804334879 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015750061720609665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08321891073137522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04012999124825001 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1502259876579046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08453999180346727 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17263891641050577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0372700160369277 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09298010263592005 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5570580251514912 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0907989451661706 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19271904602646828 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09025994222611189 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26256905402988195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0656399643048644 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37674896884709597 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5821739798411727 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03511994145810604 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9794430118054152 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.894534945487976 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08546002209186554 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02587004564702511 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15696999616920948 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.964243904687464 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19848893862217665 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04584901034832001 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04315993282943964 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026360037736594677 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03621994983404875 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2637190045788884 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07141893729567528 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06983010098338127 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15673006419092417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050530070438981056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08349004201591015 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040689948946237564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0688089057803154 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08506001904606819 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17209001816809177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04945998080074787 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08320005144923925 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9459169814363122 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06620003841817379 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15287892892956734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03726989962160587 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09362900163978338 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5564580205827951 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19447901286184788 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08952897042036057 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2630290109664202 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0645200489088893 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37314894143491983 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5759340021759272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03436999395489693 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9640619866549969 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0846299808472395 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17411005683243275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04966894630342722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0820099376142025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04072999581694603 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06777001544833183 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08455000352114439 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17185998149216175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05071901250630617 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015609897673130035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0831099459901452 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9562869090586901 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08401903323829174 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17118907999247313 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037980033084750175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09247905109077692 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5534180672839284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09024003520607948 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19342906307429075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08977996185421944 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26263901963829994 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0638799974694848 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37161901127547026 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5669140266254544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03396009560674429 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9560829969123006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07832993287593126 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10510894935578108 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.0309479013085365 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07728894706815481 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1698789419606328 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04975998308509588 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08143996819853783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04038901533931494 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06896001286804676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0752198975533247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16023905482143164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049020047299563885 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015779980458319187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13103906530886889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032010022550821304 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0364559711888433 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745599390938878 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610890030860901 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042530009523034096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09204994421452284 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2004889538511634 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09178894106298685 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19465002696961164 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08829892612993717 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18876907415688038 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08561997674405575 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31979905907064676 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.16572598926723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03957992885261774 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5553750563412905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545005064457655 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16309996135532856 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968000575900078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12998899910598993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039529986679553986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06787001620978117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596800284460187 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04963995888829231 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015918980352580547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1317099668085575 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0239069815725088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731790205463767 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15820900443941355 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042350031435489655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09143899660557508 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19143999088555574 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08939893450587988 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1940900692716241 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08828006684780121 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18874905072152615 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08557992987334728 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32002897933125496 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1549160117283463 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0402289442718029 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5429540071636438 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12669002171605825 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1528299180790782 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.413440987467766 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07667008321732283 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16776903066784143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05097000394016504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12968899682164192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06852007936686277 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07597007788717747 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1620299881324172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04946999251842499 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22931001149117947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03753998316824436 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1938150273635983 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749700702726841 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16265898011624813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053419964388012886 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205995593219995 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2030490431934595 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094900451600552 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21187006495893002 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905400374904275 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13592897448688745 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1281199511140585 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30978897120803595 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.176405930891633 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05032890476286411 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5889540081843734 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749899772927165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1668500481173396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892901051789522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22727902978658676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03936991561204195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06809993647038937 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07454899605363607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15925895422697067 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2294090809300542 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.217596000060439 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366901263594627 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1658688997849822 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05385000258684158 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202898945659399 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20275998394936323 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978007826954126 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21368905436247587 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08945993613451719 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13575900811702013 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12866989709436893 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30947907362133265 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1764559894800186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050690025091171265 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5921940794214606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22449903190135956 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25094894226640463 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.945277982391417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581000681966543 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16604899428784847 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04958000499755144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22717902902513742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018004983663559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06907002534717321 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502001244574785 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16074907034635544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830991383641958 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22790906950831413 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.217226032167673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739500392228365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589289167895913 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04859000910073519 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22672908380627632 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04021998029202223 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06756896618753672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15973998233675957 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875892773270607 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22858893498778343 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2100350577384233 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.470100997015834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07478997576981783 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621090341359377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0489699887111783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22771896328777075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03960996400564909 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684099504724145 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438997272402048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15900901053100824 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0157100148499012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2283290959894657 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.211016089655459 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16329006757587194 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052939984016120434 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134993888437748 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19337900448590517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08947006426751614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20979903638362885 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08892000187188387 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12602994684129953 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12860901188105345 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30041905120015144 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1535059893503785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05005998536944389 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5662249643355608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07570895832031965 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16259902622550726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04915008321404457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22755900863558054 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039809965528547764 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759900134056807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604999415576458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05401903763413429 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22865901701152325 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2199049815535545 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.091355018317699 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16314908862113953 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4236990353092551 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03967992961406708 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06807991303503513 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744090648368001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159728922881186 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488299410790205 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22839894518256187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06918003782629967 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4984050067141652 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446995005011559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1618099631741643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048239948228001595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42315793689340353 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681800302118063 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07493002340197563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600600080564618 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048149959184229374 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015249010175466537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22818997967988253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06885896436870098 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4941450208425522 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07660000119358301 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1652390928938985 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789989907294512 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42389892041683197 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04112906754016876 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06880995351821184 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487006951123476 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601589610800147 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04836998414248228 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22850895766168833 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06945000495761633 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.503834966570139 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22521894425153732 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3052479587495327 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.956891993060708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748499296605587 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215897630900145 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04889990668743849 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231591010466218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03998901229351759 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06892997771501541 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516995538026094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16085896641016006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04831003025174141 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015169964171946049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22869894746690989 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0703700352460146 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4991849893704057 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596900401636958 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053060008212924004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09302003309130669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2040789695456624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09111000690609217 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21046900656074286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09690993465483189 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13682001736015081 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12854905799031258 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31064904760569334 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1847050627693534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050720060244202614 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5945639461278915 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07553002797067165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16300007700920105 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4234079970046878 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975990694016218 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06869994103908539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445993833243847 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15936000272631645 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04902901127934456 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016000005416572094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22871908731758595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0701600220054388 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.499554025940597 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364002522081137 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15973905101418495 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05272997077554464 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09224005043506622 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20232005044817924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09100907482206821 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21076900884509087 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08875993080437183 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1343489857390523 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12944010086357594 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3083390183746815 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1723560746759176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05068001337349415 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.580704003572464 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524003740400076 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16390893142670393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049749971367418766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3259789664298296 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041860039345920086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866897456347942 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516995538026094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16729999333620071 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796893335878849 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2286190865561366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05904003046452999 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4043449191376567 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357005961239338 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16002904158085585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05252007395029068 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917200231924653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20388897974044085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09108893573284149 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21014001686125994 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08954992517828941 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13621000107377768 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12873997911810875 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30925904866307974 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1745661031454802 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05032902117818594 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.584224053658545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298089675605297 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31674897763878107 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.692574967630208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1312691019847989 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21989899687469006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05361007060855627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32576906960457563 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040128943510353565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06880995351821184 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07592898327857256 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16666902229189873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048749963752925396 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0156499445438385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1315689878538251 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05950999911874533 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3622849946841598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07464899681508541 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16098900232464075 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04234001971781254 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09226007387042046 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19229005556553602 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957902900874615 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19312999211251736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893099932000041 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18964905757457018 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0855099642649293 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3211389994248748 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1577960103750229 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040140002965927124 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5486039919778705 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09264005348086357 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18108892254531384 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04992994945496321 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22735900711268187 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039590056985616684 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06821006536483765 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07452000863850117 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16006897203624249 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04861003253608942 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015780096873641014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13103894889354706 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04894996527582407 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2032159138470888 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07383001502603292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1597090158611536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04236993845552206 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167904499918222 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19115908071398735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09568000677973032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19297900144010782 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08898996748030186 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1893889857456088 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08547992911189795 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32022898085415363 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.15918496157974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039710081182420254 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5470640501007438 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07916893810033798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16682897694408894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04918000195175409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17969904001802206 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039289938285946846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06807001773267984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745690194889903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15997898299247026 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048690009862184525 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01576996874064207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13118900824338198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04270998761057854 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1328760301694274 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360894232988358 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16049901023507118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04197005182504654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913500552996993 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19066000822931528 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08899904787540436 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19216001965105534 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909008465707302 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18906896002590656 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08576002437621355 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32029906287789345 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.150095951743424 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03933999687433243 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.537214033305645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23800891358405352 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35722891334444284 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.901588036678731 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22090005222707987 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3744980785995722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10374002158641815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18062896560877562 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399099662899971 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820005364716053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08578994311392307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17103005666285753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049529015086591244 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015639932826161385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0831700162962079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04430999979376793 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3683850411325693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08612009696662426 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17757900059223175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03693997859954834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09336892981082201 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5553079536184669 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09009009227156639 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19357900600880384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09000906720757484 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26433903258293867 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06461003795266151 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.373919028788805 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.574274036101997 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03398896660655737 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9683820428326726 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16304897144436836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2756189787760377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1315189292654395 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04125991836190224 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06816990207880735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08472998160868883 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17145893070846796 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050080008804798126 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558999065309763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08302892092615366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039070029743015766 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1837560450658202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08498900569975376 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17254904378205538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037260004319250584 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214004967361689 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5566290346905589 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900999091565609 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1929589780047536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08885003626346588 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26243901811540127 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06492994725704193 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3725980641320348 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5685339458286762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034259981475770473 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9586930284276605 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12333900667726994 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23544905707240105 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0742599368095398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13136898633092642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712006870657206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08450995665043592 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17031002789735794 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053780036978423595 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08296000305563211 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03897002898156643 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1425060220062733 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08529901970177889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18743891268968582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0376399839296937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09186903480440378 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5566080799326301 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970999624580145 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19300903659313917 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08959893602877855 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2628489164635539 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06404996383935213 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37269899621605873 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5708039281889796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03429001662880182 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.977302017621696 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.825543966144323 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08695002179592848 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026340014301240444 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10960898362100124 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.53730801027268 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19903900101780891 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.045149936340749264 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.040689948946237564 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025810091756284237 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03692891914397478 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24386902805417776 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07023999933153391 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06618897896260023 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15556905418634415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05051994230598211 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08358003105968237 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040158978663384914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0689199659973383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08506898302584887 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17385894898325205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049930065870285034 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08293997962027788 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9478359716013074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06385997403413057 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.152299995534122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03714894410222769 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0932400580495596 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5596580449491739 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021896403282881 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1949999714270234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09007006883621216 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2640790771692991 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06395997479557991 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3741589607670903 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5798549866303802 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03441004082560539 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9664129940792918 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08400005754083395 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17282902263104916 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050140079110860825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08227000944316387 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03958994057029486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06776000373065472 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08541997522115707 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17237896099686623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04973995964974165 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016039935871958733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08284999057650566 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.946796964854002 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08304999209940434 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17121993005275726 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03730005118995905 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09263993706554174 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5558780394494534 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09163003414869308 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1939699286594987 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08937902748584747 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26351900305598974 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06431993097066879 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37284905556589365 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5742749674245715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03444997128099203 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9631830509752035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07794995326548815 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10427006054669619 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.031987955793738 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0769690377637744 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16807892825454473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0495400745421648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08081994019448757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03984896466135979 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681800302118063 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494993042200804 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907897613942623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04936999175697565 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01571991015225649 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13071903958916664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03168999683111906 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0260259732604027 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434003055095673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16148004215210676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042109983041882515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0919300364330411 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19058899488300085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09068998042494059 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19359902944415808 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08908892050385475 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1893599983304739 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08617900311946869 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32028905116021633 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1553260264918208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039860024116933346 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5583049971610308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07606006693094969 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1649789046496153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1306090271100402 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06890005897730589 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735399080440402 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922007150948048 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015598954632878304 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13138994108885527 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0263160802423954 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07332907989621162 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591190230101347 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04278996493667364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167904499918222 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19253999926149845 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08938007522374392 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19365898333489895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08819997310638428 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1887190155684948 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08515000808984041 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31883909832686186 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1522360146045685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039560021832585335 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.540353987365961 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12657989282160997 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1530490117147565 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.406740005128086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0773200299590826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16901001799851656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04996894858777523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12961996253579855 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040079932659864426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06813905201852322 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485994137823582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607899321243167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04912994336336851 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015630037523806095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22853899281471968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03724999260157347 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.184475957415998 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503898814320564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16235897783190012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05338003393262625 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915799755603075 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20224996842443943 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09158905595541 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2099090488627553 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08962000720202923 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13629905879497528 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.129729975014925 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31103903893381357 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1730761034414172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050739035941660404 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5849740011617541 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07671990897506475 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1639999682083726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04954892210662365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22755004465579987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03981008194386959 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06813998334109783 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516902405768633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16042892821133137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875997547060251 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23122900165617466 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2200759956613183 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446901872754097 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604489516466856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053480034694075584 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09196996688842773 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20255998242646456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010895155370235 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21045899484306574 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08839997462928295 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13788905926048756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13049994595348835 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32616895623505116 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1884360574185848 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050819944590330124 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5988439554348588 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2246189396828413 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25072903372347355 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.9404990170150995 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0763799762353301 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16757904086261988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910002462565899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2270790282636881 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03909994848072529 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06787001620978117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746200093999505 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598399830982089 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863005597144365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22834993433207273 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2161060003563762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343990728259087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1586099388077855 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048419926315546036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22692896891385317 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398589763790369 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06777001544833183 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415993604809046 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16009900718927383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0491999089717865 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015179975889623165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22837903816252947 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.209445996209979 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.468241029419005 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366901263594627 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611089101061225 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048990012146532536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22726890165358782 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03966991789638996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687099527567625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08247001096606255 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16850000247359276 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01537997741252184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22820988669991493 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2192149879410863 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07512001320719719 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16358995344489813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053078983910381794 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067996870726347 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1943489769473672 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893099932000041 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20957901142537594 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08906901348382235 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12510002125054598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12955907732248306 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3003389574587345 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1531759519129992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05006999708712101 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.565565005876124 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487903349101543 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616890076547861 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0484599731862545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22685900330543518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03966002259403467 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06771902553737164 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07523992098867893 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.160659896209836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049008987843990326 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579978935420513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22844993509352207 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2099649757146835 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.088724963366985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07582001853734255 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1629090402275324 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048990012146532536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4238489782437682 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04181894473731518 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0683399848639965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487006951123476 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16023905482143164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015330035239458084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22908905521035194 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06959005258977413 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5056850388646126 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494003511965275 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162909971550107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04833994898945093 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231079947203398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039689941331744194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06828899495303631 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429008837789297 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16023998614400625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0490689417347312 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015389989130198956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22961897775530815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06973999552428722 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4979239786043763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416901644319296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177899669855833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821002949029207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231279017403722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03962009213864803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06831996142864227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07380906026810408 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16018899623304605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048530055209994316 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2284690272063017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06877898704260588 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5104040503501892 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22569892462342978 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.307028996758163 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.977792035788298 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751490006223321 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16329903155565262 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04891003482043743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42314804159104824 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0689999433234334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07508997805416584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16136898193508387 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01504004467278719 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22839894518256187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07049902342259884 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5001640422269702 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370009552687407 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16001006588339806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05298899486660957 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09327998850494623 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20294892601668835 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09439897257834673 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2095300005748868 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901990570127964 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13604993000626564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12977898586541414 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31168898567557335 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1825860710814595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050369068048894405 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5933240065351129 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486005779355764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16321998555213213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898896440863609 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42401906102895737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04161009564995766 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06876001134514809 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473002187907696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16041903290897608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880991764366627 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579978935420513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22844900377094746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06955000571906567 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5030449721962214 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384899072349072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1605090219527483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05280005279928446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09323901031166315 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2040599938482046 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09153899736702442 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20911905448883772 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08925993461161852 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.135518959723413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1289800275117159 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30926894396543503 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1762359645217657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05059002432972193 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.586384023539722 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07537007331848145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16352906823158264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879001062363386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3250989830121398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042999353259802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0685789855197072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485994137823582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1603499986231327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875997547060251 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015380093827843666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22899999748915434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05929998587816954 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3926649698987603 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16106897965073586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05324999801814556 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09278999641537666 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20762905478477478 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09120895992964506 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20721904002130032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969998452812433 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13591896276921034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13015000149607658 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31158910132944584 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1785560054704547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05035905633121729 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5888840425759554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2292089629918337 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31564896926283836 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.692474966868758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13036001473665237 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22091891150921583 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05379004869610071 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.325858942233026 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04027003888040781 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06916990969330072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16011903062462807 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483199255540967 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015520025044679642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13125897385179996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05976005923002958 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.357685076072812 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743999844416976 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16078900080174208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04236004315316677 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09161897469311953 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20578899420797825 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906999921426177 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19344897009432316 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08893990889191628 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19003893248736858 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08575001265853643 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32112898770719767 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1699249735102057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04006002563983202 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5591341070830822 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09249907452613115 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1813389826565981 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05025998689234257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22785901091992855 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04067004192620516 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06872997619211674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437996100634336 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15979993622750044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049269990995526314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13093999586999416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048530055209994316 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2059459695592523 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07379008457064629 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16019900795072317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0418700510635972 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09111897088587284 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19138003699481487 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037007112056017 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19166897982358932 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08866994176059961 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18885894678533077 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08592999074608088 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32047892455011606 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1505659203976393 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04000996705144644 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5381439588963985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0866700429469347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17526897136121988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049329944886267185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1792890252545476 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03985001239925623 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680399825796485 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736989313736558 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15865894965827465 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048810034058988094 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015549943782389164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13035896699875593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04263001028448343 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.138516003265977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07319892756640911 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907909255474806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04248006734997034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106902871280909 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18967990763485432 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08958892431110144 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1926689874380827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08875003550201654 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18866907339543104 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08540996350347996 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31929893884807825 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.148016075603664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03967003431171179 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5357739757746458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23802905343472958 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3576889866963029 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.906438015401363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22142892703413963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37459901068359613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10374991688877344 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18005992751568556 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040680053643882275 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06816897075623274 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08533999789506197 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1727889757603407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04883005749434233 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583003904670477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08300994522869587 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04382990300655365 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3706350000575185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.085200066678226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17215008847415447 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036839977838099 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09276997298002243 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5553580122068524 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1926400000229478 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0894690165296197 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2628890797495842 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06450002547353506 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3729090094566345 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5699750510975718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0344900181517005 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9585430854931474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16257003881037235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.274888938292861 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1316390698775649 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06772007327526808 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08417898789048195 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17200899310410023 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04959001671522856 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015729921869933605 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08310901466757059 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03912008833140135 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1802460066974163 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08479994721710682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17181900329887867 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0369000481441617 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10881002526730299 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5557179683819413 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09001989383250475 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19245909061282873 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913991041481495 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2632889663800597 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06503891199827194 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3735780483111739 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5870139468461275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03414996899664402 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.974793034605682 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11877005454152822 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2316189929842949 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07465900853276253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13115000911056995 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04067004192620516 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0674789771437645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08538004476577044 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17201993614435196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04929001443088055 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08316000457853079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03900006413459778 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1347549734637141 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08445000275969505 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17143995501101017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03688992001116276 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104004129767418 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5546580068767071 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09004899766296148 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20030001178383827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910906035453081 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2622689353302121 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06442994344979525 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37146895192563534 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5728750731796026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03426999319344759 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9595630001276731 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.803724009543657 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0896198907867074 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026300083845853806 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11158001143485308 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.49583900999278 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19197002984583378 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04390894901007414 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04011997953057289 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02571998629719019 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03674009349197149 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24082895833998919 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.069828936830163 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06532005500048399 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1521500525996089 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049900030717253685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08314999286085367 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04036002792418003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06848899647593498 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08498993702232838 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1726200571283698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049690017476677895 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01576007343828678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08343998342752457 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9412569925189018 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06456999108195305 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15194900333881378 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03711006138473749 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09284890256822109 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.558768049813807 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09183993097394705 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19487901590764523 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012001100927591 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2633589319884777 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0639499630779028 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.372708891518414 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5811939956620336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03448990173637867 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9666529260575771 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08410005830228329 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1729800133034587 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050020054914057255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08175894618034363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03987993113696575 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06763997953385115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08478004019707441 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17135892994701862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050050090067088604 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015560071915388107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08269899990409613 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9452969534322619 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08370995055884123 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1705290051177144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03766000736504793 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09288010187447071 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5544079467654228 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09066902566701174 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19355001859366894 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08892000187188387 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2635589335113764 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06383995059877634 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3721690736711025 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5683750389143825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035069999285042286 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9722329452633858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07750000804662704 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10398006998002529 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.033058976754546 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07754005491733551 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.169259961694479 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049810041673481464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08054892532527447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04008004907518625 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866001058369875 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07522990927100182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16052997671067715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04827999509871006 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01571991015225649 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13105000834912062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032040057703852654 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0290970094501972 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343001198023558 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16013893764466047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04191999323666096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09191897697746754 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1910689752548933 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19408902153372765 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08841999806463718 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1891689607873559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08556002285331488 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32008904963731766 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.154475030489266 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039989943616092205 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.542604062706232 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535901386290789 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16271905042231083 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04905997775495052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13035908341407776 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040380051359534264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06836990360170603 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739790266379714 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15905906911939383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04934996832162142 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13118900824338198 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.022016047500074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08128990884870291 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664990559220314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04264002200216055 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915799755603075 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19307900220155716 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08819997310638428 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19457004964351654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08833897300064564 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1895199529826641 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08577900007367134 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32073899637907743 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1554560624063015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04000996705144644 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5505350893363357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1271700020879507 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15311897732317448 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.398809909820557 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07717893458902836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16984902322292328 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04964007530361414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13006909284740686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039019971154630184 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06859994027763605 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07540907245129347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16077898908406496 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048959976993501186 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015389989130198956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22815901320427656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036850105971097946 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1785360984504223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621190458536148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05350995343178511 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09273900650441647 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2028190065175295 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09564007632434368 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21135900169610977 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017996490001678 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13707892503589392 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.129429972730577 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3120379988104105 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.197664998471737 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05055998917669058 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6096940962597728 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07539894431829453 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637389650568366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04952994640916586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.227599055506289 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399099662899971 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06822892464697361 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488008122891188 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613099593669176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887895192950964 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579978935420513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2281989436596632 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2155850417912006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07367006037384272 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15900994185358286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053378986194729805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2020790707319975 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09027996566146612 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21547905635088682 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08960894774645567 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13613991905003786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13020902406424284 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3106090007349849 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.17775599937886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05094998050481081 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5860639978200197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2240600297227502 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25030900724232197 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.942477961070836 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16512000001966953 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050168950110673904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22729008924216032 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041070044972002506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06857002153992653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502897642552853 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611789921298623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015789992175996304 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2279889304190874 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2188759865239263 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07332907989621162 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15895895194262266 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843005444854498 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22705900482833385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991008270531893 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06801995914429426 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748300226405263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15920901205390692 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22792897652834654 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2068559881299734 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4681920185685158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502001244574785 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16199995297938585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22717902902513742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950006794184446 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07479998748749495 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16013893764466047 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04924007225781679 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22790906950831413 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2114960700273514 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467006798833609 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16316899564117193 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053209951147437096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09131000842899084 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20369910635054111 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125901851803064 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21247006952762604 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09007006883621216 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12516905553638935 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13026001397520304 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30095898546278477 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.173275988548994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05043903365731239 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5854340745136142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07561000529676676 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16318995039910078 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04946906119585037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2270090626552701 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04037993494421244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06804999429732561 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743489945307374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15964906197041273 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843005444854498 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015540048480033875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22792909294366837 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.212205970659852 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.10312507301569 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07533002644777298 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16325991600751877 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048690009862184525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.423689023591578 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03981997724622488 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0685789855197072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480999920517206 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16141997184604406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04930899012833834 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22875901777297258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06940006278455257 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5011249342933297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07448892574757338 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16900908667594194 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04882004577666521 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42374804615974426 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040350016206502914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06852007936686277 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16051903367042542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04872994031757116 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22824900224804878 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06903905887156725 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5051939990371466 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07460999768227339 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16161904204636812 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048479996621608734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4235190572217107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04002894274890423 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06842997390776873 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436005398631096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16005896031856537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048699905164539814 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01546996645629406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22797903511673212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06961997132748365 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4956550439819694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2259090542793274 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3063479671254754 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.9627210246399045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07565994746983051 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16267888713628054 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900002386420965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42373896576464176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04097900819033384 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06921007297933102 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494993042200804 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16019900795072317 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22865901701152325 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07003999780863523 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5078650321811438 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390999235212803 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611789921298623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0534900464117527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09332899935543537 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20265893545001745 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254005271941423 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21038902923464775 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13607903383672237 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1297900453209877 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3112090053036809 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1786549584940076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05092995706945658 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5905540203675628 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07534900214523077 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16732898075133562 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901993088424206 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42292801663279533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04011997953057289 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06827991455793381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455900777131319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600789837539196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04891003482043743 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016129924915730953 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22835901472717524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06977992597967386 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.503844978287816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16404001507908106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053089926950633526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09323994163423777 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20423904061317444 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09152002166956663 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21143897902220488 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051908273249865 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13557006604969501 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12833904474973679 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30882901046425104 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1785459937527776 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05119002889841795 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6049849800765514 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07613003253936768 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16499008052051067 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922004882246256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3253290196880698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040359911508858204 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06887991912662983 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07399998139590025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15972903929650784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04845007788389921 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22851896937936544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05890906322747469 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3909449335187674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07313000969588757 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16042008064687252 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05313905421644449 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09261991363018751 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20155904348939657 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21662900689989328 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09040895383805037 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13568007852882147 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12907898053526878 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3096889704465866 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1807959526777267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050690025091171265 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5904249157756567 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2287889365106821 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3149190451949835 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.72011499106884 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13080902863293886 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22059900220483541 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0538600143045187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32549898605793715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039899954572319984 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0691200839355588 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15920004807412624 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049030059017241 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015569967217743397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1308299833908677 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059409067034721375 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3541049556806684 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07337995339185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15936000272631645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0421090517193079 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254005271941423 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1909889979287982 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08917995728552341 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19348901696503162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08873897604644299 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18900004215538502 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08621893357485533 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.320998951792717 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.152865937910974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03951997496187687 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5396149829030037 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09253004100173712 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18319895025342703 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050239963456988335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22885901853442192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040548969991505146 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0682600075379014 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438997272402048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599190291017294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048749963752925396 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015520025044679642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13109901919960976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04904007073491812 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.209105015732348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419008761644363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039900947362185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04176003858447075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167008101940155 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19139901269227266 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896090641617775 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19251997582614422 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08860905654728413 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18921901937574148 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08574000094085932 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3201790386810899 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.15069595631212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04018004983663559 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.54013407882303 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07592001929879189 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16441999468952417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049919006414711475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17880997620522976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03981892950832844 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06895000115036964 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07424002978950739 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599190291017294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876998718827963 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016020028851926327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13095897156745195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0430599320679903 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1328160762786865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07389998063445091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16829895321279764 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042619998566806316 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09096006397157907 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1906389370560646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017903357744217 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19142997916787863 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08859904482960701 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18821004778146744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08522998541593552 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3185289679095149 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.147405942901969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03950996324419975 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5448150224983692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23782893549650908 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35756907891482115 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.905358030460775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22180902305990458 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37476897705346346 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10317005217075348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18058903515338898 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04039006307721138 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06812007632106543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08440902456641197 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17069897148758173 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049539958126842976 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015669967979192734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08313893340528011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04439009353518486 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3624450657516718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08459005039185286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17143902368843555 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03742997068911791 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09282003156840801 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5567979533225298 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09097997099161148 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1937089255079627 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034993126988411 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26170897763222456 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06452901288866997 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37155800964683294 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.574504072777927 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03503006882965565 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9653529161587358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16433896962553263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2774490276351571 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07416890002787113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1312600215896964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042999353259802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06747001316398382 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08546002209186554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17190910875797272 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049780006520450115 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01573993358761072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08324999362230301 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039189006201922894 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1832850286737084 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08431007154285908 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1712499652057886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037458958104252815 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09198999032378197 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5573580274358392 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08975900709629059 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19347993656992912 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10278995614498854 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26328908279538155 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0642300583422184 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37285895086824894 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5854339580982924 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03460003063082695 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9744629971683025 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11949904728680849 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23050897289067507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07424992509186268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13110903091728687 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04009006079286337 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736989598721266 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08500996045768261 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17143995501101017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950899165123701 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0157100148499012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08279993198812008 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039229984395205975 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1330960551276803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08500006515532732 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17171900253742933 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03730994649231434 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09157892782241106 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5556880496442318 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029998909682035 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1939090434461832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957902900874615 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2635490382090211 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06431993097066879 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3734190249815583 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5718439826741815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03438990097492933 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9609120208770037 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.81222395785153 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08909997995942831 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02631999086588621 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11646898929029703 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.512547984719276 ms for forwarding
--------------------
No. 3
<class 'diffusers.models.embeddings.Timesteps'> take 0.244928989559412 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05519005935639143 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04585005808621645 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.036259996704757214 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03767001908272505 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2786089899018407 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1340101007372141 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09699002839624882 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2073589712381363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0515299616381526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.11033995542675257 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04115002229809761 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08132902439683676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08863990660756826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17742894124239683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016489997506141663 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08861999958753586 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0654159123077989 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07562001701444387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16558996867388487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03906001802533865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09562005288898945 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6202380172908306 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09206996764987707 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1966890413314104 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09054993279278278 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.274218968115747 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06741902325302362 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39091799408197403 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6774339601397514 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03629003185778856 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0924529526382685 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09052897803485394 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18084899056702852 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05382997915148735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08700904436409473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04424003418534994 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07040996570140123 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0905599445104599 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17756898887455463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04921003710478544 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015969970263540745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08710892871022224 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9846359025686979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09009893983602524 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17895898781716824 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03878993447870016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09352003689855337 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5787779809907079 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09276997298002243 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19461894407868385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27095910627394915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750901229679585 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3868879284709692 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6199139645323157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03581005148589611 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.024283050559461 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08219899609684944 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1102290116250515 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.392366951331496 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07877999451011419 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17272902186959982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05071994382888079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08511007763445377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04030996933579445 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07267901673913002 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483991794288158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16190006863325834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873995203524828 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13844994828104973 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0335590448230505 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0616660583764315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435993757098913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227003652602434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044119078665971756 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0924400519579649 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19909895490854979 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09031000081449747 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19520905334502459 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896289711818099 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1918599009513855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08797901682555676 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32734894193708897 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.17798603605479 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042800093069672585 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.592474989593029 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0762200215831399 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16490905545651913 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04857999738305807 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13393897097557783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03967003431171179 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07144000846892595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07443001959472895 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16047002281993628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879001062363386 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583993434906006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13548997230827808 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0412558913230896 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353897672146559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16074907034635544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04349008668214083 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09236892219632864 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19386992789804935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08963001891970634 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19508902914822102 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08960999548435211 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1937089255079627 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08756003808230162 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.33040798734873533 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.176144927740097 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0403400044888258 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5722240786999464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13011903502047062 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15812891069799662 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.53465005941689 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07753993850201368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17159897834062576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05006999708712101 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1327299978584051 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03916898276656866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07007003296166658 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07608893793076277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644589938223362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04916999023407698 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015500001609325409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23479899391531944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038720085285604 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2097159633412957 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07525004912167788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16403896734118462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054939999245107174 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09337894152849913 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2093990333378315 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09305996354669333 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21491898223757744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09139999747276306 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13905903324484825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13271893840283155 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32349792309105396 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2158850440755486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05309993866831064 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6388238873332739 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07625902071595192 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16471906565129757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04836998414248228 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23350899573415518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04060997162014246 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07065897807478905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581000681966543 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1626700395718217 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876905586570501 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23441901430487633 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.239895005710423 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07409998215734959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1614500069990754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05564896855503321 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09240000508725643 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20956899970769882 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09101990144699812 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2226489596068859 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902889296412468 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13860000763088465 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13278902042657137 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3196389880031347 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2111159740015864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051900045946240425 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6281439457088709 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23042899556457996 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2580690197646618 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.082267966121435 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07701001595705748 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16954995226114988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049578957259655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23261900059878826 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040449900552630424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06981007754802704 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07451896090060472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16176898498088121 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015750061720609665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23378909099847078 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2397959362715483 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738899689167738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16135908663272858 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04919001366943121 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23313902784138918 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07021904457360506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749099999666214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16236002556979656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048050074838101864 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23452902678400278 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2327550211921334 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.519531059078872 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398903835564852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1629990292713046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913005977869034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23376895114779472 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041779945604503155 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07040996570140123 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07508997805416584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16236002556979656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049619004130363464 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01574005000293255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.234249047935009 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2427750043570995 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07568008732050657 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16593909822404385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05440996028482914 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09240000508725643 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19813899416476488 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09036902338266373 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21086004562675953 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09013002272695303 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12964894995093346 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13237993698567152 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31008897349238396 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.177535974420607 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05203892942517996 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5993139240890741 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07609999738633633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16577995847910643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23276894353330135 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039650010876357555 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06931996904313564 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433898281306028 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16093894373625517 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047959969379007816 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23460900411009789 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2374459765851498 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.187975078821182 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07676996756345034 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17386989202350378 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04865904338657856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4343789769336581 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.044199987314641476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06928900256752968 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07409998215734959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16112998127937317 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876998718827963 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015510013327002525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23529899772256613 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07141998503357172 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5430250205099583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07544900290668011 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16400893218815327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854006692767143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4340780433267355 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039829988963902 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06973999552428722 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07507007103413343 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16135897021740675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23427908308804035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0709090381860733 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5231439610943198 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07514003664255142 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16339996363967657 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4349490627646446 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963999915868044 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06912997923791409 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08018000517040491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16752909868955612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048329937271773815 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015169964171946049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23100897669792175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07054000161588192 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5268150018528104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22753002122044563 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32266799826174974 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.077062058262527 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07585994899272919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16510894056409597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049780006520450115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42802898678928614 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04027003888040781 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07068004924803972 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07578893564641476 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16269891057163477 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842004273086786 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015729921869933605 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23174902889877558 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07147993892431259 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5226650284603238 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07484003435820341 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16460998449474573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05395000334829092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09393994696438313 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20579900592565536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09212002623826265 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21191895939409733 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09237905032932758 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13704004231840372 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13011903502047062 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3151290584355593 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2125959619879723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05219003651291132 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.636673929169774 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498008199036121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16375002451241016 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04935904871672392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4269391065463424 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040529994294047356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0694390619173646 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475004531443119 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16123999375849962 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04828907549381256 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579978935420513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2313390141353011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0704699195921421 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5156250447034836 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738389790058136 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622689887881279 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05340995267033577 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09310001041740179 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2031500916928053 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076902642846107 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2104389714077115 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09092001710087061 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13775902334600687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1308900536969304 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3160890191793442 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1873359326273203 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0511690741404891 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6040540067479014 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0761599512770772 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16581895761191845 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04971004091203213 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32851891592144966 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04764995537698269 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06992905400693417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07454992737621069 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16052997671067715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048469053581357 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015569967217743397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23014901671558619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05965994205325842 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4136850368231535 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07373897824436426 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1625589793547988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05391996819525957 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09250908624380827 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20516000222414732 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09175902232527733 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2115190727636218 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09144993964582682 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13752910308539867 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1308800419792533 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31614897307008505 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1938349343836308 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05113007500767708 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.610063947737217 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23117894306778908 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3187290858477354 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.842474944889545 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1322000753134489 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22339005954563618 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05380995571613312 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3291690954938531 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040100072510540485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07053895387798548 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07505994290113449 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16120006330311298 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822900518774986 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13996998313814402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.061499071307480335 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3829550007358193 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07424002978950739 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621799310669303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04340906161814928 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09728001896291971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1920289359986782 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08950999472290277 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19335898105055094 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089230015873909 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1899499911814928 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0872790114954114 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3253489267081022 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1694360291585326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04081008955836296 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5663850354030728 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09373994544148445 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18453900702297688 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049939961172640324 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23049896117299795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03970996476709843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06933906115591526 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412000559270382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16051006969064474 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04851899575442076 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13269996270537376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04872900899499655 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2179049663245678 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613600179553032 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042819068767130375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09238999336957932 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19432895351201296 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08960999548435211 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22436899598687887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09025901090353727 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19255990628153086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08619006257504225 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32776896841824055 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2023060116916895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04117004573345184 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5983949415385723 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08066010195761919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17070001922547817 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913902375847101 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18114002887159586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039950013160705566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06911007221788168 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415993604809046 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16059901099652052 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015660072676837444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13197003863751888 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04313001409173012 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1468160664662719 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16237900126725435 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043620006181299686 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126996155828238 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19161903765052557 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09166903328150511 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19390007946640253 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08806900586932898 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1904189120978117 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08657993748784065 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32525893766433 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1671759421005845 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04058901686221361 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.56308407895267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2381990198045969 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35769899841398 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.052017005160451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22193999029695988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3772180061787367 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10371999815106392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18121907487511635 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04050997085869312 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06833893712610006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0885099871084094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17697003204375505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901899956166744 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015420024283230305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08314999286085367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044899992644786835 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3800450833514333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08582998998463154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17433997709304094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03791996277868748 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09409990161657333 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5549279740080237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082898031920195 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1933800522238016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09025901090353727 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2645790809765458 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06494997069239616 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3770689945667982 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.582594937644899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03482005558907986 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9877529703080654 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16432907432317734 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2774290041998029 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07393991108983755 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13212906196713448 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03970996476709843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07650000043213367 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0851399963721633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17324998043477535 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04935893230140209 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016050064004957676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08367002010345459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03967003431171179 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2012359220534563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08518004324287176 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17346895765513182 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03779993858188391 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0918989535421133 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5544179584830999 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029998909682035 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19255897495895624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09027996566146612 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.262749963440001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06528000812977552 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37669797893613577 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5776039799675345 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03515998832881451 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9723729928955436 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12233993038535118 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23520994000136852 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07356901187449694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1318400027230382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040070037357509136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06777001544833183 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08458003867417574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17206906341016293 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04904996603727341 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015849946066737175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08355989120900631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039499951526522636 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1462359689176083 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08498993702232838 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.173219945281744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03933906555175781 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09399000555276871 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5571780493482947 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09148905519396067 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19292999058961868 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08985004387795925 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26220909785479307 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06453006062656641 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3744090208783746 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5800339169800282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034250086173415184 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9892329582944512 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.906683932058513 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08493999484926462 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02631999086588621 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15461002476513386 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.99338405672461 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20567001774907112 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04646007437258959 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.040319981053471565 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.0260800588876009 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03674998879432678 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24448905605822802 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07160904351621866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06959994789212942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15724904369562864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04942005034536123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08271995466202497 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06962905172258615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0942600890994072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1813990529626608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049419933930039406 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015500001609325409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.082949991337955 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9546069195494056 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06704998668283224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1545000122860074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03708002623170614 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0934090930968523 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5561079597100616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1933190505951643 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08976995013654232 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26364997029304504 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06410002242773771 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3737780498340726 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5739339869469404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034069991670548916 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9614029442891479 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08522009011358023 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17352902796119452 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04971993621438742 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0817290274426341 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04000996705144644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06828003097325563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08440902456641197 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17106905579566956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049440073780715466 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015840050764381886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08321902714669704 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9473760146647692 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08415000047534704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17091899644583464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037590041756629944 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09320001117885113 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5693279672414064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104900527745485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19285897724330425 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912000339478254 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26199896819889545 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06344995927065611 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.36866904702037573 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5796340303495526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03381003625690937 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.978463027626276 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07770990487188101 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10293000377714634 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.048618000932038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07989001460373402 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17937005031853914 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05739904008805752 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08386000990867615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03900995943695307 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067679095081985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414002902805805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15636999160051346 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792900290340185 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014829915016889572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13049005065113306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031400006264448166 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0442970087751746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0727000879123807 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15721004456281662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041749910451471806 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08994992822408676 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18904905300587416 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08831999730318785 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19132893066853285 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08706003427505493 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1890190178528428 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08492008782923222 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3171580610796809 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1375349713489413 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045989989303052425 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5350250760093331 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08459901437163353 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17442891839891672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1297390554100275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03896001726388931 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06878003478050232 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07240893319249153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15536905266344547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13072893489152193 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0188659653067589 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275992538779974 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.156259979121387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042050029151141644 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0898200087249279 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20321900956332684 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08785002864897251 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19210902974009514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08677993901073933 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18935906700789928 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0852800440043211 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31920894980430603 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3454050058498979 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039619975723326206 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7267139628529549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12636894825845957 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15160907059907913 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.573860020376742 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07544993422925472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.165830017067492 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048768939450383186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12938992585986853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03870995715260506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850901991128922 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15898991841822863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047859037294983864 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22932898718863726 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03930996172130108 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1700160102918744 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751999905332923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16229890752583742 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05368003621697426 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09191001299768686 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20269898232072592 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08937902748584747 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20932999905198812 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890200026333332 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13596902135759592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12871995568275452 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.307859038002789 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.16399594116956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05004904232919216 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5827741008251905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422000635415316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16387994401156902 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04752993118017912 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22702908609062433 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06800005212426186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734089408069849 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15709898434579372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04664994776248932 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22869894746690989 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.20077608153224 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07352896500378847 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15746906865388155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05313998553901911 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08989905472844839 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20407000556588173 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905993308871984 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21439895499497652 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918007370084524 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13473909348249435 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12874999083578587 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30598894227296114 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1680549941956997 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05061994306743145 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5715040499344468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22442894987761974 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24986895732581615 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.871849018149078 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0755189685150981 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1640989212319255 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047650071792304516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22679893299937248 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038400059565901756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06781995762139559 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320998702198267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15701004303991795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755996633321047 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22885994985699654 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2003850424662232 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275003008544445 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15668990090489388 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04713900852948427 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2261199988424778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037878984585404396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678100623190403 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07347005885094404 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15660899225622416 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04692003130912781 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01471000723540783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22814900148659945 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1896960204467177 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4312910391017795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370894309133291 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922891907393932 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2267090603709221 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0381499994546175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348007056862116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15702005475759506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794006235897541 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01493899617344141 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22818008437752724 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1972850188612938 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07389998063445091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15972997061908245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05266896914690733 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09053002577275038 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19262894056737423 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0879199942573905 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20932895131409168 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08939998224377632 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12477999553084373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12821902055293322 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.2969589550048113 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1429759906604886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049720052629709244 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5484750038012862 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408892270177603 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15951902605593204 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737998824566603 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22603897377848625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03925000783056021 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728002335876226 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343990728259087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1572099281474948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04673004150390625 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759949408471584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22769905626773834 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1924459831789136 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.030365962535143 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506995461881161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16051006969064474 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0473889522254467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4236790118739009 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04008004907518625 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06854895036667585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416994776576757 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15875999815762043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0478189904242754 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016939942725002766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22786902263760567 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06855989340692759 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4899149537086487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418892346322536 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15851901844143867 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04675996024161577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42287795804440975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03871996887028217 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06769003812223673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386005017906427 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15813903883099556 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480700982734561 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014299992471933365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22830907255411148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06853905506432056 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.475783996284008 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357995491474867 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15854998491704464 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047079985961318016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42311905417591333 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0388899352401495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0682600075379014 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275992538779974 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15644892118871212 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05063007120043039 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014510005712509155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22799894213676453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06938003934919834 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4937249943614006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22567994892597198 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3051490057259798 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.9155029701069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16050005797296762 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04814902786165476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4233290674164891 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03908993676304817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0685789855197072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406005170196295 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15731994062662125 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755903501063585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014360062777996063 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2283990615978837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06940006278455257 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4804149977862835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07336004637181759 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588489394634962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052810064516961575 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205995593219995 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20345894154161215 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0900590093806386 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20820996724069118 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0878400169312954 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1364400377497077 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12890901416540146 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3086390206590295 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1647259816527367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050200033001601696 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5691739972680807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07379997987300158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601299736648798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047008972615003586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231890197843313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04055001772940159 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0683589605614543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422000635415316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15845999587327242 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04711898509413004 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2281090710312128 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06936001591384411 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4841149095445871 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0728790182620287 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15719898510724306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052350107580423355 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08996902033686638 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20087999291718006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029905777424574 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21058903075754642 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0879999715834856 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13561907690018415 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12939004227519035 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3088790690526366 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1633860412985086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05049002356827259 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.564783975481987 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743200071156025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16158900689333677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047860085032880306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3250889712944627 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039100064896047115 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06879901047796011 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07299007847905159 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601299736648798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014969962649047375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23376895114779472 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0619000056758523 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3912650756537914 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07356889545917511 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16012892592698336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05310005508363247 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915989512577653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20358990877866745 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0894690165296197 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20806898828595877 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08754001464694738 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13572897296398878 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1281999284401536 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3066790523007512 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1612260714173317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050888978876173496 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5721339732408524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22931897547096014 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3145490773022175 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.584165993146598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1314990222454071 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22024905774742365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05340995267033577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3255580086261034 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03921997267752886 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06882997695356607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07410894613713026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15796907246112823 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04740990698337555 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014949939213693142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.131119042634964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059030018746852875 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.343785086646676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07206003647297621 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15596894081681967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04176003858447075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09116006549447775 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19123894162476063 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08836900815367699 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19122008234262466 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08686899673193693 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1886390382423997 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08557003457099199 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3193289740011096 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1507660383358598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039380043745040894 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5290339943021536 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09216996841132641 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17903000116348267 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049008987843990326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22759893909096718 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038409954868257046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06791995838284492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362896576523781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15691900625824928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479999627918005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13088888954371214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04857999738305807 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1872960021719337 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731790205463767 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569389132782817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042140018194913864 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969998452812433 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19470998086035252 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0885890331119299 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19094895105808973 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08663011249154806 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18939899746328592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08558004628866911 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3188890404999256 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.144125941209495 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040068989619612694 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.52533408254385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07976999040693045 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16560906078666449 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.061400001868605614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17997901886701584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03972998820245266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757990922778845 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07317890413105488 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15673902817070484 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04682003054767847 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1302689779549837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04175002686679363 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.130155986174941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07231906056404114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15633890870958567 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04215002991259098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09009998757392168 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18977001309394836 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08872896432876587 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19110995344817638 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08751999121159315 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18969899974763393 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08571997750550508 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3191090654581785 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.139785978011787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039218924939632416 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5199739718809724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23766898084431887 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3559989854693413 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.799048024229705 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22088002879172564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37418806459754705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10406994260847569 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17974909860640764 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039019971154630184 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06766896694898605 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08610996883362532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1709400676190853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05683908239006996 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08299003820866346 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04292000085115433 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3663850259035826 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08512998465448618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17062993720173836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03650004509836435 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032897651195526 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5550180794671178 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912000339478254 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19190902821719646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08829997386783361 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2628900110721588 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06381992716342211 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37096801679581404 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5585740329697728 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03372994251549244 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9436130532994866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16307993791997433 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2740600612014532 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07344002369791269 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13063999358564615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038578989915549755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06707001011818647 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08342997170984745 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16834901180118322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08257990702986717 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038439990021288395 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1614449322223663 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08389994036406279 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16836996655911207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03654998727142811 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09043002501130104 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5537780234590173 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884002454578876 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19059993792325258 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.088299042545259 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26205903850495815 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06349000614136457 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3694089828059077 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5557250007987022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03385893069207668 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9363530445843935 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12197892647236586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23230898659676313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07338996510952711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1306289341300726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039260019548237324 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0668700085952878 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08421903476119041 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16867904923856258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797999281436205 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0823090085759759 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038710073567926884 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1195059632882476 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08390902075916529 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16881898045539856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03700004890561104 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045004844665527 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5552070215344429 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08855003397911787 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19057898316532373 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08813000749796629 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2627590438351035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06416894029825926 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37134799640625715 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5641739591956139 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03403995651751757 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9468929385766387 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.695525048300624 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08557899855077267 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02505001612007618 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10738009586930275 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.11907001771033 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19479903858155012 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04349998198449612 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03985001239925623 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025209970772266388 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03661005757749081 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23718900047242641 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06922997999936342 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06588001269847155 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15198905020952225 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08269899990409613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03955001011490822 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06838003173470497 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08445908315479755 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16969896387308836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048410031013190746 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08265906944870949 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9261060040444136 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06303994450718164 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1475500175729394 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03681005910038948 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218999184668064 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5562180886045098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09638001210987568 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19530998542904854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08908892050385475 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.264069065451622 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06381992716342211 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3717390354722738 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5742549439892173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03404892049729824 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9517329055815935 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08457002695649862 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17119001131504774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879001062363386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08135999087244272 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038680038414895535 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750901229679585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0849299831315875 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16969989519566298 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048218993470072746 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08228002116084099 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9288269793614745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08352904114872217 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16863900236785412 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037289923056960106 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09109906386584044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5524579901248217 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912000339478254 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19086897373199463 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08840998634696007 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2625989727675915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06386998575180769 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.36935904063284397 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5550840180367231 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03363005816936493 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9371429225429893 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07753993850201368 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10275898966938257 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.944828037172556 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07740000728517771 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18182897474616766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04859000910073519 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08098001126199961 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869005013257265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06798002868890762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480999920517206 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567599829286337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014969962649047375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1306899357587099 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03148906398564577 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0240760166198015 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0728600425645709 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571399625390768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04215002991259098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09063002653419971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18956896383315325 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08811999578028917 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19190902821719646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798995986580849 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18840900156646967 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08545001037418842 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31763804145157337 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.144715934060514 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03915000706911087 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5249749412760139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436901796609163 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15899899881333113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722003359347582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12991996482014656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039319973438978195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06780994590371847 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15734904445707798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04738999996334314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014770077541470528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13111892621964216 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0021260241046548 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275992538779974 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15619001351296902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04237901885062456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978007826954126 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19815901760011911 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08850009180605412 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19111891742795706 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09049009531736374 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1899399794638157 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08500891271978617 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3177289618179202 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.149966032244265 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03958004526793957 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5306249260902405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12654904276132584 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15180895570665598 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.329511011950672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07642898708581924 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16667903400957584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048569985665380955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12909993529319763 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03820005804300308 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06772996857762337 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07478904444724321 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1592189073562622 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047289999201893806 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014939927496016026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22810895461589098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037070014514029026 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1599159333854914 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587689621374011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05349994171410799 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09119906462728977 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20195904653519392 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08903001435101032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21039892453700304 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08834002073854208 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13533909805119038 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12969993986189365 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3082980401813984 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1624450562521815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06344006396830082 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.581834047101438 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503898814320564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16085896641016006 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046989996917545795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22649893071502447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03885000478476286 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757897790521383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07321999873965979 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15623006038367748 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04684901796281338 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22805901244282722 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.195124932564795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07270998321473598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1572000328451395 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05330902058631182 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072001557797194 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20102900452911854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905993308871984 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20774896256625652 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08814898319542408 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13582000974565744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1289290376007557 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.307328999042511 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.158636063337326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05034997593611479 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5614849980920553 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22397993598133326 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24927000049501657 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.845559062436223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07551000453531742 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16357004642486572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04733994137495756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2269900869578123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03846909385174513 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06770004983991385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07314002141356468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16444898210465908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04776008427143097 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2277489984408021 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2060559820383787 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15580898616462946 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712003283202648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22661907132714987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869994543492794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06729993037879467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07371895480901003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569389132782817 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047539942897856236 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014310004189610481 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22729893680661917 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1888560838997364 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4363920092582703 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338996510952711 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15904894098639488 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04738999996334314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22743898443877697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869005013257265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866897456347942 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310009095817804 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15670002903789282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0473889522254467 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01471000723540783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22799009457230568 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1966750025749207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16065000090748072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05310901906341314 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030010551214218 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1934689935296774 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08779997006058693 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20784896332770586 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08859904482960701 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1321599120274186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12899900320917368 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3052189713343978 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1491560144349933 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049780006520450115 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5546850627288222 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456995081156492 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15960901509970427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04782003816217184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22660905960947275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03899005241692066 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06809900514781475 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15809002798050642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047829002141952515 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22822897881269455 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1963649885728955 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.037915030494332 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548998109996319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16102904919534922 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734995309263468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42321800719946623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385999446734786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759003736078739 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07379904855042696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15776907093822956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731991793960333 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01458998303860426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22783898748457432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06843998562544584 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.480144914239645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376994471997023 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582900295034051 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046779983676970005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4228790057823062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038169906474649906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07000891491770744 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07317995186895132 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15743996482342482 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04689896013587713 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01434003934264183 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22835901472717524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06838003173470497 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4925150899216533 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07356901187449694 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588489394634962 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680990241467953 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4229079931974411 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03876001574099064 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750004831701517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0725090503692627 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15647895634174347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724005702883005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014320015907287598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22823899053037167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06880005821585655 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4735549921169877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2250900724902749 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30270894058048725 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.898151964880526 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07399998139590025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15972997061908245 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705891478806734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42311905417591333 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038830097764730453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06882904563099146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344002369791269 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15715998597443104 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04802900366485119 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014669960364699364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22852898109704256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07031997665762901 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.484484993852675 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345899939537048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15892903320491314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05272997077554464 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09171909186989069 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20169001072645187 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09008904453366995 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2088389592245221 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798995986580849 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.136188929900527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12916000559926033 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3084989730268717 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1635259725153446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05012995097786188 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5685040270909667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442000787705183 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602190313860774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42319903150200844 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03902008756995201 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678299693390727 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408903911709785 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15830900520086288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779000300914049 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01504004467278719 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22810895461589098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06925000343471766 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4802149962633848 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07226003799587488 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15739991795271635 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05318003240972757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09075994603335857 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2020190004259348 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09751995094120502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20986900199204683 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09005004540085793 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1357899745926261 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12797897215932608 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30671898275613785 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1718160239979625 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05030003376305103 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5735749620944262 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07505901157855988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16228901222348213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047569978050887585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3250190056860447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039089005440473557 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680300872772932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07314002141356468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15737907961010933 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821002949029207 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015330035239458084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2284790389239788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058489968068897724 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.375605002976954 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07249903865158558 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15738909132778645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053110066801309586 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09060907177627087 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20150002092123032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08901895489543676 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20742893684655428 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08823990356177092 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13630895409733057 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12949993833899498 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3093089908361435 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.160496030934155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05090900231152773 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.563454046845436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22995902691036463 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3169889096170664 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.569895919412374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13123895041644573 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2211789833381772 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053660012781620026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32617803663015366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03966991789638996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850005593150854 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366901263594627 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1568689476698637 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046590110287070274 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014849938452243805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13116898480802774 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05904992576688528 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3461950002238154 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07249903865158558 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15646906103938818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042230007238686085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106891229748726 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1908999402076006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08897006046026945 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19020901527255774 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08750997949391603 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1887290272861719 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08528993930667639 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31760893762111664 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.140685984864831 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03971997648477554 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5225039096549153 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09262899402529001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18005899619311094 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04943995736539364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22732908837497234 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03916001878678799 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673000467941165 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07330009248107672 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1638500252738595 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04759000148624182 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015138997696340084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13083999510854483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047930050641298294 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1940059484913945 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073009985499084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15645998064428568 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04337995778769255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08904899004846811 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19042904023081064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08853001054376364 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1920890063047409 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08815003093332052 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18949899822473526 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0850799260661006 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3185679670423269 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1408349964767694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03926991485059261 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.521213911473751 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07883901707828045 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16519904602319002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0479499576613307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1790189417079091 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038770027458667755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06808002945035696 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07311010267585516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15693902969360352 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015050056390464306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1306090271100402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042380066588521004 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1147259501740336 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07255002856254578 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569989835843444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04166003782302141 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017903357744217 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18961995374411345 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08853990584611893 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2040690742433071 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910009637475014 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18794892821460962 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08479994721710682 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31627900898456573 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1526859598234296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039319973438978195 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5338739613071084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2380390651524067 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3563690697774291 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.798127993941307 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22103905212134123 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3736290382221341 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10300008580088615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17986900638788939 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03955001011490822 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06785907316952944 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08461996912956238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16792002134025097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810898099094629 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08290004916489124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04337006248533726 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3486261013895273 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08566002361476421 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17135892994701862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037070014514029026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09114900603890419 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.556217972189188 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08920999243855476 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18867896869778633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08771999273449183 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2626590430736542 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06521900650113821 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37213799078017473 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5567339723929763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03404996823519468 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9408529624342918 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16287004109472036 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.274588936008513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0732900807633996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13102998491376638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03910902887582779 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06762007251381874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08427002467215061 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16920908819884062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759949408471584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08234009146690369 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038489000871777534 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1632549576461315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08473999332636595 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17032003961503506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03658991772681475 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09022001177072525 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5540680140256882 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08858891669660807 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18974998965859413 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08754001464694738 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2611689269542694 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06425997707992792 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3684390103444457 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5485250623896718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033549964427948 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9298929255455732 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11803896632045507 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22842898033559322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07384002674371004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1303990138694644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03979005850851536 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673199538141489 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08475000504404306 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16957905609160662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014939927496016026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08264905773103237 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03870995715260506 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1169760255143046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0838589621707797 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1681790454313159 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03638991620391607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09073992259800434 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5548889748752117 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08776003960520029 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18933904357254505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08729007095098495 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26206905022263527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0637200428172946 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.36883901339024305 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5486240154132247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03421003930270672 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9284329609945416 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.650194901041687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08718995377421379 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02498994581401348 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1091900048777461 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 57.65908199828118 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.18828990869224072 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.041498919017612934 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03833998925983906 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02535991370677948 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03578001633286476 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23190898355096579 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06940995808690786 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0654000323265791 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15217997133731842 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04814902786165476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08259003516286612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03941997420042753 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06814999505877495 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08470006287097931 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17051002942025661 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04845892544835806 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01516006886959076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08264998905360699 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9285369887948036 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0623590312898159 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14805910177528858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03711995668709278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09157904423773289 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5559780402109027 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978007826954126 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1917890040203929 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08867995347827673 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2629089867696166 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06357894744724035 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3702880349010229 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5599139733240008 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034069991670548916 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.938923029229045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08366000838577747 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17058895900845528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048930058255791664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08139899000525475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03882008604705334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06722996477037668 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08470902685075998 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1734589459374547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0484599731862545 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159952454268932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08228898514062166 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9337560040876269 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0831700162962079 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16859895549714565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03669003490358591 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09181990753859282 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5546179600059986 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08895900100469589 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1922199735417962 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08785002864897251 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2623189939185977 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06355997174978256 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3684090916067362 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.555335009470582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03371003549546003 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9365830812603235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07755996193736792 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10255002416670322 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.938038928434253 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07607904262840748 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16527902334928513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047539942897856236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08042901754379272 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903999458998442 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0680300872772932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735290814191103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15702901873737574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04776997957378626 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01516006886959076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311290543526411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0318099046126008 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.008035964332521 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298006676137447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15813903883099556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042019993998110294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19148888532072306 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884002454578876 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19102892838418484 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10294897947460413 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18977001309394836 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0868999632075429 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32037904020398855 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1621260782703757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03925000783056021 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5432749642059207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377995643764734 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15962007455527782 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13016897719353437 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03848003689199686 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789993494749069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07318996358662844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15638000331819057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04717998672276735 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015138997696340084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1308299833908677 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0026070522144437 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07241894491016865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15636906027793884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04220998380333185 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09092898108065128 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1906800316646695 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08904002606868744 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1916489563882351 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08751999121159315 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18979900050908327 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08539005648344755 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31876901630312204 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.144056092016399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03921007737517357 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5241840155795217 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12593006249517202 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1507799606770277 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.325249978341162 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07647997699677944 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16608007717877626 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04851899575442076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12919993605464697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06781902629882097 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08204998448491096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16658997628837824 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04717905540019274 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014840043149888515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2283990615978837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03709993325173855 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1685260105878115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07341010496020317 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15804904978722334 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05313998553901911 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09087997023016214 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2015189966186881 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012897498905659 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2102790167555213 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08813000749796629 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13531895820051432 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12927991338074207 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3075690474361181 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.161086023785174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049919006414711475 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5639140037819743 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07501000072807074 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16055000014603138 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04757894203066826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22719905246049166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038260011933743954 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06836000829935074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436901796609163 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582390395924449 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04694995004683733 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014979974366724491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22797903511673212 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1965360026806593 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07265899330377579 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15629897825419903 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05326000973582268 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104900527745485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20089000463485718 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10330008808523417 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21104898769408464 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0883699394762516 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13571896124631166 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1295000547543168 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3084189957007766 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.179095939733088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05006999708712101 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5805839793756604 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2238689921796322 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2492489293217659 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.8549289824441075 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07661001291126013 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1653290819376707 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04733994137495756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22663897834718227 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038199941627681255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06737990770488977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735489884391427 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15779898967593908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046930043026804924 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22762897424399853 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.196736004203558 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0719990348443389 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1553189940750599 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04707009065896273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2268690150231123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03921007737517357 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0676099443808198 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07285003084689379 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1561800017952919 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047560082748532295 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014508957974612713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22793002426624298 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1886759893968701 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.426480990834534 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284991443157196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.157840084284544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04743004683405161 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2272999845445156 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03984896466135979 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06786000449210405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298996206372976 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1565789571031928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755996633321047 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2278289757668972 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1951960623264313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446890231221914 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16118900384753942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05352997686713934 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09003898594528437 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19113998860120773 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089038978330791 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20826898980885744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08774001616984606 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12511899694800377 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12889993377029896 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.2978290431201458 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1402660747990012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04950002767145634 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.546334009617567 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746299047023058 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16265909653156996 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22680906113237143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842997830361128 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06776000373065472 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396901492029428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15799899119883776 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04651991184800863 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01516006886959076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2274590078741312 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1971560306847095 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.028745926916599 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15974999405443668 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04685006570070982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42473897337913513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03999995533376932 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06797909736633301 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436005398631096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15938992146402597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483290059491992 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22827903740108013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06867002230137587 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4997849939391017 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406005170196295 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16001902986317873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04708999767899513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42274908628314734 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03905000630766153 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06804009899497032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345003541558981 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15674892347306013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712003283202648 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014730030670762062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22828904911875725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06823998410254717 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4810248976573348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365993224084377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15837897080928087 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687998443841934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4226079909130931 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03897992428392172 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678100623190403 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07352989632636309 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1570089953020215 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04708999767899513 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014510005712509155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2287690294906497 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06964895874261856 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4770739944651723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2252089325338602 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3040190786123276 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.912021919153631 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489905692636967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16064895316958427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42309798300266266 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03883999306708574 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684000551700592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369904778897762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15734892804175615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22819906007498503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06938900332897902 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4827840495854616 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369008380919695 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05290901754051447 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09241001680493355 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20101899281144142 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09138998575508595 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.208089011721313 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08819892536848783 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13611989561468363 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12976897414773703 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30947907362133265 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.164156012237072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04991993773728609 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5676449984312057 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07404002826660872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15987001825124025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05570997018367052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231689963489771 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03923999611288309 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06818899419158697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1568300649523735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0464289914816618 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014890101738274097 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22779894061386585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.069280038587749 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4870649902150035 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07340905722230673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15906896442174911 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05264009814709425 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09121897164732218 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2021300606429577 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998892735689878 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2073689829558134 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08804001845419407 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.135818962007761 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12843997683376074 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30751898884773254 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1604559840634465 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05003996193408966 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5649840934202075 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418997120112181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16076897736638784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04862004425376654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3253690665587783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03908993676304817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06813905201852322 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387006189674139 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15764008276164532 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785892087966204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014870078302919865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2288589021191001 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060540041886270046 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3796350685879588 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327902130782604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594889909029007 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05276000592857599 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082001633942127 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20106008742004633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09002909064292908 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20811997819691896 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08878996595740318 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13576901983469725 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12937001883983612 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3085189964622259 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.162195927463472 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05026895087212324 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5674139140173793 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2290690317749977 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3140390617772937 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.571325033903122 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1309090293943882 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21888897754251957 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053239986300468445 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32528897281736135 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879901487380266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06816000677645206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15677895862609148 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048320041969418526 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014599994756281376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1308100763708353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05904003046452999 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3397249858826399 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07295003160834312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15735893975943327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042100087739527225 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09827991016209126 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19121903460472822 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08834991604089737 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19049004185944796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08796900510787964 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1889290288090706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0860800500959158 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3182990476489067 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.148605952039361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03958994057029486 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.53053505346179 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09211001452058554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1794100971892476 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04844891373068094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22767006885260344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915990237146616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06793998181819916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406901568174362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15766907017678022 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806008655577898 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1307789934799075 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048410031013190746 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.18717597797513 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343001198023558 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15703903045505285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04157004877924919 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910999167710543 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1899789785966277 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08754897862672806 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1903800293803215 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08796993643045425 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18774904310703278 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08545001037418842 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3171289572492242 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1369159910827875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03908993676304817 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5157940797507763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579999510198832 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1792899565771222 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0482689356431365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17846992705017328 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899015635252 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06779993418604136 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07326994091272354 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15732890460640192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824995994567871 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015119905583560467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13056001625955105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04226900637149811 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.129776006564498 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0727800652384758 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15818001702427864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04180893301963806 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913001511245966 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18961902242153883 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08728995453566313 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19004906062036753 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08779997006058693 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18824997823685408 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.085678999312222 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31766900792717934 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1345359962433577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03933999687433243 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5148349339142442 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2375999465584755 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35620899870991707 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.78204801119864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22124906536191702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3732689656317234 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10442896746098995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17974001821130514 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03911007661372423 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06776000373065472 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08490995969623327 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17050898168236017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048410031013190746 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522002276033163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08279003668576479 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04326901398599148 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.357734901830554 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08526002056896687 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1702900044620037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03716000355780125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09101990144699812 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5553379887714982 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935888763517141 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19080995116382837 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08845003321766853 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26096904184669256 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06412994116544724 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.36899896804243326 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5554650453850627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03363995347172022 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.938703004270792 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16157899517565966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27288903947919607 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0735200010240078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13111892621964216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03920996095985174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06753997877240181 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08559005800634623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17039000522345304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913902375847101 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08306989911943674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038560014218091965 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1653859401121736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08568994235247374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17142901197075844 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036800047382712364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09953894186764956 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5562480073422194 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896799610927701 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19086897373199463 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0877799466252327 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2617000136524439 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06410002242773771 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3693389007821679 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5667739789932966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03384996671229601 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9510630518198013 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11960999108850956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23077998775988817 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07377902511507273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1307500060647726 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039470032788813114 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06708910223096609 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0842600129544735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1685300376266241 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04805997014045715 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08230004459619522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03863009624183178 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.116956933401525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08418003562837839 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16892992425709963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03665999975055456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051989763975143 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5551379872485995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08834002073854208 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20749890245497227 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08826900739222765 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26248907670378685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06425997707992792 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3713188925758004 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5733740292489529 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03402004949748516 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9548030104488134 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.713195031508803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08927995804697275 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025070039555430412 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1062790397554636 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 57.696151081472635 ms for forwarding
--------------------
No. 4
<class 'diffusers.models.embeddings.Timesteps'> take 0.24333002511411905 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05458004307001829 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04587997682392597 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.035640085116028786 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.038529047742486 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2789590507745743 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.13221008703112602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09658001363277435 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.20661007147282362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05132902879267931 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.10571000166237354 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04075001925230026 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08085905574262142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08893001358956099 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1774500124156475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048888963647186756 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01673994120210409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08906004950404167 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.059046946465969 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07690000347793102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16743899323046207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03924011252820492 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09580899495631456 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6268779980018735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09229907300323248 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19697006791830063 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254901669919491 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27629907708615065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06768002640455961 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39340893272310495 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6867839731276035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036659068427979946 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.104182029142976 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09048997890204191 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18078903667628765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05050003528594971 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0871199881657958 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07148901931941509 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09139010217040777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17958006355911493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049720052629709244 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016089994460344315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08828006684780121 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9877469856292009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0900499289855361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1794290728867054 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03960996400564909 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09310001041740179 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5838879151269794 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106902871280909 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19342999439686537 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09156903252005577 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.27204910293221474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0676099443808198 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38774905260652304 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6219839453697205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03574998117983341 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.026831964030862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08288002572953701 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11099001858383417 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.405156920664012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07857999298721552 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1717300619930029 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05064008291810751 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08539005648344755 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04069006536155939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07188995368778706 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07556995842605829 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16320007853209972 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04850002005696297 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01569895539432764 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1398199237883091 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033829943276941776 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0611270554363728 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16339006833732128 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04349998198449612 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0930300448089838 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1972089521586895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09352003689855337 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19635900389403105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09043910540640354 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19382999744266272 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08875899948179722 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3301589749753475 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1997759575024247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040409970097243786 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5982940094545484 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07592001929879189 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16478903125971556 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13451906852424145 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918004222214222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07025001104921103 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08859997615218163 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18982007168233395 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048798974603414536 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01580989919602871 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13583002146333456 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.074325991794467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396005094051361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607489539310336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04366005305200815 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09192898869514465 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19432906992733479 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016005787998438 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1940589863806963 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08958997204899788 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19395898561924696 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08765899110585451 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.33006793819367886 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1762650683522224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04057993646711111 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5710740117356181 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13099901843816042 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15863904263824224 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.573629052378237 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0854589743539691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17875898629426956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050020054914057255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1336189452558756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03946991637349129 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07068994455039501 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07519894279539585 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16207899898290634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04915997851639986 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579978935420513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23638899438083172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03870006185024977 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.213336014188826 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482990622520447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16340904403477907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054780044592916965 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09340897668153048 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.211079022847116 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09129999671131372 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21450896747410297 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078998118638992 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13901898637413979 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13286899775266647 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3193579614162445 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2038949644193053 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05259993486106396 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6250440385192633 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07594900671392679 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16421894542872906 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04883995279669762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23575907107442617 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039770035073161125 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07106002885848284 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07491989526897669 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16096001490950584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04851992707699537 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23546000011265278 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2489050859585404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07521000225096941 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16397994477301836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05515897646546364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09190000128000975 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20609900821000338 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08979008998721838 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21307996939867735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08974899537861347 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13883993960916996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13260904233902693 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31797902192920446 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1930559994652867 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052489922381937504 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.613594009540975 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23234891705214977 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2597490092739463 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.070347968488932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07718009874224663 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16867893282324076 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04904996603727341 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2343989908695221 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040770042687654495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07078005000948906 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07409998215734959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16006897203624249 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015879981219768524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2355789765715599 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.241125981323421 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07297005504369736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15929900109767914 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048939953558146954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23433903697878122 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03936002030968666 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06908993236720562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.089899986051023 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17675897106528282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04909001290798187 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2365990076214075 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2466850457713008 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5339910062029958 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472897414118052 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16405899077653885 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049449969083070755 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2354989992454648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978004679083824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06999005563557148 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16192998737096786 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048600020818412304 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015759957022964954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23553997743874788 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2431249488145113 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581000681966543 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16609998419880867 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05577900446951389 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09187008254230022 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19891909323632717 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012001100927591 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21331897005438805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09012897498905659 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12838002294301987 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13284897431731224 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30860898550599813 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.178506063297391 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0521800247952342 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6042350325733423 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07650000043213367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16481999773532152 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04954996984452009 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23459899239242077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047449953854084015 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07034000009298325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748300226405263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16165897250175476 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824995994567871 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610000617802143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2357090124860406 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2475249823182821 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.20499499887228 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07664004806429148 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16667903400957584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43702800758183 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039460021071136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07035990711301565 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07594993803650141 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16388006042689085 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04912994336336851 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015609897673130035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2366789849475026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07241999264806509 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5391139313578606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503898814320564 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16369903460144997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048509915359318256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43703801929950714 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040109967812895775 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.069280038587749 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601589610800147 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880991764366627 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015230034478008747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23230910301208496 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0707689905539155 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5215439489111304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09380001574754715 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18242897931486368 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810991231352091 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.43158908374607563 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03968901000916958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06986991502344608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465004455298185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16104895621538162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23325905203819275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07077003829181194 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5406949678435922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302700886502862 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3247479908168316 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.0871409475803375 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07544993422925472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16328995116055012 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048990012146532536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4308590432628989 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04041893407702446 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07055001333355904 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07523002568632364 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16155897174030542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483199255540967 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015440047718584538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23307895753532648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07144000846892595 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5238249907270074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385900244116783 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16187899746000767 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054060015827417374 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09387906175106764 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21362898405641317 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09257998317480087 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21104898769408464 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914999982342124 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13891898561269045 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1309189246967435 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31987810507416725 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.208805013448 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05138001870363951 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6279839910566807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545994594693184 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16356899868696928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834006540477276 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4310279618948698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07035001181066036 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16169995069503784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04831003025174141 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01580000389367342 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23285998031497002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07186993025243282 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5235040336847305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07404002826660872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16237900126725435 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053999945521354675 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09307998698204756 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20424998365342617 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252899326384068 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21025899332016706 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08966994937509298 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13853900600224733 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14687003567814827 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3343189600855112 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2098050210624933 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05177990533411503 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6289539635181427 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515901234000921 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16543897800147533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049930065870285034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3310890169814229 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039618927985429764 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07045990787446499 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07405003998428583 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608789898455143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015620025806128979 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23289897944778204 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06046006456017494 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4142149593681097 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16193895135074854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054119969718158245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09257998317480087 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20371901337057352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09139999747276306 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21147890947759151 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08943001739680767 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13822002802044153 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13118900824338198 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31680904794484377 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1886650463566184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051539973355829716 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6070640413090587 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23339898325502872 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32090803142637014 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.868574095889926 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13862899504601955 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23033900652080774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05412998143583536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.33250905107706785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040409970097243786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07035001181066036 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15994894783943892 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047749956138432026 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558999065309763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13387994840741158 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.061169033870100975 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.383375027216971 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419008761644363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16211892943829298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04296004772186279 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09461003355681896 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19322906155139208 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048997890204191 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.193169922567904 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08941895794123411 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19170006271451712 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08781009819358587 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32850902061909437 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1755159357562661 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040590064600110054 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.572675071656704 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09385007433593273 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1835799776017666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979909863322973 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23177999537438154 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04056899342685938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06976001895964146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747699523344636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16169901937246323 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846008960157633 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1337589928880334 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050080008804798126 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.225135987624526 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506995461881161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631290651857853 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04342000465840101 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09242992382496595 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19289902411401272 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09077892173081636 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19468995742499828 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909008465707302 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19191892351955175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08744001388549805 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32694905530661345 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1701459297910333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04114897456020117 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5693239402025938 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08002994582056999 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16981898806989193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050080008804798126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1820690231397748 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04024000372737646 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06946991197764874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437902968376875 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600090181455016 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048209913074970245 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016069971024990082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13323896564543247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044229906052351 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1518059764057398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427891250699759 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16244908329099417 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043409992940723896 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134900756180286 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19131996668875217 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021000005304813 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1935489708557725 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08817005436867476 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19075896125286818 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08715002331882715 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3251689486205578 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1625359766185284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0402799341827631 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.559633994475007 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23947900626808405 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3595189191401005 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.042537887580693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22299890406429768 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37861906457692385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10379008017480373 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18156901933252811 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04006002563983202 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06881903391331434 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08849997539073229 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17616990953683853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860898479819298 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016059959307312965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08378003258258104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04495005123317242 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3808560324832797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08678005542606115 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.175770022906363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03826990723609924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09396998211741447 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5586179904639721 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122991468757391 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19293907098472118 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0907989451661706 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.28332893270999193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06792007479816675 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3996290033683181 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6097149346023798 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03455905243754387 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.009961986914277 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16389903612434864 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27690897695720196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13270904310047626 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040079932659864426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06849004421383142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08486001752316952 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17245893832296133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08407002314925194 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04037993494421244 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1913259513676167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08602894376963377 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17288897652179003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037409947253763676 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.092698959633708 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5584980826824903 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09427010081708431 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19192893523722887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08942896965891123 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26484893169254065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06538990419358015 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37948903627693653 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5872740186750889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034590018913149834 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9810920348390937 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12281991075724363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23581902496516705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07455004379153252 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13361906167119741 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041199964471161366 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06890995427966118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0846589682623744 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17284892965108156 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04972994793206453 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01601001713424921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08378899656236172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04046002868562937 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1555959936231375 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0852199736982584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17307896632701159 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03760005347430706 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09312003385275602 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.558157917112112 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021896403282881 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1921490766108036 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893700635060668 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26243901811540127 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06552005652338266 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.375628937035799 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5788250602781773 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03465008921921253 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9774830434471369 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.927264065481722 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08560996502637863 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025848974473774433 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15356007497757673 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 60.108122997917235 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19820895977318287 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04836008884012699 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04066003020852804 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026159919798374176 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03696000203490257 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25572895538061857 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07192895282059908 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07057993207126856 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575700007379055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050709000788629055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08370005525648594 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04008994437754154 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06839900743216276 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08516001980751753 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17122994177043438 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04902889486402273 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610990148037672 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0837499974295497 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9445470059290528 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06768899038434029 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15380897093564272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03679993096739054 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09308895096182823 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.558307976461947 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08932000491768122 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19428902305662632 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08974992670118809 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26504904963076115 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0648299464955926 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37511903792619705 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.576444017700851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034529948607087135 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9638430094346404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08458003867417574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17332995776087046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04933006130158901 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0848500058054924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040440005250275135 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06830005440860987 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09540002793073654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18470990471541882 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015729921869933605 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08373009040951729 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.965106999501586 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08389900904148817 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17117895185947418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03708002623170614 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09263900574296713 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5543080624192953 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978997357189655 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19327900372445583 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935993537306786 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2636989811435342 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06406009197235107 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.372827984392643 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5690140426158905 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03451993688941002 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9583329558372498 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0782989664003253 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10471895802766085 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.040437961928546 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076299998909235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16790907829999924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04924007225781679 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08173997048288584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04046002868562937 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06816000677645206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742599368095398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594389323145151 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049390015192329884 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1307599013671279 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03215007018297911 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.029046019539237 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384002674371004 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067002434283495 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0417300034314394 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09155995212495327 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19709905609488487 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08999998681247234 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1933289458975196 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0889789080247283 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18860993441194296 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0864989124238491 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32059894874691963 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1624960461631417 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03969005774706602 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5515449922531843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532001473009586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234908252954483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048359972424805164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13019901234656572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04018004983663559 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06781995762139559 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073009985499084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.157649046741426 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048840069212019444 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015750061720609665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1312090316787362 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0185959981754422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362896576523781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15954894479364157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04239997360855341 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09116996079683304 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19145896658301353 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896901272237301 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.38331898394972086 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08915993385016918 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18938002176582813 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08517899550497532 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.320859020575881 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.344354939647019 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03997993189841509 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7330440459772944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1334089320152998 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16067898832261562 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.595489987172186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07773004472255707 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17084891442209482 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05018990486860275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12948003131896257 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039569917134940624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687700230628252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482897490262985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610389444977045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04857999738305807 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0156499445438385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22861897014081478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03714999184012413 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1827159905806184 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442000787705183 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16120891086757183 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053270021453499794 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09765895083546638 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2039989922195673 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218999184668064 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21339894738048315 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09023991879075766 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1359399175271392 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12858910486102104 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30906894244253635 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.184884924441576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050749978981912136 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5964440535753965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566995918750763 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16667903400957584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04972994793206453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2271089470013976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03971997648477554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06785895675420761 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07528008427470922 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16101007349789143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048410031013190746 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22941897623240948 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.223895000293851 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07478008046746254 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16140902880579233 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053629977628588676 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20706909708678722 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0907799694687128 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21272001322358847 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09044900070875883 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13530999422073364 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12880901340395212 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3091689432039857 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1812259908765554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051149982027709484 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5933839604258537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22427004296332598 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2508800243958831 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.951118073426187 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0763199059292674 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16599008813500404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049149966798722744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22654002532362938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03957911394536495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06882997695356607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456005550920963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16054895240813494 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048569985665380955 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015659956261515617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22815901320427656 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2161060003563762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385993376374245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15877909027040005 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04817999433726072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22675900254398584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039899954572319984 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06814999505877495 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08300901390612125 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16806903295218945 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900002386420965 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22799905855208635 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2148460373282433 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.473742002621293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16103999223560095 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892994184046984 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22700894623994827 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04021904896944761 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06849993951618671 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469009142369032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15945907216519117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049110036343336105 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015369965694844723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22839894518256187 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2126860674470663 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486902177333832 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16303895972669125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05313998553901911 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233900345861912 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19466993398964405 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0903300242498517 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2102989237755537 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08889997843652964 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12432900257408619 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12844998855143785 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.2980689750984311 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1552060022950172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05064008291810751 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5688439598307014 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07505994290113449 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16217899974435568 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22690894547849894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07129996083676815 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413001731038094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16060902271419764 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842004273086786 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015600002370774746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22821896709501743 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2152360286563635 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.092645016498864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0768100144341588 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16418995801359415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049339025281369686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42343896348029375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04032999277114868 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0686390558257699 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07901003118604422 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16603001859039068 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892901051789522 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525005791336298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2288990654051304 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06895000115036964 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5084949554875493 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07441896013915539 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16135897021740675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047909910790622234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42359798680990934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025001544505358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06795988883823156 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736890360713005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602190313860774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014950055629014969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22855889983475208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0687700230628252 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4951149933040142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472001016139984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16197003424167633 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06222003139555454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4244990414008498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04016002640128136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06788899190723896 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503991946578026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16058993060141802 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048798974603414536 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015030032955110073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22863899357616901 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06876990664750338 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5124649507924914 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22541906218975782 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30526891350746155 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.975102958269417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538008503615856 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16251998022198677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834996070712805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4232489736750722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03985990770161152 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0695990165695548 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545005064457655 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16082008369266987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04837906453758478 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2285689115524292 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06967002991586924 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.497955061495304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07395900320261717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16100902575999498 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05343998782336712 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09306904394179583 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2035499783232808 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09148998651653528 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21866895258426666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09020010475069284 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13496901374310255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1286299666389823 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3091789549216628 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1864049592986703 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05088001489639282 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.599324052222073 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518904749304056 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16326899640262127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913995508104563 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42321800719946623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820994894951582 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16055908054113388 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834006540477276 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015520025044679642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22837903816252947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0699490774422884 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4979139668866992 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16127002891153097 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052949064411222935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09367999155074358 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20390900317579508 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09166006930172443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2128100022673607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980906568467617 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13565004337579012 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1288490602746606 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30942901503294706 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.181115978397429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050870003178715706 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5931439120322466 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07604004349559546 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16643991693854332 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04984904080629349 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3256590571254492 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951007965952158 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06898993160575628 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494993042200804 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1636900706216693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2290499396622181 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05893898196518421 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.401985064148903 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392000406980515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16229995526373386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053168972954154015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09190000128000975 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2033290220424533 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09059999138116837 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21063897293061018 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983910083770752 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13588008005172014 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12866896577179432 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30944892205297947 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1746360687538981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05080993287265301 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.587094971910119 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22908905521035194 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31483802013099194 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.72306402400136 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13126898556947708 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.220649060793221 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053909956477582455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32557896338403225 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04127994179725647 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06883894093334675 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453002035617828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599990064278245 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892994184046984 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015669967979192734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1309600193053484 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05948892794549465 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.359394984319806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07309997454285622 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15950901433825493 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04183000419288874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923000043258071 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19245909061282873 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896799610927701 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19846996292471886 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08877902291715145 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19000901374965906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08567003533244133 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3210089635103941 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1613559909164906 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040109967812895775 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5500540612265468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09287998545914888 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18194003496319056 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050288974307477474 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.227969023399055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03912008833140135 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06824999582022429 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416901644319296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15968899242579937 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04883005749434233 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13085897080600262 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04849000833928585 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.203396008349955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445900700986385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16071891877800226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04197994712740183 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09904906619340181 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19195000641047955 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913909550756216 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19393907859921455 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08879008237272501 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18966896459460258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08552998770028353 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3205189714208245 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1614259565249085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03996898885816336 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5512841055169702 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08041004184633493 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16805902123451233 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049359980039298534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17907004803419113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950006794184446 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684000551700592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15905906911939383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892004653811455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595995854586363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13130903244018555 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04243990406394005 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1331860441714525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07321999873965979 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15918898861855268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04234991502016783 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112001862376928 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1906190300360322 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905003778636456 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19217003136873245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0894690165296197 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1892299624159932 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08552905637770891 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3202089574187994 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1501159751787782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039619975723326206 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.538145006634295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23944908753037453 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3595480229705572 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.91019799746573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22147910203784704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3741680411621928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10388006921857595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18040905706584454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04057993646711111 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866990588605404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08584000170230865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17304904758930206 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04851992707699537 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01555995550006628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08312903810292482 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04406995140016079 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3695049565285444 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08525897283107042 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17244892660528421 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03717991057783365 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09251909796148539 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.555317965336144 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09003002196550369 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19226898439228535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972000796347857 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2608499489724636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06470002699643373 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3704479895532131 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5671539586037397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03486999776214361 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9586229464039207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16316003166139126 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.28324907179921865 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0742599368095398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13116898480802774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039869919419288635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06786000449210405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0835490645840764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17056905198842287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050080008804798126 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015540048480033875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08280901238322258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03938993904739618 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.186726032756269 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08473999332636595 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17171900253742933 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03729003947228193 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09301002137362957 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5541079444810748 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032990783452988 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19188900478184223 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08973001968115568 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.261149019934237 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0644089886918664 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3708979347720742 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5661240322515368 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0344900181517005 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9563731038942933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12246996629983187 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23398897610604763 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0744890421628952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13136002235114574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039829988963902 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06776896771043539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08396001067012548 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17041899263858795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04916999023407698 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08340994827449322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04050997085869312 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1401450028643012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08519005496054888 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1727700000628829 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03750005271285772 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09318010415881872 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5556580144912004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08984003216028214 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19291997887194157 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0899189617484808 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2619990846142173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06434996612370014 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37101900670677423 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5689448919147253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03395997919142246 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9584730034694076 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.79553407523781 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08654000703245401 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02610008232295513 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10892900172621012 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.74420702457428 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19887008238583803 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.045269960537552834 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04055991303175688 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025919987820088863 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03718002699315548 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2445490099489689 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06932893302291632 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06665999535471201 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1537799835205078 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04964007530361414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08348003029823303 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876998718827963 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06913894321769476 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08523010183125734 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1724699977785349 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898000042885542 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015779980458319187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08274998981505632 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.950826914049685 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06333994679152966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15046901535242796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03674998879432678 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09332899935543537 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5577880656346679 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006995242089033 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1939090434461832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09047903586179018 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26344903744757175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0645399559289217 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3738390514627099 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5763540286570787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03421003930270672 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9597419304773211 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08420005906373262 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17297000158578157 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0496390275657177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08183997124433517 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03988004755228758 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06799993570894003 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08481007535010576 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17191004008054733 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0493490369990468 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01580000389367342 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08263997733592987 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9450770448893309 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08257897570729256 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16998895443975925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037770019844174385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09324902202934027 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5609879735857248 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1944090472534299 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897599384188652 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26330898981541395 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06433902308344841 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37227803841233253 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5778039814904332 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03451004158705473 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9663330167531967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0784390140324831 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10495900642126799 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.0298379976302385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07717008702456951 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1689799828454852 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04939897917211056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08099002297967672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040359911508858204 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06809900514781475 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458007894456387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601100666448474 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04841899499297142 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01604994758963585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13085000682622194 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032070092856884 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.028347061946988 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07417995948344469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039004549384117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04213990177959204 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09227893315255642 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1909690909087658 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09085994679480791 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19567902199923992 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09036005940288305 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.191868981346488 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08859997615218163 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3275889903306961 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1671949177980423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039289938285946846 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5569740207865834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07562898099422455 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16287900507450104 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1300489529967308 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03994000144302845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06874999962747097 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738389790058136 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15942903701215982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048810034058988094 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015960074961185455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13122905511409044 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0233059292659163 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07304002065211535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15953904949128628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04300998989492655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09238009806722403 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19305897876620293 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08932896889746189 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19516993779689074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08929893374443054 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19009900279343128 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08664000779390335 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32427895348519087 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1626259656623006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03991997800767422 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5526139177381992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1271899091079831 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15385996084660292 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.4174610413610935 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07727008778601885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16896892338991165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04959001671522856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12897897977381945 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040260027162730694 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06812007632106543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506005931645632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16142998356372118 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901003558188677 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01593900378793478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23520994000136852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03729900345206261 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.187735004350543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07525994442403316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16312010120600462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05353894084692001 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09275996126234531 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2029889728873968 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126006625592709 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21109904628247023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08974899537861347 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13595994096249342 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1326490892097354 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3148689866065979 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1815560283139348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05047989543527365 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5964149497449398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07598998490720987 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16375002451241016 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048840069212019444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22725900635123253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039560021832585335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681489473208785 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465004455298185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16038003377616405 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812993574887514 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015650060959160328 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22810895461589098 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.214934978634119 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457006722688675 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16069004777818918 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053699943237006664 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253004100173712 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20257895812392235 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967006579041481 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2165789483115077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901190796867013 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1359300222247839 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13014895375818014 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3108589444309473 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1808760464191437 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05071994382888079 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5931640518829226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22420892491936684 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25101902429014444 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.946257966570556 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07685006130486727 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16782002057880163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968896973878145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2272190758958459 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040190061554312706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687700230628252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07424899376928806 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598390517756343 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048770103603601456 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015849946066737175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22839894518256187 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2194360606372356 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07382896728813648 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15865894965827465 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04861992783844471 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22718904074281454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03941997420042753 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06779900286346674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434003055095673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15868002083152533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04841899499297142 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501990482211113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22788892965763807 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2047450290992856 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.467861049808562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07763993926346302 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1671690260991454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048479996621608734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22808904759585857 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04097993951290846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06819004192948341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07528008427470922 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16094895545393229 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015430036000907421 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22850895766168833 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2221760116517544 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0758500536903739 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16460008919239044 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053020077757537365 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917200231924653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1947790151461959 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902500469237566 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20868901629000902 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028904605656862 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.12513995170593262 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12972892727702856 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3006990300491452 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1571460636332631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050110043957829475 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.571855042129755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07651001214981079 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16626901924610138 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04966999404132366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2272190758958459 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04060007631778717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06818899419158697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458997424691916 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612800406292081 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825007636100054 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01576007343828678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2279590116813779 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2195350136607885 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.109865054488182 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07603899575769901 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637890236452222 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048390007577836514 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42423803824931383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04105991683900356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06822007708251476 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458892650902271 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613890053704381 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048720045015215874 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015170080587267876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22861897014081478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06903999019414186 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5102139441296458 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552991155534983 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1620299881324172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048378948122262955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42311905417591333 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0400301069021225 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06833893712610006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475004531443119 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608500024303794 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048708985559642315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015269964933395386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22853899281471968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06945000495761633 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4970849733799696 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524900138378143 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16166898421943188 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4232879728078842 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04036002792418003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06822997238487005 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466902025043964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16027898527681828 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873995203524828 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23516907822340727 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07019995246082544 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5057650161907077 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2255789004266262 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3061590250581503 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.97439200989902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075560063123703 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624989090487361 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4239690024405718 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040289945900440216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06879994180053473 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472897414118052 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16098900232464075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22919895127415657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07003999780863523 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5008540358394384 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355993147939444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16138993669301271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05319993942975998 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09410001803189516 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20381901413202286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09158998727798462 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21009903866797686 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08930906187742949 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13613991905003786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12935895938426256 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3108589444309473 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.178345992229879 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05067000165581703 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5914150280877948 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0890200026333332 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17869006842374802 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049390015192329884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4242589930072427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04085991531610489 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06828899495303631 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747699523344636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16052997671067715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824891220778227 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015660072676837444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22896903101354837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07007992826402187 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5199650079011917 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07375003769993782 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611590851098299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05277001764625311 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233003947883844 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20364904776215553 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09123899508267641 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20998995751142502 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972990326583385 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1358699519187212 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12874009553343058 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3094789572060108 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1757260654121637 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050539965741336346 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5871940413489938 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07631001062691212 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16582990065217018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04938896745443344 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3256590571254492 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040619983337819576 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06896001286804676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07529999129474163 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17015996854752302 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050029950216412544 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015689991414546967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2286600647494197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05888903979212046 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4080150285735726 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07417995948344469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16216898802667856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053329975344240665 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09322992991656065 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20245893392711878 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09117892477661371 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2092299982905388 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08954002987593412 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13584992848336697 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12959004379808903 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.310619012452662 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1759259505197406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05060900002717972 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5901940641924739 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22946891840547323 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3161189379170537 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.727885015308857 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13062905054539442 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2217390574514866 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05355000030249357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32604800071567297 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040560029447078705 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06892997771501541 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07540907245129347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604290446266532 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04924007225781679 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311389496549964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05968997720628977 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3598150108009577 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08739891927689314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17581903375685215 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04284002352505922 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256892371922731 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1922700321301818 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969008922576904 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19210902974009514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09004003368318081 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18875894602388144 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08523999713361263 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31943898648023605 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1544759618118405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03984000068157911 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5600139740854502 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18099904991686344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05010003224015236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2278890460729599 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969995304942131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06852997466921806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159058952704072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016000005416572094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13093906454741955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04905997775495052 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.202716026455164 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342908065766096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607389422133565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04199007526040077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09196903556585312 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19525992684066296 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08899904787540436 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19146909471601248 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935004007071257 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.188878970220685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08669996168464422 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.328248948790133 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1639859294518828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03960996400564909 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5537140425294638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07855996955186129 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16726891044527292 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05019002128392458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17926900181919336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04015001468360424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06860005669295788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423909846693277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15921902377158403 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871003329753876 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015980098396539688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13140903320163488 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043169944547116756 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1359560303390026 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370905950665474 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1605789875611663 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04221999552100897 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167904499918222 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19102997612208128 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08950894698500633 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1931790029630065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891699455678463 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.19036896992474794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0861600274220109 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3223889507353306 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.154746045358479 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039739999920129776 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5442039584740996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23882894311100245 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35873905289918184 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.931218064390123 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22180902305990458 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37484895437955856 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10396004654467106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1810200046747923 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04088995046913624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06771995685994625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0856500118970871 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17209898214787245 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04901993088424206 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08328992407768965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0442389864474535 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3706149766221642 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08488004095852375 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17228990327566862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03722892142832279 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214004967361689 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5552880465984344 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037903510034084 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19187002908438444 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090269953943789 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2625390188768506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06436998955905437 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3721689572557807 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5682550147175789 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03466999623924494 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.958732958883047 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1625589793547988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27543900068849325 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07452000863850117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13129995204508305 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04029006231576204 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789900362491608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08452998008579016 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17114996444433928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04989898297935724 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08292007260024548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04357995931059122 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.186736044473946 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08501007687300444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17325999215245247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03766000736504793 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09300000965595245 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5554279778152704 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08939998224377632 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1906990073621273 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896004874259233 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2629889640957117 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06415997631847858 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37270900793373585 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5666239196434617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034400029107928276 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9589030416682363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1199300168082118 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2324999077245593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0733700580894947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13088993728160858 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039618927985429764 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06769003812223673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0843399902805686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17160899005830288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04959991201758385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015779980458319187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08303998038172722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038680038414895535 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1342059588059783 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08518993854522705 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17198000568896532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037038931623101234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09158998727798462 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5548479966819286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905900176614523 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1919299829751253 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08859904482960701 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2610889496281743 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06444996688514948 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3711390309035778 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5640950296074152 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034269061870872974 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9540030043572187 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.789424017071724 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08787994738668203 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02587004564702511 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11224905028939247 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.57028893660754 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19223999697715044 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04338892176747322 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.040499959141016006 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026399968191981316 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.036829966120421886 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24270906578749418 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06951892282813787 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06615999154746532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15381001867353916 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05010899621993303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08290004916489124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04016002640128136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068349065259099 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0855099642649293 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17267989460378885 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048558926209807396 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.083319959230721 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9410670027136803 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06299000233411789 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1504090614616871 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03688002470880747 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09323889389634132 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.633268035016954 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09210000280290842 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1944799441844225 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017891716212034 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2643089974299073 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06430002395063639 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37387898191809654 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.656543929129839 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034349970519542694 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.039851970039308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08421996608376503 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17272005788981915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05055905785411596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08173997048288584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04002999048680067 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06733997724950314 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08489005267620087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1714599784463644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05030992906540632 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01599895767867565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08298992179334164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9455470135435462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08274894207715988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17011899035423994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037209945730865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09294005576521158 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5555079551413655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08894992060959339 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19424897618591785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08991989307105541 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2633589319884777 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06458000279963017 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37344801239669323 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5791640616953373 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03416009712964296 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9657730590552092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07827999070286751 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10493898298591375 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.099877995438874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07714005187153816 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16764993779361248 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04943902604281902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08041004184633493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975990694016218 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06816000677645206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07514003664255142 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609299797564745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049008987843990326 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13065000530332327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03190001007169485 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0247969767078757 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406994700431824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16081996727734804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041860039345920086 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0918699661269784 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19144895486533642 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09023002348840237 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19476900342851877 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909997995942831 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18873007502406836 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08566898759454489 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32009801361709833 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1567659676074982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03985990770161152 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5464350581169128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545901462435722 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16301905270665884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894996527582407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12981891632080078 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04051998257637024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06827991455793381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435993757098913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16802898608148098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049390015192329884 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13153895270079374 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.031516003422439 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343990728259087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15997898299247026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04276004619896412 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218009654432535 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19146897830069065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08949905168265104 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19335001707077026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08887902367860079 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.188948935829103 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08496001828461885 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31863898038864136 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1521560372784734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04025001544505358 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.541694044135511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12689991854131222 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15342002734541893 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.399401066824794 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07735902909189463 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1683990703895688 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04934996832162142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12980890460312366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03926991485059261 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06892008241266012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07534900214523077 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161678995937109 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048789894208312035 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22857903968542814 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03730994649231434 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.179856015369296 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459998596459627 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16241997946053743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05359994247555733 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09215995669364929 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20328897517174482 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10652001947164536 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21257891785353422 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021000005304813 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13532000593841076 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12985907960683107 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31016895081847906 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1939259711652994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05034997593611479 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6068450640887022 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07591000758111477 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16319903079420328 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05000003147870302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2270890399813652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04024000372737646 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750901229679585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466995157301426 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16433000564575195 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892004653811455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015590107068419456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22868893574923277 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2205550447106361 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07373990956693888 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1603689743205905 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05377002526074648 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09216996841132641 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20371901337057352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09009905625134706 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2111300127580762 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08962990250438452 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13564899563789368 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12973998673260212 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3101190086454153 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1756560998037457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050798989832401276 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5859740087762475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22520904894918203 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2525090239942074 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.9483080403879285 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07714994717389345 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16835902351886034 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894996527582407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22726901806890965 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04412000998854637 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843998562544584 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415004074573517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15844893641769886 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875997547060251 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015579978935420513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22794899996370077 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2199960183352232 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342000026255846 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15913904644548893 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04891003482043743 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22681907285004854 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040010083466768265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0672490568831563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387006189674139 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907897613942623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880002234131098 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2282590139657259 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2056550476700068 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4691009894013405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07410906255245209 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16217899974435568 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048660091124475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22793898824602365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03966002259403467 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06828003097325563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407006341964006 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16000994946807623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048390007577836514 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01563900150358677 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22798997815698385 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2124150525778532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07409998215734959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16895995941013098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053718918934464455 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09290000889450312 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19360892474651337 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972000796347857 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21080905571579933 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896890414878726 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1252898946404457 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1298889983445406 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3006890183314681 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1578460689634085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04988000728189945 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.577754970639944 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07536006160080433 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643989235162735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871992859989405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2268790267407894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04029995761811733 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789900362491608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07460999768227339 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16135000623762608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048369052819907665 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01588999293744564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22829894442111254 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2153949355706573 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.101124941371381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0765800941735506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16377901192754507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048470101319253445 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4234790103510022 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03983895294368267 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06858992855995893 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07500010542571545 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16084895469248295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048779998905956745 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2291389973834157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07042009383440018 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5048549976199865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1632890198379755 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048699905164539814 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4231879720464349 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969005774706602 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06880005821585655 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453002035617828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16095000319182873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483999028801918 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015420024283230305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22844003979116678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06910006050020456 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4993040822446346 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459893822669983 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16105908434838057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04813005216419697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42303791269659996 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963999915868044 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757001392543316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07468904368579388 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16049901023507118 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159952454268932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22826902568340302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06944010965526104 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4921940164640546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22487889509648085 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.303969020023942 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.956681979820132 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07548904977738857 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16289902850985527 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04929001443088055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42322801891714334 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04076003096997738 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.068910070694983 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546890992671251 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16137899365276098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048470101319253445 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410012565553188 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22816890850663185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06984011270105839 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5018850099295378 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413991261273623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16189995221793652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05300994962453842 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256997145712376 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20285905338823795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09212002623826265 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21125900093466043 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08952000644057989 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.1355790300294757 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1298799179494381 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3105779178440571 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.177484984509647 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05001993849873543 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5902140876278281 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751490006223321 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630790065973997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049080001190304756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4230779595673084 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03996002487838268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684899277985096 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489905692636967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15994894783943892 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830991383641958 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.029250048100948334 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22930896375328302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07009902037680149 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5131339896470308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07443001959472895 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16211997717618942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053249066695570946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09272003080695868 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2034190110862255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112001862376928 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21141895558685064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957891259342432 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.13512000441551208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12866896577179432 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.30840898398309946 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.175055978819728 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05110003985464573 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5899239806458354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757899833843112 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16430998221039772 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04902901127934456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3252290189266205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039929989725351334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06831996142864227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07536006160080433 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16098003834486008 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843994975090027 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01590896863490343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22860988974571228 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05888903979212046 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3923650840297341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736600486561656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608988968655467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05288003012537956 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202002547681332 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20833895541727543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169906843453646 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21129997912794352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953001815825701 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.135940033942461 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12965896166861057 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31045894138514996 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.182015985250473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05079992115497589 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.59287394490093 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2294389996677637 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3159790067002177 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.710405021905899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13125897385179996 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22158899810165167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05395000334829092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32610888592898846 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03995909355580807 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06911996752023697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480999920517206 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15990901738405228 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1312500098720193 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06001000292599201 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.359604997560382 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402000483125448 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16012892592698336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04258006811141968 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09123003110289574 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1917890040203929 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08973001968115568 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19318005070090294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910894393920898 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18937001004815102 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08673896081745625 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32264902256429195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1575659736990929 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03979995381087065 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5466950135305524 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09353994391858578 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18257007468491793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049949041567742825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22815994452685118 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399099662899971 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06853998638689518 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07440894842147827 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15865906607359648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04882004577666521 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13102905359119177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04864996299147606 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2091160751879215 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372896652668715 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599190291017294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042230007238686085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09093002881854773 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1904699020087719 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08941895794123411 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19252998754382133 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08949998300522566 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.188878970220685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08579995483160019 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.32016902696341276 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.151675940491259 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03986898809671402 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5410740161314607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07563002873212099 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16318890266120434 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048819929361343384 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17961894627660513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06829004269093275 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07359904702752829 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17400900833308697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048810034058988094 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015720026567578316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1312090316787362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04257995169609785 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1440160451456904 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406901568174362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16002892516553402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0423000892624259 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913889380171895 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1908800331875682 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09052909445017576 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1934089232236147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08855003397911787 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.18881901632994413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08572009392082691 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.31967891845852137 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1515960795804858 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03908004146069288 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5393641078844666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23831904400140047 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3576090093702078 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.911718032322824 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22119900677353144 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37468899972736835 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10344001930207014 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18043909221887589 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04000996705144644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06822904106229544 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08522998541593552 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17224997282028198 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04994997289031744 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08316000457853079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04382000770419836 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3640549732372165 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08503999561071396 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17351005226373672 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03815907984972 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09387009777128696 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5543980514630675 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09077892173081636 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19211997278034687 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021896403282881 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2622590400278568 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06440992001444101 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3717090003192425 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5709250001236796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03413890954107046 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.970923040062189 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16286899335682392 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2748690312728286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07479998748749495 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13116898480802774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08532905485481024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17245905473828316 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01580000389367342 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08281890768557787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03891007509082556 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1803460074588656 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08509005419909954 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17210899386554956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03708002623170614 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09262003004550934 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5575179820880294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09166006930172443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19276898819953203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08933001663535833 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.2619590377435088 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0644599786028266 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3715179627761245 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.5794939827173948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034710043109953403 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9688330357894301 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12107903603464365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23345905356109142 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07445900700986385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13091997243463993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04000996705144644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06735906936228275 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08515000808984041 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17300003673881292 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04931993316859007 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08273997809737921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03903999458998442 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1375260073691607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0847099581733346 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17156009562313557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03724999260157347 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09183003567159176 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.559277948923409 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970906492322683 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1912600127980113 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912896737456322 ms for forwarding
<class 'torch_int.nn.fused.GEGLUQ'> take 0.26183901354670525 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06435997784137726 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3851590445265174 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.585594960488379 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034430064260959625 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.976202940568328 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.82422404922545 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0893899705260992 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026219990104436874 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11161994189023972 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.58447798527777 ms for forwarding
