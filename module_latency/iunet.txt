--------------------
No. 1
<class 'diffusers.models.embeddings.Timesteps'> take 0.36802911199629307 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.11609005741775036 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.07555983029305935 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.08245999924838543 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04555005580186844 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.4417689051479101 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 961.8987150024623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.3071990795433521 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.5417789798229933 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07243896834552288 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14253007248044014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0432401429861784 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1317090354859829 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10718917474150658 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21012895740568638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049640191718935966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.03447011113166809 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09549921378493309 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6406739596277475 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07923995144665241 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17163902521133423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04186015576124191 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1216100063174963 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.8422872051596642 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09562913328409195 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20569004118442535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09218999184668064 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14873920008540154 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.4955180920660496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07639010436832905 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.6292681209743023 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 2.2062030620872974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03926013596355915 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.667410997673869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09412993676960468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.210530124604702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05168910138309002 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09054993279278278 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040449900552630424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07581012323498726 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09434996172785759 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18344982527196407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049560097977519035 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016540056094527245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09369011968374252 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0452968999743462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09374902583658695 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1834789291024208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03940984606742859 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09571900591254234 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.604348024353385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09334995411336422 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1951991580426693 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094993583858013 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14173914678394794 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3796480596065521 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07336004637181759 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5038878880441189 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7702640034258366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03878003917634487 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1869719494134188 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08738995529711246 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11747004464268684 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 7.814283948391676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07872888818383217 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17621880397200584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04958012141287327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09112991392612457 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04096003249287605 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0777500681579113 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07557892240583897 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17123902216553688 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910980351269245 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01640990376472473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14515896327793598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03393995575606823 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.105346018448472 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07554003968834877 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16845902428030968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04641013219952583 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09460002183914185 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21429895423352718 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09255018085241318 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19519985653460026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08947891183197498 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11505000293254852 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2651689574122429 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09551993571221828 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4124990664422512 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2891460210084915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042319996282458305 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.701383851468563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07753982208669186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17110980115830898 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04912004806101322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14638900756835938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040040118619799614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07368996739387512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07557007484138012 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1640899572521448 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015830155462026596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14533894136548042 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0815169662237167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07523898966610432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16556913033127785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04546996206045151 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09526987560093403 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20119897089898586 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09192898869514465 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20167999900877476 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09124912321567535 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11518015526235104 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2646490465849638 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09605009108781815 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4127989523112774 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.300096046179533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04307902418076992 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7124440055340528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1409898977726698 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1701698638498783 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.889510968700051 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07840991020202637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17405999824404716 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05010003224015236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1434790901839733 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04066992551088333 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07441011257469654 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07795006968080997 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677200198173523 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016700010746717453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25372905656695366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03991997800767422 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2623050715774298 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506995461881161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1648501493036747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05721999332308769 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09400001727044582 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23191911168396473 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09283004328608513 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21565891802310944 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09222002699971199 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11372007429599762 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22447993978857994 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14434894546866417 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4239981062710285 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.34753598831594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05553988739848137 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7845151014626026 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0780599657446146 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17102016136050224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05038990639150143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2541190478950739 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0410501379519701 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0769591424614191 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07690000347793102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1665898598730564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499100424349308 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01664995215833187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.255838967859745 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3158859219402075 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535004988312721 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667100004851818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059450045228004456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09549013338983059 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20659901201725006 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909299124032259 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21361908875405788 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08945888839662075 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11313008144497871 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22264989092946053 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14488911256194115 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4177689552307129 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.305385958403349 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05663000047206879 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7439539078623056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2499499823898077 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2785900142043829 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.50973804295063 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07880991324782372 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17391005530953407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04996894858777523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2526598982512951 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04064897075295448 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07511000148952007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07729977369308472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1662189606577158 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049480004236102104 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01634005457162857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2536789979785681 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.316535985097289 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07645017467439175 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16697891987860203 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049080001190304756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521390561014414 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398498959839344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0748499296605587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07569906301796436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644291914999485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04897988401353359 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0156499445438385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2536389511078596 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2928959913551807 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.6629117783159018 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07615005597472191 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16807904466986656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04877988249063492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25395886041224003 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03984011709690094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07841899059712887 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07611000910401344 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16652001067996025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04934915341436863 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25572883896529675 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3036350719630718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07624994032084942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1699100248515606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0602800864726305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09813997894525528 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2142791636288166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914291013032198 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21142000332474709 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09323912672698498 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10475004091858864 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21193898282945156 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14491006731987 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40742894634604454 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.307095866650343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0560590997338295 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7639938741922379 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07713004015386105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16762991435825825 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050819944590330124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25455886498093605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04016002640128136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0723500270396471 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07578893564641476 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1649288460612297 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049549853429198265 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01625996083021164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25275908410549164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3015060685575008 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.491614876314998 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07784995250403881 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17007999122142792 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863901995122433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4709989298135042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03972998820245266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07585901767015457 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08562998846173286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17614918760955334 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04974985495209694 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016379868611693382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25650905445218086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0767598394304514 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6308750491589308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0759088434278965 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1668089535087347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04839012399315834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47038798220455647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03910018131136894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07501989603042603 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07533910684287548 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16518891789019108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01580989919602871 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25359890423715115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07911003194749355 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6068650875240564 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566995918750763 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16528996638953686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4707779735326767 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039659906178712845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07256888784468174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07540988735854626 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16461987979710102 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05074008367955685 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01667998731136322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25704014115035534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07647997699677944 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6142551321536303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25113997980952263 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.4622088745236397 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.505451932549477 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07707998156547546 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17039990052580833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049768947064876556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4732389934360981 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04145991988480091 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08015893399715424 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07705995813012123 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16672001220285892 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049868831411004066 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016479985788464546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2541390713304281 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08025998249650002 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6339351423084736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07461896166205406 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16571907326579094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058110104873776436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09463983587920666 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20665000192821026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233900345861912 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21489011123776436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205914102494717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11322996579110622 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22393884137272835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14409003779292107 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4206690937280655 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3132961466908455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055579934269189835 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.748624024912715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07853005081415176 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18675881437957287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05076010711491108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47058891505002975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040828948840498924 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07541012018918991 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07721013389527798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16999896615743637 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05016988143324852 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01672981306910515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2533190418034792 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08006999269127846 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6454451251775026 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518893107771873 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1663391012698412 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05795015022158623 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09374902583658695 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20686001516878605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09127985686063766 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21388893947005272 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09131990373134613 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11348910629749298 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22304896265268326 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14447001740336418 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41764904744923115 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3053049333393574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05542999133467674 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7420139629393816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07703900337219238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17190910875797272 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05104998126626015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36205886863172054 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04197005182504654 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0765589065849781 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0771000050008297 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16777007840573788 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04942901432514191 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01650000922381878 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25436910800635815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06612995639443398 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5100250020623207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0764201395213604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16722897998988628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05797017365694046 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09631994180381298 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20647910423576832 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162910282611847 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21490012295544147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162910282611847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11333008296787739 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22277911193668842 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14468003064393997 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41653914377093315 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3091659639030695 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054898904636502266 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7436540219932795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2541190478950739 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3449090290814638 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.598035994917154 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14164880849421024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23582903668284416 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0570700503885746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3627380356192589 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04155002534389496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0754399225115776 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07632910273969173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664890442043543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05022995173931122 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016339821740984917 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14459900557994843 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06505986675620079 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4668351504951715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07520010694861412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16406900249421597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047770095989108086 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0971299596130848 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20253891125321388 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0950999092310667 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1938999630510807 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110895916819572 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11515012010931969 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2634588163346052 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09540002793073654 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40919892489910126 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2828661128878593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042930012568831444 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.693944213911891 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10041007772088051 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.193469924852252 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05088909529149532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2536699175834656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04063989035785198 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07223011925816536 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0752799678593874 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16422895714640617 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0495098065584898 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01672981306910515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1457489561289549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052930088713765144 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2914659455418587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458007894456387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16368902288377285 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045140041038393974 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11024996638298035 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2017191145569086 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09485986083745956 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19440986216068268 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057880379259586 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11517014354467392 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.263839028775692 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09710993617773056 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4111691378057003 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2954161502420902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042958883568644524 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7067939043045044 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08622999303042889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17944001592695713 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05043996497988701 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19833003170788288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04097912460565567 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07254001684486866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524993270635605 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16399892047047615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049140071496367455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016390113160014153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14686910435557365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04652002826333046 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.215966185554862 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07500010542571545 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16516889445483685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04536006599664688 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09634997695684433 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1994690392166376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09089987725019455 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19489997066557407 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915289856493473 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11430005542933941 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26365905068814754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09554997086524963 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4090790171176195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2726460117846727 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04275888204574585 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.682403963059187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26161898858845234 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.38974895142018795 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.672077838331461 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23783999495208263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3996281884610653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10727997869253159 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.20078918896615505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041499966755509377 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07242988795042038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09568990208208561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18697883933782578 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050560105592012405 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016829930245876312 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09109894745051861 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04787999205291271 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4666449278593063 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09155902080237865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1819389872252941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03972998820245266 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09461003355681896 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6057389546185732 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0925001222640276 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19252905622124672 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106984362006187 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14292914420366287 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3841191064566374 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07294910028576851 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5120178684592247 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7776340246200562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03700004890561104 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1923528984189034 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17790007404983044 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2965989988297224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07797894068062305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14611007645726204 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04090997390449047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07214886136353016 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09150989353656769 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18127984367311 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499701127409935 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016630161553621292 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0914598349481821 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04191999323666096 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.262645935639739 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09217998012900352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18303003162145615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03940984606742859 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0948198139667511 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6048879586160183 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09436998516321182 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19212905317544937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09132991544902325 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1425398513674736 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3825791645795107 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07246993482112885 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5065989680588245 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7739450559020042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03782915882766247 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1952029783278704 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12959004379808903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2477690577507019 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0786101445555687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14658900909125805 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04072999581694603 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07322989404201508 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09160884656012058 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1805990468710661 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979013465344906 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01616007648408413 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09354902431368828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04161009564995766 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2145561631768942 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09092013351619244 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17977901734411716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03948993980884552 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09391992352902889 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6045580375939608 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09138905443251133 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1920198556035757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112898260354996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1418201718479395 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.380269018933177 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07287994958460331 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5021090619266033 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.759844133630395 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037289923056960106 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.169092884287238 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.750595014542341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09316997602581978 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02776901237666607 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 3.9899370167404413 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 1031.604487914592 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.27530896477401257 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.10684900917112827 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.06549013778567314 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03048987127840519 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03916001878678799 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.3421381115913391 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.09308988228440285 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08285907097160816 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17945910803973675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050530070438981056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09433995001018047 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04089903086423874 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07708999328315258 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09444914758205414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1818689052015543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04955008625984192 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01921015791594982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09094015695154667 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0261558927595615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07090019062161446 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15787011943757534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03905990161001682 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10235980153083801 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6230080034583807 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233900345861912 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1990501768887043 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08949893526732922 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14154007658362389 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.38425903767347336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07263990119099617 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.506408978253603 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.8021839205175638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03708992153406143 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.2101830691099167 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09159999899566174 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18200906924903393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04976987838745117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08903001435101032 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03972998820245266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07133884355425835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09144982323050499 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1800099853426218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04897010512650013 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015980098396539688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09071012027561665 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9905870538204908 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09035901166498661 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17882883548736572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03897002898156643 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09375903755426407 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6029980722814798 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149010293185711 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19005895592272282 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986983448266983 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13956916518509388 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37705921567976475 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0720098614692688 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4963779356330633 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.747573958709836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03688992001116276 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.156993141397834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08515897206962109 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11344882659614086 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.612108089029789 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07776007987558842 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17062993720173836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04961993545293808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0879399012774229 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040400074794888496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07256888784468174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457984611392021 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16169995069503784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04960992373526096 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01622992567718029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.143399927765131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033769989386200905 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.068376936018467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07554003968834877 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16261893324553967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04461011849343777 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214901365339756 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19997009076178074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912398099899292 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19310996867716312 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09007006883621216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11348002590239048 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.261989189311862 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09399000555276871 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40373881347477436 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2544961646199226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042109983041882515 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.654474064707756 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581990212202072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1642100978642702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04991888999938965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14337990432977676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04046992398798466 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07123895920813084 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742599368095398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17015007324516773 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048588961362838745 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016179867088794708 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14406000263988972 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.072936924174428 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073309987783432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16094883903861046 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04497985355556011 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248894639313221 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19927998073399067 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09077903814613819 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19150995649397373 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08949008770287037 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11415011249482632 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26227906346321106 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.094669871032238 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4055490717291832 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2543858028948307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042250147089362144 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6554738394916058 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1393898855894804 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16711000353097916 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.727711832150817 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0773801002651453 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16976986080408096 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05006999708712101 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14224904589354992 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040090177208185196 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07257005199790001 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07522990927100182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631800550967455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04952005110681057 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016150064766407013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525588497519493 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040140002965927124 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2405761517584324 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749398022890091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16320007853209972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055969925597310066 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10844017378985882 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20898878574371338 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072991088032722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21455902606248856 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09079999290406704 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11222879402339458 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22127898409962654 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14397897757589817 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4144280683249235 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.314816065132618 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05427980795502663 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7404051031917334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07690885104238987 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16630906611680984 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049819936975836754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510291524231434 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039830105379223824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07299007847905159 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075159827247262 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16292999498546124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016488833352923393 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25246990844607353 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2858959380537271 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07553980685770512 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16363989561796188 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057480065152049065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09240000508725643 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2035689540207386 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967006579041481 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21263910457491875 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897599384188652 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11175894178450108 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2305090893059969 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14472892507910728 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4231981001794338 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2977849692106247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054560136049985886 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7257051076740026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24883891455829144 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2769290003925562 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.379059050232172 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07809000089764595 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17188885249197483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05049002356827259 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2512391656637192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040319981053471565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07149903103709221 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07578008808195591 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16315001994371414 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979921504855156 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016159843653440475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516589593142271 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2910249643027782 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07403991185128689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16196002252399921 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05005905404686928 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25051902048289776 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03970996476709843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07148995064198971 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541896775364876 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631290651857853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968000575900078 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0159598421305418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25173905305564404 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2763659469783306 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.6133409701287746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07554888725280762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16471906565129757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050140079110860825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.250399112701416 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03982987254858017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0752289779484272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754999928176403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16382010653614998 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04952913150191307 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01600012183189392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.252248952165246 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3001649640500546 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07511000148952007 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16601011157035828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056848861277103424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09227986447513103 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1999491360038519 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21256902255117893 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09123003110289574 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10501989163458347 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2116500400006771 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14385906979441643 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4030379932373762 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2821860145777464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054200179874897 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7124251462519169 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07687998004257679 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16641896218061447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05003996193408966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2507090102881193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07122894749045372 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07558008655905724 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623199786990881 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968000575900078 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01611979678273201 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25193020701408386 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2831550557166338 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.401194863021374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0760199036449194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17419015057384968 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04952982999384403 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46974909491837025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040590064600110054 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07301988080143929 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524015381932259 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16368995420634747 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049869995564222336 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015860190615057945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2520889975130558 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07559987716376781 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6083349473774433 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07543899118900299 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16323896124958992 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879990592598915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46929786913096905 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04001986235380173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07119984365999699 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469905540347099 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16250903718173504 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499100424349308 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015840167179703712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523090224713087 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0757700763642788 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5885450411587954 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459986954927444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16249995678663254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048369867727160454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4691481590270996 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03945990465581417 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07179006934165955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07500988431274891 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16399985179305077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04992005415260792 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016260193660855293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527390606701374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07514888420701027 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5922640450298786 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24935882538557053 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3410489298403263 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.291483132168651 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579894736409187 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16566901467740536 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46952790580689907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039430102333426476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07329997606575489 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07569883018732071 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16440893523395061 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048819929361343384 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01577986404299736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527390606701374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07711001671850681 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5969050582498312 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515004836022854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16345898620784283 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05695014260709286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09281886741518974 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20610983483493328 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08952012285590172 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21241907961666584 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09052990935742855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12576906010508537 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2357990015298128 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14341995120048523 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4270488861948252 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3033950235694647 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054579926654696465 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.732204109430313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07653888314962387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16649882309138775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04964997060596943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4691679496318102 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04061008803546429 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07262988947331905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0764590222388506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16620894894003868 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049530062824487686 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016069971024990082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2522389404475689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07653003558516502 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.609585015103221 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16382918693125248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05695992149412632 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233900345861912 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20283996127545834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21246913820505142 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911010809242725 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11256895959377289 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22084894590079784 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14252006076276302 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41061919182538986 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.283084973692894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05440996028482914 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7091939225792885 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07749907672405243 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1775091513991356 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05089002661406994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3600688651204109 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040280167013406754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0729789026081562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07528997957706451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1626999583095312 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04917895421385765 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016249949112534523 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2522389404475689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06401981227099895 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.488524954766035 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579009979963303 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1653488725423813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056789955124258995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248009882867336 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20578899420797825 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076017886400223 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21157879382371902 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995900861918926 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11276989243924618 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22175000049173832 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1429188996553421 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4135388880968094 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2910061050206423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054419972002506256 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7194540705531836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25211903266608715 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34172902815043926 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.419086087495089 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14060898683965206 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23397919721901417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0567601528018713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3786392044275999 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04158914089202881 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0727199949324131 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07567997090518475 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16323896124958992 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1438490580767393 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06534997373819351 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.466335030272603 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393001578748226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16096909530460835 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045029912143945694 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09291013702750206 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19966904073953629 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08990010246634483 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19225990399718285 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09107915684580803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11530984193086624 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2626890782266855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09404006414115429 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4042091313749552 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2577360030263662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0428489875048399 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.659034052863717 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10048993863165379 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1906000543385744 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05022995173931122 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2517891116440296 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040400074794888496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07254001684486866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420917972922325 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16181915998458862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048430170863866806 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015879981219768524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.15165889635682106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053759897127747536 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2797161471098661 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438892498612404 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16264896839857101 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04554004408419132 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09222002699971199 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20161899738013744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968892507255077 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19197002984583378 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995900861918926 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11413986794650555 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2612289972603321 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0945699866861105 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40368898771703243 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2565860524773598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0421600416302681 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6582838725298643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08529983460903168 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17472985200583935 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04992005415260792 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19742012955248356 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03971019759774208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07199915125966072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385015487670898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593299675732851 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049069058150053024 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016089994460344315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14363019727170467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04641013219952583 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1897268705070019 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16228994354605675 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04463992081582546 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09297998622059822 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19777915440499783 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995993994176388 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1927891280502081 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048986248672009 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1142788678407669 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2619689330458641 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09548990055918694 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4056589677929878 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2639649212360382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04249997437000275 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6661949921399355 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.26107882149517536 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.38786884397268295 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.526699082925916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23734918795526028 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39655901491642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10662991553544998 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19887881353497505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040010083466768265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07233908399939537 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09240000508725643 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18094992265105247 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050798989832401276 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016730045899748802 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09051011875271797 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04744017496705055 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.448185881599784 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09034993126988411 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17921882681548595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039120204746723175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0933490227907896 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6029880605638027 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09161001071333885 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2073291689157486 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0904998742043972 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14073890633881092 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3780380357056856 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07140007801353931 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4983979742974043 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7670639790594578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036109937354922295 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1754030603915453 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17587002366781235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29187905602157116 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07798010483384132 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1453699078410864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04031904973089695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07218983955681324 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09165983647108078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17957878299057484 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04941015504300594 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016120029613375664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09039998985826969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041618943214416504 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2486760970205069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09091012179851532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17864000983536243 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03898888826370239 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0937401782721281 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6032579112797976 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905599445104599 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1905390527099371 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906588975340128 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14094007201492786 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37780916318297386 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0724100973457098 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.498918816447258 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7513851635158062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035969074815511703 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1569530945271254 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12845988385379314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24500885047018528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07788022048771381 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1439889892935753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04003988578915596 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07223989814519882 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0917990691959858 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17967913299798965 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04971004091203213 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015990110114216805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09033014066517353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04171999171376228 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.196976052597165 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09025982581079006 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1791201066225767 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038430094718933105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09344983845949173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6030879449099302 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134900756180286 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19141985103487968 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08993898518383503 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1417100429534912 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3800489939749241 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0722298864275217 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.500658992677927 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7538350075483322 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0366398598998785 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.160273026674986 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.625324910506606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09382003918290138 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02616015262901783 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1418201718479395 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 63.04824398830533 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20373007282614708 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05552009679377079 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.042319996282458305 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025739893317222595 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03705988638103008 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2567088231444359 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07449882104992867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06961985491216183 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15870993956923485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0507100485265255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09029009379446507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04072999581694603 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07198890671133995 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09079999290406704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17771986313164234 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049748923629522324 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016659963876008987 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09035994298756123 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9798270184546709 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06874906830489635 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15769898891448975 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03900006413459778 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09342888370156288 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6054779514670372 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091990068554878 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1906389370560646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900999091565609 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1402590423822403 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3773581702262163 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07234979420900345 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4962580278515816 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7473339103162289 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03651995211839676 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.151062013581395 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09071989916265011 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1802800688892603 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050279777497053146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08836900815367699 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0398498959839344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07184990681707859 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09108008816838264 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17853896133601665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04937010817229748 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01597008667886257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09246892295777798 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9859369602054358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0898498110473156 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17691985704004765 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03848993219435215 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202980436384678 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6018979474902153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09009893983602524 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19173999316990376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967891335487366 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1404699869453907 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.376909039914608 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07171998731791973 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4950491711497307 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7420151270925999 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0363900326192379 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1444831509143114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08503999561071396 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11218991130590439 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.481428863480687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07784017361700535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16998895443975925 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05759997293353081 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08828914724290371 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040220096707344055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07114000618457794 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0755090732127428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162788899615407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049690017476677895 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015799887478351593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14354893937706947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03356998786330223 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.07193598523736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610899344086647 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04421989433467388 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09230989962816238 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20459899678826332 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902500469237566 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19323895685374737 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08845003321766853 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11346000246703625 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2601190935820341 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09384914301335812 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40039815939962864 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2512458488345146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04205992445349693 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6466549132019281 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07586902938783169 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16356888227164745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04915008321404457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14212890528142452 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03982009366154671 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07106992416083813 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444899529218674 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16031903214752674 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048789894208312035 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01600012183189392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14347885735332966 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0543260723352432 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344991900026798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16000005416572094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04449905827641487 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920100137591362 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21239882335066795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09058997966349125 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1923888921737671 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884002454578876 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11398899368941784 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2608590293675661 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09476905688643456 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4018479958176613 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2623260263353586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04211999475955963 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.65937514975667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1389891840517521 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16677891835570335 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.704511189833283 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07751001976430416 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16931886784732342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04963995888829231 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14177989214658737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04067993722856045 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07126899436116219 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07564015686511993 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234000213444233 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04918896593153477 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015910016372799873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523690927773714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03963988274335861 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2291159946471453 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453979924321175 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16181915998458862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056240009143948555 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09292992763221264 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20545907318592072 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037996642291546 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21257903426885605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233900345861912 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11207000352442265 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22113998420536518 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14383881352841854 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.410689041018486 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.287825871258974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05452986806631088 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7095550429075956 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07670000195503235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1648899633437395 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04960992373526096 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25085918605327606 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07118913345038891 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07647997699677944 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1638901885598898 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048588961362838745 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01628999598324299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521788701415062 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2787659652531147 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466995157301426 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16148993745446205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05659903399646282 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09230012074112892 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23677898570895195 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10205013677477837 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23707887157797813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918903768062592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11191004887223244 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22028014063835144 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1420199405401945 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4070289433002472 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3676658272743225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05370005965232849 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7877540085464716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24757999926805496 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2737189643085003 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.381899118423462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09101000614464283 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18246006220579147 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879012703895569 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2507301978766918 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03864895552396774 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07136003114283085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515004836022854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16036909073591232 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871003329753876 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015230150893330574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2519190311431885 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2839960400015116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400987669825554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16214000061154366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048108864575624466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2504989970475435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0388899352401495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07050996646285057 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07378007285296917 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15872903168201447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048140063881874084 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014940043911337852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25137909688055515 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2568661477416754 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5842420291155577 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455912418663502 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16012904234230518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2503891009837389 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869994543492794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0720699317753315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07401895709335804 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15865894965827465 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834984429180622 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015199882909655571 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510391641408205 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2581159826368093 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16258913092315197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05695992149412632 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09224889799952507 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1955397892743349 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08838996291160583 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20925910212099552 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08876016363501549 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1046089455485344 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20748889073729515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14301994815468788 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39593898691236973 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2517049908638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05329004488885403 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6785738989710808 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07637985982000828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16392883844673634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25039887987077236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038830097764730453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07008993998169899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15944894403219223 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048179877921938896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516091335564852 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2625858653336763 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.2948059272021055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503991946578026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215001232922077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792981781065464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4691679496318102 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039590056985616684 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07089902646839619 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480010390281677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16163010150194168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05521904677152634 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015519792214035988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25211903266608715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07449998520314693 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5836150851100683 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442990317940712 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039889305830002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047980109229683876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4681791178882122 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038539059460163116 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0707400031387806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07425015792250633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15938910655677319 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048759859055280685 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25178887881338596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07486995309591293 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5668550040572882 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419893518090248 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15916908159852028 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795007407665253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4688780754804611 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03881985321640968 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07077003829181194 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372908294200897 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15880889259278774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04871003329753876 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014980090782046318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25098887272179127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07421988993883133 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5653150621801615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24787988513708115 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32737920992076397 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.210401955991983 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545994594693184 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16294000670313835 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048839021474123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46883895993232727 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039139995351433754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07144897244870663 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515004836022854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612701453268528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048378948122262955 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2517891116440296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07554003968834877 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5756450593471527 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415981963276863 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609991304576397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0563599169254303 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122001938521862 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20075892098248005 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08840998634696007 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21105888299643993 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09648897685110569 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1117901410907507 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2185299526900053 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1426690723747015 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4059190396219492 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2731559108942747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05412008613348007 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6919751651585102 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07495004683732986 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16165990382432938 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047770095989108086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46862801536917686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0403400044888258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07130997255444527 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16036001034080982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2515988890081644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07540895603597164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.575895119458437 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354002445936203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16021006740629673 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056199030950665474 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09475997649133205 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2023091074079275 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935993537306786 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20958902314305305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08721998892724514 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11253892444074154 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2184188924729824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14204904437065125 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4055080935359001 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2715260963886976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053870026022195816 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6886249650269747 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0764590222388506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16339891590178013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3602390643209219 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038668978959321976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07171998731791973 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552001625299454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1605090219527483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04784995689988136 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015379860997200012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.253019155934453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06481003947556019 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4587549958378077 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532886229455471 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667090691626072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056549906730651855 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09113992564380169 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20338897593319416 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08852919563651085 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2104700542986393 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08792895823717117 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11224998161196709 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21873903460800648 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14181993901729584 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4055390600115061 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2652960140258074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0541189219802618 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6893339343369007 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2511190250515938 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3400691784918308 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.232456959784031 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14018896035850048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23178895935416222 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05695014260709286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36029890179634094 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039428938180208206 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07112999446690083 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423991337418556 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15868991613388062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04777987487614155 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14351983554661274 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06381911225616932 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4248949009925127 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07448019459843636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067991964519024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053249066695570946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09270990267395973 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19892887212336063 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986983448266983 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19074883311986923 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08837995119392872 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11383998207747936 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25971909053623676 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09357905946671963 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3984780050814152 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.239116070792079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04171999171376228 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6419850289821625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10087015107274055 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1874789595603943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25161891244351864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0395698007196188 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07076887413859367 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15768990851938725 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0478189904242754 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015220139175653458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14340016059577465 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05196896381676197 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2475249823182821 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733700580894947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1584598794579506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0438089482486248 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970010094344616 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1974890474230051 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08903001435101032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18984894268214703 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08808006532490253 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11416897177696228 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2630490344017744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09459909051656723 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40290807373821735 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2371859047561884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04160008393228054 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6278750263154507 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08470891043543816 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17212890088558197 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19614910706877708 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039010075852274895 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07080007344484329 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396982982754707 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15891878865659237 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015179859474301338 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1432490535080433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044869957491755486 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1699860915541649 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344898767769337 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15882891602814198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04433002322912216 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09133992716670036 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19740918651223183 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09131012484431267 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19036000594496727 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08824886754155159 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11395011097192764 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2595291007310152 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09441003203392029 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3995590377599001 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2397458776831627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041689956560730934 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.632455037906766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25996891781687737 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3856578841805458 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.341849014163017 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23666908964514732 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39471895433962345 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10679895058274269 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1990289893001318 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03999006003141403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07127993740141392 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09172898717224598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1774888951331377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049640191718935966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015750061720609665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08984003216028214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04693889059126377 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.425465103238821 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09145005606114864 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1777301076799631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038489000871777534 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09182002395391464 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6015480030328035 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09001977741718292 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18976000137627125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897690188139677 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14045997522771358 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3774289507418871 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07159006781876087 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49616885371506214 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7385450191795826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03610900603234768 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1403131540864706 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17493916675448418 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2916289959102869 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07766997441649437 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1444290392100811 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07174001075327396 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09112898260354996 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18052896484732628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015880214050412178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08971896022558212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04109996370971203 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2527061626315117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17799902707338333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03867992199957371 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248009882867336 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6018378771841526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09037903510034084 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19007991068065166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08989893831312656 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1403100322932005 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37723896093666553 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07114000618457794 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49503915943205357 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7400740180164576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03563007339835167 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.142653102055192 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12595881707966328 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2429590094834566 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07837009616196156 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14492892660200596 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042987711727619 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0709898304194212 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09104004129767418 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17698900774121284 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049140071496367455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015960074961185455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08995900861918926 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04205014556646347 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.195605844259262 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09015901014208794 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17759879119694233 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03893021494150162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252014569938183 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6023780442774296 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08975900709629059 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19034999422729015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14042016118764877 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3772289492189884 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07179006934165955 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4959080833941698 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7410239670425653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03587990067899227 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1434929221868515 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.535716079175472 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09560002945363522 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02559996210038662 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11844001710414886 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.98529782705009 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.2072989009320736 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.0513899140059948 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.043899985030293465 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02569984644651413 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03681983798742294 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2538489643484354 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07208995521068573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0698890071362257 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585891004651785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051870010793209076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09033014066517353 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040489016100764275 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07205992005765438 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09161001071333885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17851893790066242 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05019013769924641 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016600126400589943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09047985076904297 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9962569456547499 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0654000323265791 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15198998153209686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03921007737517357 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09239907376468182 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6049280054867268 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980999700725079 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19151880405843258 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909008465707302 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14060898683965206 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37702801637351513 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0718599185347557 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49586803652346134 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7447241116315126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03637978807091713 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.140304073691368 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09063002653419971 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17957016825675964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04976987838745117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08861906826496124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07158005610108376 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09092898108065128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17689913511276245 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04903995431959629 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0900391023606062 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9801068808883429 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08950009942054749 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17771986313164234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03895885311067104 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09282003156840801 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6045480258762836 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069917723536491 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19124988466501236 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08863909170031548 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14015985652804375 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37654885090887547 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0713001936674118 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49450783990323544 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7435941845178604 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03647012636065483 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1535931155085564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08537014946341515 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1126900315284729 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.489987950772047 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07844995707273483 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16966019757092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048869987949728966 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08711987175047398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04036002792418003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07163011468946934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07499987259507179 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16132998280227184 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979991354048252 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016159843653440475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14369911514222622 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03347988240420818 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0624269489198923 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07367902435362339 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16055908054113388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0445100013166666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09251898154616356 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19794004037976265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09203911758959293 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19252998754382133 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08868006989359856 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11301995255053043 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25939918123185635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09427987970411777 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40058884769678116 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.264215912669897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04229997284710407 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.661194022744894 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07617007941007614 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16418006271123886 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049249036237597466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1425999216735363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03989017568528652 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07134885527193546 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16027013771235943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049519119784235954 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016169855371117592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14377012848854065 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0566671844571829 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343897596001625 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15901890583336353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04461989738047123 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09249895811080933 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19879010505974293 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048916399478912 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19231997430324554 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08936901576817036 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11387001723051071 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26110908947885036 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0947299413383007 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4018389154225588 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.250085886567831 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0421090517193079 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6455138102173805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1397200394421816 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16754004172980785 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.705961957573891 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07724994793534279 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16975915059447289 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0491999089717865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14215894043445587 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04021008498966694 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07142988033592701 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07525994442403316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227900050580502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049369875341653824 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01613004133105278 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25238911621272564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039350008592009544 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2311860918998718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413001731038094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16085989773273468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05630007945001125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09203911758959293 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20393007434904575 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09096995927393436 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21395878866314888 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09019998833537102 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11194916442036629 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21922891028225422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14221994206309319 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4070289433002472 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2801450211554766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05405000410974026 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6999738290905952 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07682992145419121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1652589999139309 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050449976697564125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2506489399820566 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03999006003141403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07131998427212238 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07590907625854015 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16209902241826057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049140071496367455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25221891701221466 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2849560007452965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455889135599136 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16303895972669125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05652988329529762 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09140907786786556 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20379992201924324 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906290952116251 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21210010163486004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897599384188652 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11217012070119381 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2197090070694685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1420800108462572 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40777912363409996 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.278335927054286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05414988845586777 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7014341428875923 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24828920140862465 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2803490497171879 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.303389091044664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07807021029293537 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16988907009363174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05045998841524124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510589547455311 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04021008498966694 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07187016308307648 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07515912875533104 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16200891695916653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04946999251842499 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016010133549571037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25210902094841003 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.283896155655384 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458915933966637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16166898421943188 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04994985647499561 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25118887424468994 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04276004619896412 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07110019214451313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07599988020956516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16256887465715408 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04974007606506348 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015980098396539688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25235884822905064 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2807559687644243 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.609741874039173 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07539987564086914 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1647300086915493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0500301830470562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25071902200579643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04008994437754154 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07211882621049881 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07553002797067165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16251998022198677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049198977649211884 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015739817172288895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25180913507938385 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2763950508087873 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535982877016068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16543990932404995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05664885975420475 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0923799816519022 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19567902199923992 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09115994907915592 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2141590230166912 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071989916265011 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10512000881135464 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2107289619743824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14279899187386036 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3996780142188072 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.269086031243205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05355989560484886 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.703744987025857 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07748999632894993 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16689905896782875 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05047000013291836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2508389297872782 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03982009366154671 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07116887718439102 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07620011456310749 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16335980035364628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050099100917577744 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016219913959503174 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2514990046620369 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2813450302928686 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.358374979346991 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0766098964959383 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16657914966344833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049209920689463615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46914885751903057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07138005457818508 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07539894431829453 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630689948797226 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05025998689234257 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015789875760674477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2521390561014414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07489998824894428 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5919150318950415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0752799678593874 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623597927391529 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04946999251842499 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46877795830368996 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399700365960598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07070996798574924 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1718688290566206 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 2.8181408997625113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05518016405403614 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018159858882427216 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2605789341032505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07657008245587349 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 4.276725929230452 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07875985465943813 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1691700890660286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047659967094659805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47191884368658066 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040079932659864426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0918388832360506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07915007881820202 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16731000505387783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875892773270607 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015389872714877129 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.253899022936821 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07619010284543037 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6331849619746208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2496491651982069 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3402489237487316 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 8.011914091184735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07865997031331062 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17640995793044567 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04924996756017208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4711579531431198 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04100007936358452 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0768899917602539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07763016037642956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16588997095823288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2539500128477812 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07811910472810268 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.612864900380373 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07653003558516502 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16478006727993488 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058398814871907234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10011997073888779 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23565883748233318 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09277998469769955 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23925909772515297 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248894639313221 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11496990919113159 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24203909561038017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14358991757035255 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4341290332376957 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3820461463183165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05604908801615238 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.8157840240746737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07665995508432388 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16565993428230286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048839021474123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4685788881033659 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038700178265571594 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07407902739942074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07622991688549519 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1654401421546936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04798895679414272 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015560071915388107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2520889975130558 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07645995356142521 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.588835148140788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07641990669071674 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1635989174246788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05837017670273781 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09085005149245262 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20587886683642864 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126006625592709 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21167006343603134 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0883189495652914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11323997750878334 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21921982988715172 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14611007645726204 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4147288855165243 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2900361325591803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057238852605223656 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7207940109074116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07602013647556305 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16557006165385246 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05152914673089981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36086910404264927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038499943912029266 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07626996375620365 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472001016139984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1603288110345602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25245919823646545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06599980406463146 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4701050240546465 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07460988126695156 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16297982074320316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05691009573638439 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21256902255117893 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884300097823143 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21462887525558472 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102001786231995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11240900494158268 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21994905546307564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1423289068043232 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4089081194251776 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2911860831081867 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05504000000655651 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7150649800896645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25212904438376427 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3416989929974079 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.488045867532492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14094007201492786 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23574009537696838 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05629006773233414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3631790168583393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03999006003141403 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07364992052316666 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535004988312721 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16457983292639256 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015379860997200012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14362996444106102 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06567989476025105 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4460759703069925 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07563014514744282 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622689887881279 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04464993253350258 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09157904423773289 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20175008103251457 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09031989611685276 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19511906430125237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045982733368874 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11526001617312431 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26071909815073013 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09575998410582542 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40342891588807106 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2563660275191069 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04360009916126728 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.65826384909451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10157003998756409 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.190389109775424 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04916987381875515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2539691049605608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03943988122045994 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07330020889639854 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08252914994955063 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17151888459920883 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913005977869034 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016150064766407013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14578900299966335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05436013452708721 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2848658952862024 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477891631424427 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16570906154811382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04679011180996895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09229895658791065 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20020012743771076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912791583687067 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1943900715559721 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09278999641537666 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11555897071957588 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26272889226675034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09609991684556007 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4080988001078367 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2673460878431797 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043899985030293465 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6776940319687128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0808490440249443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17441902309656143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04966999404132366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1978192012757063 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040479935705661774 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07229996845126152 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445900700986385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161309028044343 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04905997775495052 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015829922631382942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14549912884831429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04606996662914753 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.195225864648819 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428904063999653 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16504921950399876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04521990194916725 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09453995153307915 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20026997663080692 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09144889190793037 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19950000569224358 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09273900650441647 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11424999684095383 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26239920407533646 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09437999688088894 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4068890120834112 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.274316105991602 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04222895950078964 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6793839167803526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2609190996736288 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.38746907375752926 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.566228138282895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23657013662159443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3975280560553074 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1070098951458931 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19854912534356117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04026992246508598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07421011105179787 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09196996688842773 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18209917470812798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049969879910349846 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016029924154281616 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09039905853569508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0473198015242815 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4480650424957275 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09059906005859375 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18105912022292614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04173978231847286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256997145712376 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6076879799365997 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09532994590699673 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.195828964933753 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09046983905136585 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14236895367503166 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.38103898987174034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07390882819890976 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5029779858887196 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7738142050802708 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036579789593815804 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.189102815464139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17456989735364914 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29179896228015423 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07751886732876301 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14615990221500397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040430109947919846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07290905341506004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09175017476081848 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1816397998481989 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922901280224323 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016020145267248154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09285006672143936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042289961129426956 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2527259532362223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09107985533773899 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17934013158082962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0406499020755291 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09283912368118763 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6042178720235825 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09072991088032722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19198888912796974 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0994501169770956 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1435300800949335 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3817579708993435 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0733700580894947 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5037279333919287 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7648639623075724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03681983798742294 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1744021214544773 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12715021148324013 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24314899928867817 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.078519806265831 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1438299659639597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04081893712282181 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07434003055095673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09180000051856041 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1806889194995165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09126006625592709 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04180893301963806 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2009360361844301 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09085005149245262 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1797298900783062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03920914605259895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09315996430814266 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6061880849301815 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09008985944092274 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19218889065086842 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09214901365339756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1409701071679592 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37873885594308376 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07453002035617828 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5026489961892366 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.762464875355363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03685988485813141 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.169982995837927 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.674024932086468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09587989188730717 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.027189962565898895 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.17185905016958714 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 65.51439594477415 ms for forwarding
--------------------
No. 2
<class 'diffusers.models.embeddings.Timesteps'> take 0.250399112701416 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.055108917877078056 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04898989573121071 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03817002288997173 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.038190046325325966 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2834091428667307 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.14898902736604214 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09425892494618893 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2071191556751728 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05113985389471054 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09870994836091995 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04053907468914986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07573002949357033 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08554989472031593 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17333915457129478 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854006692767143 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01729000359773636 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08428981527686119 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0419071186333895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16287993639707565 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11674989946186543 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.611587893217802 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920100137591362 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23239897564053535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09084888733923435 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1316498965024948 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37644896656274796 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06928015500307083 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5002690013498068 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.845414051786065 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03716000355780125 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.28125206194818 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0883699394762516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17943885177373886 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898989573121071 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08319015614688396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03986014053225517 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07324898615479469 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08755014277994633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17602997832000256 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0490888487547636 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015990110114216805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08471985347568989 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.977897085249424 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09223911911249161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2007491420954466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037789810448884964 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09653903543949127 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5607889033854008 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11199014261364937 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1981891691684723 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177997708320618 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12962915934622288 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3597091417759657 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06810901686549187 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47547812573611736 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.733133802190423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03727991133928299 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.162712858989835 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08126907050609589 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1101689413189888 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.7012079525738955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.079659977927804 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1748800277709961 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048039015382528305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08264998905360699 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03932998515665531 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07445993833243847 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07644016295671463 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16181007958948612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0484599731862545 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015670200809836388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13260985724627972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0353700015693903 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0557069908827543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453910075128078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16212905757129192 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04294002428650856 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09402912110090256 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1969600562006235 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032920934259892 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19130995497107506 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0888989306986332 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1077800989151001 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25030900724232197 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08987006731331348 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38893893361091614 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2351458426564932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04418916068971157 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6484439838677645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18606986850500107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794984124600887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13187015429139137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03905990161001682 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0692389439791441 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07380987517535686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16091018915176392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048039015382528305 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1329299993813038 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0580969974398613 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348996587097645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16049901023507118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04306994378566742 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09480887092649937 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19164010882377625 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972012437880039 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1945490948855877 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08793012239038944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10610884055495262 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24765916168689728 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08856994099915028 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38446905091404915 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2287350837141275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040839891880750656 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.629853853955865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12758001685142517 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1548989675939083 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.6609108578413725 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07993984036147594 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1738099381327629 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048168934881687164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1296699047088623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939005546271801 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0726690050214529 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08415011689066887 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1722599845379591 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04853890277445316 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01579010859131813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22834003902971745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039278995245695114 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2121559120714664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07547019049525261 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16397004947066307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05286908708512783 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20861905068159103 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09185983799397945 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21202885545790195 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08917017839848995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10446994565427303 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20997016690671444 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13197888620197773 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3897279966622591 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2669560965150595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0513298436999321 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.687905052676797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0759391114115715 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16496912576258183 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834006540477276 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22638915106654167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03884010948240757 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06976001895964146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07543014362454414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16437005251646042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04763016477227211 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015449943020939827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2278289757668972 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2300859671086073 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503991946578026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644589938223362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054170144721865654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09262911044061184 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2191700041294098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09229988791048527 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23824884556233883 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09009987115859985 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10391906835138798 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21063885651528835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13187993317842484 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3902690950781107 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3160749804228544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05118991248309612 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7421739175915718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22460916079580784 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25304919108748436 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.239359965547919 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07831002585589886 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17164903692901134 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05091005004942417 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22705900482833385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040289945900440216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06915000267326832 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0902889296412468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1949588768184185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05258014425635338 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01619011163711548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22780895233154297 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.28611596301198 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467903196811676 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162449199706316 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049789901822805405 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22645900025963783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04019984044134617 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06856000982224941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07522013038396835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16291998326778412 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049640191718935966 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01579010859131813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22814911790192127 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.230065943673253 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.563351998105645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07685995660722256 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16848999075591564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050690025091171265 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22703898139297962 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040830112993717194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06909901276230812 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07513980381190777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16280007548630238 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04986906424164772 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583993434906006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22756005637347698 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2385649606585503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16718008555471897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05436898209154606 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09366008453071117 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19777892157435417 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0916698481887579 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23943884298205376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09087007492780685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09805988520383835 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20308000966906548 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13225898146629333 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3832781221717596 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.292316010221839 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05054008215665817 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7181450966745615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09202910587191582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1938489731401205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.056560151278972626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22990908473730087 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04082010127604008 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06807898171246052 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08407002314925194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17309002578258514 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05010003224015236 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01622992567718029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22722012363374233 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2861259747296572 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.353504860773683 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07603992708027363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1658399123698473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04987884312868118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42100902646780014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06946991197764874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748499296605587 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634398940950632 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015478814020752907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22833002731204033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07241894491016865 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5196548774838448 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07904996164143085 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17853989265859127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.058320118114352226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42388890869915485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03979983739554882 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06805895827710629 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1620498951524496 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057339202612638474 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.019349856302142143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23152888752520084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07051997818052769 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5685649123042822 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16330904327332973 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049080001190304756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4224490839987993 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0405388418585062 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06849993951618671 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746601726859808 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630689948797226 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0491999089717865 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610000617802143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2276790328323841 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06931996904313564 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5279650688171387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2245199866592884 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.321089057251811 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.102583905681968 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07656984962522984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16653002239763737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049558933824300766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41903904639184475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03985990770161152 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0717691145837307 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754399225115776 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16375980339944363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048479996621608734 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22717006504535675 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07050903514027596 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5146650839596987 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08864002302289009 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19550882279872894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05631009116768837 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0931799877434969 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20021898671984673 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09933905676007271 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21015014499425888 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10856892913579941 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10361988097429276 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21184911020100117 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13029994443058968 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3903789911419153 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2995160650461912 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05123903974890709 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7623440362513065 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07748999632894993 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16693002544343472 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04961015656590462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4176290240138769 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04009995609521866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06783916614949703 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0758699607104063 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1642601564526558 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04942994564771652 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01625996083021164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22709998302161694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0727588776499033 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5201850328594446 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414980791509151 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631299965083599 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053108902648091316 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09253993630409241 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19953888840973377 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10199006646871567 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22673886269330978 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08781882934272289 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10397005826234818 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2091799397021532 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1469890121370554 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4037991166114807 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3010059483349323 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051110051572322845 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.719204941764474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07580011151731014 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16478984616696835 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049170106649398804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3220790531486273 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03890995867550373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07055001333355904 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07441989146173 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16144919209182262 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04784995689988136 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015750061720609665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22682896815240383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0588900875300169 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3973959721624851 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532909512519836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16370904631912708 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053590163588523865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0921301543712616 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22258912213146687 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09879888966679573 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20881020464003086 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015994146466255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10481011122465134 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2100188285112381 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13178004883229733 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3919790033251047 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.30172586068511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05389912985265255 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.732473960146308 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.226929085329175 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31084916554391384 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.1796870585531 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13839895837008953 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2295889426022768 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05298992618918419 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32266881316900253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039590056985616684 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06874999962747097 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538986392319202 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16247015446424484 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048249028623104095 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015550060197710991 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13042008504271507 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05976995453238487 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.378895016387105 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08868006989359856 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18718000501394272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04299893043935299 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1897788606584072 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08920999243855476 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1996189821511507 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10561011731624603 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1040289644151926 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24500908330082893 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08734897710382938 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.379408011212945 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2472460512071848 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04020007327198982 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6713750082999468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09203888475894928 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18095900304615498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04918989725410938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2253891434520483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039529986679553986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06859004497528076 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07358985021710396 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16040983609855175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843994975090027 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.029749935492873192 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13190903700888157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04867999814450741 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2288561556488276 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08926913142204285 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19270903430879116 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04270998761057854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09301910176873207 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18977001309394836 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08993898518383503 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1905299723148346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09859004057943821 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10380987077951431 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24605891667306423 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08795992471277714 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3842189908027649 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2351959012448788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04002894274890423 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6662341076880693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07840991020202637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16644992865622044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048309098929166794 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17801998183131218 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039049889892339706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789993494749069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07331999950110912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15897001139819622 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04809885285794735 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13076001778244972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04284013994038105 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.135225873440504 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07902015931904316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1759899314492941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04663015715777874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09709014557301998 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18860911950469017 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09018019773066044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19993912428617477 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09044003672897816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11004903353750706 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2566289622336626 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08767982944846153 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3946979995816946 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2531860265880823 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039449892938137054 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6760248690843582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23480900563299656 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3525090869516134 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.33611998334527 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22005988284945488 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.375028932467103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10325992479920387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17848890274763107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396000687032938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06847991608083248 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09108008816838264 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1894589513540268 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057709868997335434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.017700018361210823 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08254987187683582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044179847463965416 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4029860030859709 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08448888547718525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17271912656724453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037419842556118965 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09372993372380733 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5515280645340681 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0921089667826891 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20687980577349663 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09211013093590736 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12726010754704475 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3550988622009754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06758002564311028 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4715379327535629 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6938738990575075 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03531016409397125 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0904529374092817 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16263918951153755 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27675903402268887 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07359008304774761 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13111019507050514 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04076980985701084 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06851018406450748 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08326000533998013 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17104018479585648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04933006130158901 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016120029613375664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08303998038172722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039400067180395126 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1927159503102303 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08464884012937546 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1769689843058586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041930004954338074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11085881851613522 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5509280599653721 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09057018905878067 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22453884594142437 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09019998833537102 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12727989815175533 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35528908483684063 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07665995508432388 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4811978433281183 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7408039420843124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03454997204244137 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1549719385802746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12263981625437737 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23789890110492706 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13024010695517063 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040218932554125786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06801006384193897 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08314009755849838 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17085904255509377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016069971024990082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08241995237767696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03903987817466259 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1578660923987627 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08320994675159454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1715801190584898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03732996992766857 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09257998317480087 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5514980293810368 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09685009717941284 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19012903794646263 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08858996443450451 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1265290193259716 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3538290038704872 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06737001240253448 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46855793334543705 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.681263791397214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034769997000694275 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.075033960863948 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.305186035111547 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08445908315479755 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025169923901557922 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15790993347764015 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.880616936832666 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19536004401743412 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.0505989883095026 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04208995960652828 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02484000287950039 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03681005910038948 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24783890694379807 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06915000267326832 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06827013567090034 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15526008792221546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049629947170615196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08192891255021095 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039419857785105705 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06810016930103302 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08366885595023632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17091911286115646 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016279984265565872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08246907964348793 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9412271901965141 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06626010872423649 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16371998935937881 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04233908839523792 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10513002052903175 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5499378312379122 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22212904877960682 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08926889859139919 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12687989510595798 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35350886173546314 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06896001286804676 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4703388549387455 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7160549759864807 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03426894545555115 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.129042986780405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08356990292668343 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17221993766725063 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04974915646016598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08104019798338413 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04063011147081852 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06717001087963581 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.084729865193367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17222878523170948 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04946999251842499 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01634005457162857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08220900781452656 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9486870840191841 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08378992788493633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17023994587361813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03627990372478962 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09188009425997734 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5470081232488155 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08832011371850967 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1887190155684948 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08824001997709274 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1262398436665535 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.351418973878026 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06673997268080711 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46269805170595646 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.645105192437768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033880118280649185 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.033843891695142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07692002691328526 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10273000225424767 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.2592001631855965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07641990669071674 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16722897998988628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049080001190304756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07980992086231709 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03967992961406708 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06675906479358673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392000406980515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16046990640461445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04782993346452713 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015390105545520782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13192999176681042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031670089811086655 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0286869946867228 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07382011972367764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15888898633420467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042190076783299446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09011896327137947 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18938002176582813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08876901119947433 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18950016237795353 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0872989185154438 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303989984095097 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24320906959474087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08680997416377068 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37480914033949375 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1973560322076082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039449892938137054 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5834339428693056 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0758098904043436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622599083930254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.129269203171134 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039040111005306244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06732996553182602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07326016202569008 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15718908980488777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1293690875172615 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.010996988043189 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754999928176403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16937893815338612 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04566996358335018 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09331014007329941 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18977909348905087 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08854013867676258 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20996900275349617 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884002454578876 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10393001139163971 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24435995146632195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08723908104002476 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3771691117435694 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2295960914343596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038929982110857964 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6392951365560293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12510898523032665 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15146913938224316 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.515962140634656 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07608998566865921 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1653488725423813 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04874984733760357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12761983089148998 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03928900696337223 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757001392543316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480010390281677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589090097695589 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01511978916823864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22639892995357513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036599813029170036 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1676358990371227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07424992509186268 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16035884618759155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052350107580423355 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09075994603335857 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20155892707407475 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08857902139425278 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20773988217115402 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0900188460946083 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10961992666125298 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.23002899251878262 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13007991947233677 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40771905332803726 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2688159476965666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049948925152421 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6782940365374088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07507018744945526 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1700699795037508 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04823901690542698 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22403011098504066 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038828933611512184 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06768014281988144 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444992661476135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15945895574986935 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22564898245036602 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2116159778088331 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07311999797821045 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15725987032055855 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05223904736340046 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967006579041481 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19811908714473248 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08791009895503521 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20530889742076397 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08883001282811165 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10341894812881947 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22495887242257595 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13632886111736298 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4090680740773678 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.256435876712203 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04988000728189945 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6624950803816319 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22127898409962654 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24750898592174053 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.068360060453415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07701897993683815 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16564899124205112 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0502199400216341 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22728904150426388 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03890995867550373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670100562274456 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438007742166519 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15913904644548893 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04928000271320343 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595006324350834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2256990410387516 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2257162015885115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445900700986385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15964917838573456 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048419926315546036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22382894530892372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03846990875899792 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06662984378635883 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074049923568964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15981891192495823 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01516006886959076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22549903951585293 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2001460418105125 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.468851860612631 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749488826841116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608789898455143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04838989116251469 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22348901256918907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03830995410680771 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742007099092007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485016249120235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15958910807967186 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015529803931713104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2249288372695446 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2001360300928354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07526995614171028 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630089245736599 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052419956773519516 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065913036465645 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19215000793337822 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08949893526732922 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20627002231776714 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08907984010875225 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09755999781191349 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2039091195911169 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1323102042078972 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3834690432995558 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.236695796251297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049179187044501305 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6475438605993986 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07573980838060379 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613700296729803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048459041863679886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2240501344203949 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03909901715815067 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06727012805640697 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739898532629013 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15855999663472176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731002263724804 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0153400469571352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2246189396828413 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2013160157948732 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.1437658946961164 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07464992813766003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16200891695916653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41727907955646515 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915000706911087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06813905201852322 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325992919504642 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15801005065441132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015189871191978455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22528995759785175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06839912384748459 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4850751031190157 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08896016515791416 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19065896049141884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773982800543308 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41642901487648487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040010083466768265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06725895218551159 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08662999607622623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1817301381379366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05592918023467064 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248389646410942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06786989979445934 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5459249261766672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07317005656659603 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15783892013132572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04664994776248932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41705905459821224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038830097764730453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06621889770030975 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07383991032838821 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15727011486887932 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04739896394312382 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22490997798740864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06750901229679585 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.469374867156148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2210990060120821 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2995289396494627 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.9629840068519115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435912266373634 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16139913350343704 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047369860112667084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41696918196976185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04599918611347675 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06713997572660446 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457006722688675 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15899003483355045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04751002416014671 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501990482211113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22601895034313202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07042987272143364 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4972551725804806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0886889174580574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1857189927250147 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05254009738564491 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09079999290406704 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19998988136649132 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08786888793110847 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20628003403544426 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.087750144302845 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10277004912495613 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20662881433963776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1296598929911852 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3803588915616274 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.229046145454049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049718888476490974 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6655339859426022 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434003055095673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16023009084165096 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761992022395134 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41631911881268024 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03909994848072529 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06743893027305603 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15823985449969769 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794914275407791 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015039928257465363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2254799474030733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06915000267326832 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4825749676674604 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301010191440582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15851901844143867 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052419956773519516 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08963001891970634 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20827888511121273 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08876994252204895 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23256917484104633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08736900053918362 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10449998080730438 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20786002278327942 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12934906408190727 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3812990617007017 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.270356122404337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05000014789402485 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6757950652390718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09072013199329376 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1894200686365366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053629977628588676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32013910822570324 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038929982110857964 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06789993494749069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492909207940102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602889969944954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04818988963961601 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015380093827843666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22580893710255623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05793990567326546 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4108859468251467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07411884143948555 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15944894403219223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052140094339847565 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134993888437748 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19844993948936462 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10261894203722477 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2252901904284954 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08813990280032158 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10394002310931683 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20734895952045918 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13192999176681042 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3938991576433182 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2818558607250452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05350913852453232 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6985440161079168 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22886903025209904 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32024900428950787 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.968517115339637 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12943893671035767 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.218698987737298 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05258992314338684 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3212189767509699 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038590049371123314 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06696907803416252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420009933412075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15956000424921513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04732911475002766 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1296699047088623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05852989852428436 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3422260526567698 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327995263040066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1572899054735899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04170997999608517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029894135892391 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18905987963080406 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08870987221598625 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18872995860874653 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08774991147220135 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1042000949382782 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2439690288156271 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08696992881596088 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3749788738787174 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1988861951977015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03894884139299393 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.581875141710043 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09226007387042046 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.179970171302557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06356905214488506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2269200049340725 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04682992585003376 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0742599368095398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07307901978492737 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16046897508203983 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12977910228073597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047900015488266945 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.244935905560851 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07260986603796482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15733018517494202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041839899495244026 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10433979332447052 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19450904801487923 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884980343282223 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1885988749563694 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08778995834290981 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10327878408133984 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24284911341965199 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08668913505971432 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37397793494164944 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.21938600204885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03943988122045994 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6022049821913242 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07791910320520401 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1647390890866518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04832982085645199 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1768690999597311 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038460129871964455 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06602006033062935 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073558883741498 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15768897719681263 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047629931941628456 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12957886792719364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04254002124071121 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1248160153627396 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338006980717182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15835021622478962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13199006207287312 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18872879445552826 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0882099848240614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18989015370607376 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0880090519785881 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10353978723287582 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24329009465873241 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0874099787324667 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3755791112780571 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2000559363514185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03899005241692066 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6753650270402431 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23436895571649075 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35143783316016197 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.13562998175621 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21881889551877975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37237885408103466 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10253884829580784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17827004194259644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03962009213864803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728898733854294 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08490984328091145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16987998969852924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04805903881788254 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08209003135561943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043280189856886864 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3503460213541985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08330913260579109 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16897893510758877 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036510173231363297 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09095901623368263 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5492591299116611 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09555905126035213 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18830993212759495 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798018097877502 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12672017328441143 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35138893872499466 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653997115790844 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4632091149687767 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6542240045964718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033529940992593765 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.039523096755147 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16259890981018543 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2758190967142582 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07244991138577461 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1293090172111988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655999459326267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08323905058205128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.168628990650177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048040179535746574 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015179859474301338 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0815889798104763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03806012682616711 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1666158679872751 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0830900389701128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16859988681972027 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0363900326192379 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913301482796669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5475780926644802 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08853990584611893 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18832902424037457 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08821999654173851 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.127479899674654 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35468884743750095 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655021570622921 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4750979132950306 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6540649812668562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033559976145625114 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0365731325000525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12301001697778702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23560901172459126 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07346016354858875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12932997196912766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039190053939819336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06677000783383846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08367002010345459 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16789906658232212 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879990592598915 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015500001609325409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08240994065999985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038828933611512184 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1295259464532137 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08334987796843052 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.168520025908947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03666989505290985 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08926005102694035 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5492581985890865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891401432454586 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18693902529776096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08817994967103004 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1268000341951847 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3513891715556383 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06633996963500977 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46204798854887486 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6404150519520044 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03451993688941002 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.024203073233366 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.970366954803467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08534989319741726 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.027319882065057755 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11043902486562729 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.67543413862586 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.1978101208806038 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04602898843586445 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.040499959141016006 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025360146537423134 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0366999302059412 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24210894480347633 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06903987377882004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06603007204830647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15292991884052753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900014027953148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.082019018009305 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903009928762913 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06822985596954823 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08372915908694267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1698690466582775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840991459786892 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01564016565680504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08185906335711479 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9321668185293674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.062270089983940125 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1469799317419529 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036529963836073875 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09195995517075062 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5505180452018976 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08954992517828941 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19067898392677307 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08880998939275742 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1261490397155285 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3515689168125391 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06814999505877495 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46535790897905827 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6541050281375647 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03363005816936493 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0339139737188816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08382019586861134 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16999896615743637 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04959991201758385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08048000745475292 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963988274335861 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06719003431499004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08328002877533436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16894889995455742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048590125516057014 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01553981564939022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08215988054871559 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9361270349472761 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0819498673081398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16755913384258747 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03631017170846462 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0910591334104538 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5454188212752342 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08815992623567581 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18931902013719082 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08810986764729023 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12604892253875732 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35158894024789333 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06615999154746532 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4620179533958435 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6410539392381907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034060096368193626 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0244631450623274 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07742992602288723 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10313000530004501 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.131260888651013 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0762399286031723 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16533001326024532 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07973890751600266 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03916001878678799 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08225999772548676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412908598780632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15914905816316605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05079014226794243 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295488327741623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03123003989458084 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0352770332247019 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731500331312418 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611888874322176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041930004954338074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09115994907915592 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18825894221663475 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08828006684780121 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18976908177137375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08705002255737782 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1031800638884306 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24141906760632992 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08700904436409473 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37256814539432526 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1936358641833067 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03896001726388931 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5839450061321259 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480895146727562 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16024895012378693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12822006829082966 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038119032979011536 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653997115790844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733998604118824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15731900930404663 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803994670510292 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12923008762300014 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0064069647341967 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0769798643887043 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16118003986775875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04155002534389496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09193900041282177 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1884999219328165 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08742883801460266 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1987398136407137 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08827005513012409 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1034298911690712 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24333898909389973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08714012801647186 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3748589660972357 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2080559972673655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039458973333239555 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5954941045492887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12487010098993778 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15159999020397663 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.47406286932528 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07611000910401344 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16543897800147533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840013571083546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12758909724652767 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03893999382853508 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06682006642222404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413886487483978 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15866896137595177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048220157623291016 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22555887699127197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03630993887782097 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.161445863544941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390906102955341 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587888691574335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05252007395029068 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081978350877762 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20001898519694805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09601912461221218 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2076299861073494 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0892390962690115 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10399008169770241 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.207540113478899 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13055000454187393 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38175913505256176 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.243756152689457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04958990029990673 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6484539955854416 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08336012251675129 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17104996368288994 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22384896874427795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038350000977516174 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06663892418146133 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07405993528664112 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15829014591872692 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04760897718369961 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01553981564939022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22503896616399288 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2092860415577888 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406994700431824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15934021212160587 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05205906927585602 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09075994603335857 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1968990545719862 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08776993490755558 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2058190293610096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08705002255737782 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10235002264380455 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20914897322654724 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1357488799840212 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39215805009007454 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.239126082509756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04992005415260792 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6451748088002205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22154906764626503 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24772877804934978 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.011809920892119 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579894736409187 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16534910537302494 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0485801137983799 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22375909611582756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03921007737517357 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0672598835080862 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07381010800600052 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15841005370020866 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.03310898318886757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.226279953494668 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.224716193974018 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07470999844372272 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588298473507166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04799012094736099 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22405898198485374 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039190053939819336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06691995076835155 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418007589876652 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15862006694078445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015049008652567863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2252201084047556 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1978058610111475 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4652720894664526 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07509998977184296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16096001490950584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048170099034905434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22364896722137928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03871996887028217 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06697908975183964 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427018135786057 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588999293744564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22494001314044 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.198377227410674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07547996938228607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16361987218260765 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0524700153619051 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082979522645473 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1908489502966404 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900999091565609 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2068390604108572 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08870987221598625 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09689899161458015 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20166905596852303 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.130500178784132 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37622894160449505 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2208570260554552 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04897988401353359 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.635165186598897 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581990212202072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1614987850189209 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048549845814704895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22413907572627068 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03876001574099064 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06633996963500977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15905918553471565 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015039928257465363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22448902018368244 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1982161086052656 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.124766914173961 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480010390281677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16144895926117897 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475000124424696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4167589358985424 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038479920476675034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06680004298686981 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342989556491375 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15829992480576038 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04768907092511654 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01480989158153534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22511882707476616 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06783008575439453 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4743050560355186 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07867999374866486 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1636589877307415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047320034354925156 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41610910557210445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03827991895377636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06856909021735191 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15909015201032162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0481591559946537 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22548995912075043 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06786012090742588 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4960048720240593 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369997911155224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15792902559041977 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046740053221583366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41615916416049004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03850995562970638 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06664893589913845 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442012429237366 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15900004655122757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04752911627292633 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22470997646450996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06809900514781475 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4685348141938448 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22143893875181675 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3042991738766432 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.896814003586769 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07478008046746254 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16105896793305874 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04746997728943825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4163288976997137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03881985321640968 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728013977408409 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07547996938228607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15980983152985573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047859037294983864 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014940043911337852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22552884183824062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06995000876486301 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.48248509503901 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390021346509457 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15885918401181698 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05249003879725933 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09063002653419971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1986988354474306 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0887399073690176 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.206278869882226 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08721998892724514 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10247994214296341 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20612007938325405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13045896776020527 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38158800452947617 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2286361306905746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04931003786623478 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6339949797838926 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754399225115776 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16127899289131165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41680806316435337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906991332769394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06706011481583118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506902329623699 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16014906577765942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04785996861755848 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522991806268692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22518890909850597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06904010660946369 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4811051078140736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07296889089047909 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571390312165022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0524700153619051 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030010551214218 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19884994253516197 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09839888662099838 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20750006660819054 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0872691161930561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10423990897834301 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20697899162769318 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1292999368160963 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37979893386363983 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2398760300129652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04973984323441982 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.643654191866517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07586018182337284 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16382988542318344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792003892362118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3204389940947294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03865011967718601 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06747012957930565 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535913027822971 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598589587956667 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797009751200676 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015360070392489433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2256990410387516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05856994539499283 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3775560073554516 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455912418663502 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039912588894367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05229003727436066 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19867997616529465 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08801906369626522 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2063398715108633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08813897147774696 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10425015352666378 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2076900564134121 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.129360007122159 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3798389807343483 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.229046145454049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05068979226052761 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6384939663112164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22641895338892937 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31063915230333805 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.776877937838435 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1293590757995844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2175190020352602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05269981920719147 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32098894007503986 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742891855537891 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364992052316666 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15709013678133488 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047069042921066284 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015360070392489433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295800320804119 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058270059525966644 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3365659397095442 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07262011058628559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.157439848408103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041670165956020355 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09066890925168991 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18872995860874653 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08791009895503521 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18954905681312084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08759996853768826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.103748869150877 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24420907720923424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08692010305821896 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3753490746021271 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1968459002673626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03970996476709843 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.580365002155304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09141000919044018 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1787289511412382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2245891373604536 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04675006493926048 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06729015149176121 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298891432583332 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567790750414133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015460187569260597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12973882257938385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048170099034905434 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1946861632168293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15919911675155163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04176003858447075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905599445104599 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1884691882878542 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08783000521361828 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18721888773143291 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08757016621530056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10338006541132927 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24228892289102077 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08681905455887318 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37302798591554165 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1916758958250284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03890995867550373 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5764450654387474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07730885408818722 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16357889398932457 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860991612076759 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17683906480669975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385199673473835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06683007813990116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369904778897762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567190047353506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01558009535074234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1297390554100275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04198984242975712 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1203058529645205 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07260008715093136 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15730899758636951 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04139007069170475 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010010398924351 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18789898604154587 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983002044260502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19035907462239265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08779997006058693 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10343990288674831 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24262885563075542 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08752895519137383 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3746978472918272 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2158961035311222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03902008756995201 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6008547972887754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23459992371499538 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3525889478623867 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.973291143774986 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2197690773755312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3727290313690901 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.1039199996739626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17753895372152328 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918004222214222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06703007966279984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08359993807971478 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16797007992863655 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048770103603601456 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015400117263197899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08175894618034363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043250154703855515 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.349576050415635 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08411891758441925 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1689388882368803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03671995364129543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09232014417648315 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5472081247717142 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08877902291715145 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19844993948936462 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08863909170031548 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13098004274070263 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35620899870991707 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06587989628314972 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4661688581109047 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6567949205636978 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03416999243199825 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0408430136740208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16092904843389988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2725590020418167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07278984412550926 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295190304517746 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03830995410680771 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655999459326267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08317898027598858 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16761897131800652 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04766997881233692 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08182995952665806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0381888821721077 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1632959358394146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0833999365568161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16795005649328232 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03621005453169346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09158998727798462 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5485881119966507 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08788006380200386 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18745893612504005 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08793012239038944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1260889694094658 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.34968904219567776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06600003689527512 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46001793816685677 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6473447903990746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033700140193104744 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0295840222388506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11912989430129528 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23122993297874928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07451907731592655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12949993833899498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039389822632074356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06700982339680195 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08354010060429573 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16910000704228878 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04828977398574352 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015138881281018257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08219992741942406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038579804822802544 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1260861065238714 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08278898894786835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16765901818871498 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03637978807091713 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902500469237566 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5500779952853918 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891189556568861 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18752994947135448 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08861883543431759 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12720981612801552 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3618490882217884 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06650993600487709 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47225900925695896 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6531250439584255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033419812098145485 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.034722827374935 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.9669280461967 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.086609972640872 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02508983016014099 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1099489163607359 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.9853769633919 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.18944917246699333 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04484015516936779 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03993883728981018 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02427981235086918 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.035490142181515694 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23649912327528 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06990996189415455 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06556999869644642 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15245890244841576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049170106649398804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08189002983272076 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03924011252820492 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06769993342459202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08427887223660946 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.170139130204916 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08233007974922657 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9304061532020569 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.062000006437301636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1470698043704033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03617000766098499 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09168894030153751 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5497680976986885 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927984163165092 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18830900080502033 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08928007446229458 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12623914517462254 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3508778754621744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06691995076835155 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4621779080480337 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.646983902901411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03337999805808067 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0235939882695675 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08215988054871559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16878009773790836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057709868997335434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08070887997746468 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039430102333426476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06682984530925751 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08310889825224876 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16800896264612675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08121994324028492 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9385959710925817 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08203997276723385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16644992865622044 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03648991696536541 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030988439917564 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5483780987560749 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08826982229948044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1874889712780714 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08828006684780121 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12576906010508537 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3506690263748169 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06701890379190445 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4616479855030775 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6399240121245384 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033769989386200905 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.021094085648656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07734890095889568 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10324898175895214 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.11786893568933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07702992297708988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16716006211936474 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722899757325649 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07940991781651974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03878003917634487 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06687990389764309 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325015030801296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15717907808721066 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.129238935187459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03084004856646061 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.009796978905797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0915799755603075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17685908824205399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041950028389692307 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106006473302841 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18837908282876015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08757016621530056 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18852902576327324 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08716993033885956 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10308995842933655 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24232990108430386 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08687912486493587 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37316884845495224 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1903459671884775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03921007737517357 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5937250573188066 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07452908903360367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16032904386520386 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12804893776774406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038629863411188126 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0664200633764267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07273000665009022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15650899149477482 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047579873353242874 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014969846233725548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1289800275117159 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0050060227513313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730601605027914 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15615997835993767 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190998151898384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902591273188591 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1873900182545185 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798018097877502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18885917961597443 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08659996092319489 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10345992632210255 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24376902729272842 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08754990994930267 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3754091449081898 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2015358079224825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03938889130949974 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.58396502956748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12500002048909664 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15063979662954807 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.442712921649218 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07578986696898937 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16515888273715973 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755984991788864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12771901674568653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03862008452415466 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06706011481583118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07378892041742802 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15748897567391396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0474399421364069 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01482013612985611 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22518914192914963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03642006777226925 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.155946170911193 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320893928408623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1580389216542244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0521000474691391 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048008359968662 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19839010201394558 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08823885582387447 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21008006297051907 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08930917829275131 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10382989421486855 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2081599086523056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12949993833899498 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38152909837663174 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2338960077613592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050239963456988335 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6392739489674568 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07533002644777298 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067991964519024 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792003892362118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22354885004460812 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03853999078273773 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0666789710521698 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446995005011559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16864994540810585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04757894203066826 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015140045434236526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2255300059914589 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2078159488737583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07336004637181759 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15724985860288143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05242996849119663 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08990895003080368 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19860011525452137 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0883401371538639 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20731892436742783 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0877799466252327 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10298890992999077 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2065291628241539 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12909993529319763 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37905899807810783 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2288661673665047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049900030717253685 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.63277518004179 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22166897542774677 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24892902001738548 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.984789924696088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07615005597472191 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1654401421546936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894984886050224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22367993369698524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03872998058795929 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06683985702693462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412000559270382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15898910351097584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048369867727160454 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22551906295120716 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2030461803078651 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07316982373595238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15777000226080418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773912951350212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2241102047264576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040038954466581345 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0674098264425993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074789859354496 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16014906577765942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04858989268541336 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22477912716567516 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2023358140140772 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.448031911626458 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07307995110750198 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15950016677379608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04940992221236229 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22377981804311275 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867992199957371 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06720004603266716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354002445936203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15814905054867268 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22506900131702423 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2000659480690956 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07397006265819073 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16132905147969723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05180994048714638 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09052013047039509 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19049900583922863 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08841999806463718 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2063990104943514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08808006532490253 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09641004726290703 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19879010505974293 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12983893975615501 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3727381117641926 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.215195981785655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04933006130158901 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.622375100851059 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07537915371358395 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16074907034635544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048140063881874084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22388901561498642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975001163780689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06807991303503513 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15833904035389423 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842993803322315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015230150893330574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22519915364682674 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2023258022964 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.116187104955316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481989450752735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1618298701941967 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4167079459875822 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0384598970413208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06692996248602867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07289997301995754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571991015225649 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048239948228001595 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2253891434520483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06790994666516781 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.475824974477291 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426016964018345 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15968992374837399 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742015153169632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41620898991823196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03911997191607952 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06693997420370579 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743200071156025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15904009342193604 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04876009188592434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014959834516048431 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22514001466333866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06797909736633301 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4837048947811127 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459986954927444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039004549384117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767905920743942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168190062046051 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0382100697606802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06659002974629402 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07307995110750198 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15704985707998276 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0482702162116766 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22505992092192173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06782892160117626 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4727951493114233 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22146920673549175 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2987391781061888 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.883742891252041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420987822115421 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596999354660511 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046969857066869736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168190062046051 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03844010643661022 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749900057911873 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16027013771235943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047819921746850014 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22533000446856022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06937887519598007 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4779551420360804 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355003617703915 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15930994413793087 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05239900201559067 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010988287627697 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2048388123512268 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08861999958753586 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2063990104943514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08795992471277714 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10381895117461681 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20711892284452915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12971879914402962 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38054795004427433 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2373561039566994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04978012293577194 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.644185045734048 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07493910379707813 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16194907948374748 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048239948228001595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41751889511942863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039378879591822624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802007555961609 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739500392228365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15908898785710335 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22537913173437119 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0691697932779789 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4849849976599216 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07351883687078953 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907897613942623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05258992314338684 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09044003672897816 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.197339104488492 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08807005360722542 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2055398654192686 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08765887469053268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10399986058473587 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2062798012048006 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13082893565297127 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38184900768101215 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2292959727346897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05030003376305103 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6364350449293852 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07578008808195591 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16335002146661282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04898989573121071 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3207188565284014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03922893665730953 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06758002564311028 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07409998215734959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15938910655677319 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04800991155207157 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015470199286937714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22580893710255623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05822000093758106 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3793958351016045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412000559270382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16117887571454048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05252007395029068 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069009684026241 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19900896586477757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08848006837069988 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20720018073916435 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08850893937051296 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1040799543261528 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2078202087432146 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1298289280384779 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38200896233320236 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2341560795903206 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05022995173931122 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6442150808870792 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22585014812648296 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3100389149039984 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.786798153072596 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12987013906240463 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2197090070694685 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06858003325760365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32162899151444435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03936002030968666 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06816000677645206 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438985630869865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15959003940224648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12980890460312366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0584600493311882 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.363515853881836 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350905798375607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16009900718927383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04181987605988979 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0917299184948206 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19005988724529743 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08932896889746189 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19086012616753578 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08823908865451813 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10441988706588745 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24323910474777222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0871000811457634 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37511903792619705 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2043060269206762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039869919419288635 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.591355074197054 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09148009121417999 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17821998335421085 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248389646410942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03902008756995201 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06685894913971424 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0728401355445385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15687989071011543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047170091420412064 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1297001726925373 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048690009862184525 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.19267706759274 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07484015077352524 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.169629929587245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041530001908540726 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169918484985828 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18861005082726479 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08823978714644909 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18789898604154587 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08748006075620651 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10329904034733772 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24253898300230503 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0871601514518261 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37446897476911545 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1968258768320084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039220089092850685 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5932440292090178 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748499296605587 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610091421753168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047770095989108086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17652986571192741 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039079925045371056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06630993448197842 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07511000148952007 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16136886551976204 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04759989678859711 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295700203627348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041930004954338074 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1191058438271284 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07288996130228043 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571900211274624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042038969695568085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09075994603335857 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18792902119457722 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08797016926109791 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18869922496378422 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08789007551968098 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10333908721804619 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24306890554726124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08910894393920898 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37811812944710255 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.199405873194337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039400067180395126 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.583595061674714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23338990285992622 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35123899579048157 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.0045309625566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22570905275642872 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.38055889308452606 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10253000073134899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.18041906878352165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039560021832585335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06714998744428158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08390005677938461 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16971002332866192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048350077122449875 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015620142221450806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08244998753070831 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04337006248533726 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3630460016429424 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08364906534552574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1687288749963045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03691995516419411 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09238999336957932 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5488581955432892 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08895900100469589 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18730014562606812 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798902854323387 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12671994045376778 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3522289916872978 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0664200633764267 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4626489244401455 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6426150687038898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03381003625690937 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0268631633371115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1613388303667307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2742588985711336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07261009886860847 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295290421694517 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03944011405110359 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06652995944023132 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08430890738964081 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16953889280557632 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822993651032448 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08172006346285343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03818911500275135 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1662160977721214 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08304021321237087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16795005649328232 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036319950595498085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149988181889057 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5468579474836588 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08939998224377632 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18784892745316029 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08868006989359856 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12708897702395916 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3518888261169195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.066359993070364 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46224799007177353 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.644104951992631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03350013867020607 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0272540859878063 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12143002822995186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23316009901463985 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07376912981271744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1288098283112049 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04197005182504654 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06699888035655022 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08449982851743698 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1686199102550745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742015153169632 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015379860997200012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0817000400274992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03850017674267292 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.144415931776166 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08292892016470432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16867904923856258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036949990317225456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09159999899566174 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5486789159476757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893990509212017 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18840981647372246 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08789892308413982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12628012336790562 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.351788941770792 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06707990542054176 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4630088806152344 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6438951715826988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0338901299983263 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0273830741643906 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.979316964745522 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08739018812775612 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025029992684721947 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10660896077752113 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.90776705928147 ms for forwarding
--------------------
No. 3
<class 'diffusers.models.embeddings.Timesteps'> take 0.2491490449756384 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05482998676598072 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.045830151066184044 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.037460122257471085 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03842008300125599 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2790391445159912 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15607988461852074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09471992962062359 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2078500110656023 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05134986713528633 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1013600267469883 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03969017416238785 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0776390079408884 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08752010762691498 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17616990953683853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04844903014600277 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01764995977282524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08676992729306221 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0548669379204512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07576914504170418 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16546901315450668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03866991028189659 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09552901610732079 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6158680189400911 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126006625592709 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19466900266706944 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980999700725079 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13132905587553978 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37618912756443024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06976001895964146 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4986079875379801 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7810440622270107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03539002500474453 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1967929787933826 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09009009227156639 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17974898219108582 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910002462565899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08286908268928528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03986014053225517 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09666010737419128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08752918802201748 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17639901489019394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048380112275481224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522991806268692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08456897921860218 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9981158655136824 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08908985182642937 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1761100720614195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03789900802075863 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0933399423956871 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5556980613619089 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1913788728415966 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089969951659441 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13027992099523544 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35973801277577877 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850995123386383 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47615799121558666 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6801250167191029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03526010550558567 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.080074045807123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07993984036147594 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10863016359508038 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.5585889387875795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07856008596718311 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17179897986352444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873005673289299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08230982348322868 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039190053939819336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07255887612700462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757100060582161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16242009587585926 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04850910045206547 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015440164133906364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13291998766362667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032769981771707535 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0479670017957687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07520988583564758 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17119990661740303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04354002885520458 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09255902841687202 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19564898684620857 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09082001633942127 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19357888959348202 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08904002606868744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10591908358037472 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24904892779886723 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08960999548435211 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38763810880482197 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2345651630312204 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04032999277114868 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.642374088987708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0762490089982748 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16463897190988064 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892004653811455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13166991993784904 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03896886482834816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06863009184598923 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07331999950110912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15940889716148376 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761992022395134 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015399884432554245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1325600314885378 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0291959624737501 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07289997301995754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604398712515831 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04350999370217323 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09258999489247799 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1916489563882351 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089599983766675 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1927688717842102 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08933991193771362 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.106368912383914 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24988921359181404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.095688970759511 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39437785744667053 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2376259546726942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04096003249287605 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6380350571125746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1292091328650713 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1596191432327032 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.628980929031968 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07714005187153816 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16820896416902542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04924018867313862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1314501278102398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039139995351433754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0706890132278204 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07587997242808342 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1643600407987833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048669055104255676 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015540048480033875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23100897669792175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03825989551842213 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1989760678261518 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07520010694861412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637497916817665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05342997610569 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0934000127017498 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21091895177960396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11038989759981632 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2218298614025116 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09145890362560749 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10583014227449894 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2139299176633358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13325992040336132 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3972291015088558 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3264259323477745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05556992255151272 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7712940461933613 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07661990821361542 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16665016300976276 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05029002204537392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22936007007956505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04054000601172447 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07154000923037529 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07598986849188805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16428879462182522 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015730038285255432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23104902356863022 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.259596087038517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16164989210665226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054589007049798965 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09198999032378197 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20147906616330147 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032012894749641 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21054898388683796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09042001329362392 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1050001010298729 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2117701806128025 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13339892029762268 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3930279053747654 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2671658769249916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05482998676598072 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.705495174974203 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23019895888864994 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.26035914197564125 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.312589859589934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07781991735100746 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17055892385542393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049380119889974594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22963900119066238 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04678010009229183 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07615890353918076 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07639988325536251 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1635500229895115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04899897612631321 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23136986419558525 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.270984997972846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498986087739468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623199786990881 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2291190903633833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879982978105545 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06841891445219517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1620100811123848 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048519112169742584 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23120013065636158 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2282559182494879 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.548360964283347 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08986005559563637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18767896108329296 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894984886050224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22957916371524334 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03877980634570122 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06896001286804676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16144895926117897 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05539995618164539 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01843995414674282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23129908367991447 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2715759221464396 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535889744758606 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656990498304367 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05294010043144226 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09302003309130669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19750883802771568 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09129894897341728 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20846002735197544 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010895155370235 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09755999781191349 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21727918647229671 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13466994278132915 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40082912892103195 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2712560128420591 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050508882850408554 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6925239469856024 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07656007073819637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16515981405973434 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049250200390815735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22640987299382687 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03933999687433243 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06782007403671741 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402012124657631 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16006012447178364 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04807906225323677 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01546996645629406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22800895385444164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2209659907966852 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.297536099329591 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07626996375620365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1646790187805891 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04740012809634209 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42109889909625053 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03864988684654236 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06888899952173233 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506995461881161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1624501310288906 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892912693321705 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2278198953717947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06897910498082638 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5028049238026142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07495004683732986 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16353907994925976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049449969083070755 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42013893835246563 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0403400044888258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07649906910955906 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07503991946578026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16232021152973175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05005905404686928 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2276089508086443 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07069995626807213 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5205550007522106 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518893107771873 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16425899229943752 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04897010512650013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4209890030324459 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039869919419288635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06827991455793381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07508997805416584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630790065973997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04951003938913345 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2280490007251501 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07002009078860283 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5136650763452053 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22403011098504066 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32198894768953323 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.025253864005208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546995766460896 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16546016559004784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04902901127934456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42152893729507923 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04059984348714352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07118890061974525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07597007788717747 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16427994705736637 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04915916360914707 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015940051525831223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22888998501002789 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07128994911909103 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5234749298542738 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459986954927444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16290880739688873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05324999801814556 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914598349481821 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20050909370183945 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08979998528957367 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20786887034773827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08880998939275742 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10371999815106392 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21001999266445637 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13135885819792747 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3891091328114271 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2524060439318419 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051110051572322845 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.669884892180562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07558008655905724 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1640799455344677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049190130084753036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4239780828356743 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039139995351433754 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06850995123386383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749488826841116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227900050580502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015389872714877129 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22794888354837894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0719800591468811 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5156250447034836 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07547019049525261 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644599251449108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05386979319155216 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0921089667826891 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21817907691001892 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09139999747276306 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20708912052214146 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08857995271682739 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1049491111189127 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21108891814947128 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1321400050073862 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39069890044629574 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2759550008922815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051029957830905914 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6963840462267399 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0767088495194912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.167149119079113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0499100424349308 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32375892624258995 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03840983845293522 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07134978659451008 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07553002797067165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16263010911643505 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737009294331074 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015570083633065224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22827996872365475 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059538986533880234 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.403875183314085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1626000739634037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05393894389271736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233993478119373 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20173913799226284 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927006274461746 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2095489762723446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09070010855793953 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10382896289229393 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21747895516455173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13200892135500908 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3973080310970545 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.266096020117402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05083996802568436 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6831648536026478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2278289757668972 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.314139062538743 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.027148062363267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13121007941663265 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2223001793026924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05281902849674225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32486882992088795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03887992352247238 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06897002458572388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446901872754097 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608789898455143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015380093827843666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13120891526341438 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06053992547094822 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3652858324348927 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07816986180841923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16492907889187336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04301988519728184 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09334017522633076 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19061914645135403 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918996900320053 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1920289359986782 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0883401371538639 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10445993393659592 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24566007778048515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08811894804239273 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38146902807056904 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.220725942403078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04017003811895847 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6194351483136415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19701896235346794 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04861014895141125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.226559117436409 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04271999932825565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07032905705273151 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422012276947498 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16046012751758099 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04695914685726166 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015630153939127922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13112998567521572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04968000575900078 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2333160266280174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16111996956169605 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043369829654693604 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09080907329916954 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19064010120928288 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08964003063738346 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18991902470588684 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935993537306786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10453001596033573 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2457587979733944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08797994814813137 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3812687937170267 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2165659572929144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040668994188308716 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.613433938473463 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0786499585956335 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16822991892695427 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1778199803084135 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385199673473835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06868015043437481 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073679955676198 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16089994460344315 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047040171921253204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1312699168920517 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04315981641411781 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.137105980888009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0935201533138752 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18158997409045696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04303990863263607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10484992526471615 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21420884877443314 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08937995880842209 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19158911891281605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0892002135515213 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1036887988448143 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24549895897507668 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09354902431368828 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39040809497237206 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.267945859581232 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040350016206502914 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6845751088112593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2348290290683508 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3525991924107075 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.226070018485188 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21865894086658955 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3741290420293808 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10386016219854355 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17869891598820686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396000687032938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06910995580255985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08768984116613865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17498992383480072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04798988811671734 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999881386756897 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08276896551251411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04443991929292679 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3722560834139585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08409889414906502 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17130887135863304 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03758003003895283 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09266985580325127 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5489080213010311 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09672902524471283 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18937001004815102 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08964003063738346 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12772902846336365 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35538896918296814 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06844010204076767 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4721179138869047 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6796840354800224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03442005254328251 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0718530286103487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16396911814808846 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27784891426563263 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13107992708683014 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039319973438978195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06793998181819916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08386978879570961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17063994891941547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04850002005696297 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08270982652902603 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03933999687433243 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1843058746308088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08542882278561592 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1725491601973772 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03697001375257969 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09199883788824081 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5489790346473455 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09003002196550369 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19951886497437954 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986005559563637 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12758886441588402 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3556390292942524 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06774906069040298 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4719079006463289 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.674124039709568 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03439001739025116 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.066213171929121 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12228009290993214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23449910804629326 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07384014315903187 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13031018897891045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03908993676304817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06825989112257957 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08342019282281399 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17075985670089722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0477801077067852 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08332892321050167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039400067180395126 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1465358547866344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08277897723019123 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17037917859852314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04006992094218731 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248009882867336 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5500281695276499 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891189556568861 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18789991736412048 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918903768062592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12791994959115982 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35542878322303295 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0681800302118063 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47138892114162445 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.671105157583952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0346300657838583 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0664529874920845 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.137136094272137 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0854399986565113 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03038998693227768 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.17483998090028763 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.23957014642656 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.23196893744170666 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05622999742627144 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04357891157269478 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02492009662091732 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03691995516419411 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.25535887107253075 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07128994911909103 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06988993845880032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567190047353506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04903017543256283 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08224882185459137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939983434975147 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679700169712305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08326908573508263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16881898045539856 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048930058255791664 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08205999620258808 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9365170262753963 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06572995334863663 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15184003859758377 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03671995364129543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0927590299397707 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5640580784529448 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006005711853504 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21043908782303333 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09149010293185711 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.132089015096426 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3629291895776987 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06793998181819916 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4807179793715477 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7152340151369572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0342000275850296 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1016341634094715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08439994417130947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17400900833308697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049900030717253685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08098920807242393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03980984911322594 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06718002259731293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08369912393391132 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17100898548960686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0495698768645525 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016089994460344315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08268002420663834 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9516370482742786 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08273986168205738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17034984193742275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03687990829348564 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09244889952242374 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5462979897856712 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901299063116312 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19157910719513893 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09074993431568146 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12647896073758602 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35340897738933563 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0669590663164854 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46820822171866894 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6622538678348064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034289900213479996 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0559041295200586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07893913425505161 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11360901407897472 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.264689145609736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0841999426484108 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18771016038954258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05834992043673992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08026999421417713 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399700365960598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0679302029311657 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07484015077352524 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16122008673846722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04995986819267273 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015919096767902374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12932997196912766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03181002102792263 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0671268682926893 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07358891889452934 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16106897965073586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04206993617117405 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09256904013454914 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18967990763485432 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912488903850317 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1916701439768076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08959905244410038 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10906998068094254 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26055891066789627 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08692010305821896 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39720884524285793 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2384860310703516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03942009061574936 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6315339598804712 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07633003406226635 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645700540393591 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04895986057817936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1292100641876459 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797009751200676 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07375981658697128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466995157301426 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16102008521556854 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894006997346878 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016079982742667198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12968992814421654 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0489469859749079 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07607880979776382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1769987866282463 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042449915781617165 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09306007996201515 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1901998184621334 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021884761750698 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1923500094562769 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953898213803768 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10404014028608799 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24494901299476624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0879000872373581 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3807288594543934 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2213059235364199 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03999914042651653 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.632113941013813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1261201687157154 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15351991169154644 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.6398718152195215 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07761013694107533 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16870885156095028 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04940014332532883 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12815906666219234 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04016002640128136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07462012581527233 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08218991570174694 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17435895279049873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0491398386657238 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01597008667886257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2256990410387516 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037109944969415665 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2175750453025103 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07441011257469654 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177003271877766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055770156905055046 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10388996452093124 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20208884961903095 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061978198587894 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21769898012280464 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09151012636721134 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10419008322060108 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21046889014542103 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13039889745414257 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3881379961967468 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2817359529435635 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05016988143324852 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7070351168513298 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07733888924121857 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1668690238147974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04973984323441982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22480892948806286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04008994437754154 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802985444664955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16119005158543587 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048969872295856476 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016009900718927383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22530904971063137 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2219061609357595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07461896166205406 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16106897965073586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053060008212924004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10947906412184238 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21011009812355042 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980999700725079 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20873919129371643 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905301421880722 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10388880036771297 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22802897728979588 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13163010589778423 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4091088194400072 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.309575978666544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0502800103276968 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.73037382774055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22848905064165592 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25650905445218086 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.2379189766943455 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07742992602288723 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.170139130204916 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05083996802568436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2243989147245884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04055979661643505 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06822985596954823 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07533910684287548 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16186898574233055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049930065870285034 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01579010859131813 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22507901303470135 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2280759401619434 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472001016139984 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1614589709788561 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04988000728189945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22388878278434277 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04006992094218731 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06691017188131809 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07525901310145855 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16138888895511627 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050220172852277756 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015879981219768524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22555887699127197 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2160961050540209 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4892219807952642 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745789147913456 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16224919818341732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04975008778274059 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22449903190135956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399700365960598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06878003478050232 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07850909605622292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1656890381127596 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04939991049468517 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015620142221450806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2255789004266262 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2220959179103374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492909207940102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17784908413887024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05233008414506912 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913690309971571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19262009300291538 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967914618551731 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.206769909709692 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08850009180605412 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09668990969657898 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2008287701755762 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1309998333454132 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3779290709644556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2314659543335438 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0495188869535923 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6587041318416595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07561990059912205 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1619500108063221 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488900113850832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2237800508737564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03861007280647755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06666895933449268 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16291998326778412 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797894507646561 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224800081923604 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.209205947816372 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.185425816103816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07500988431274891 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16152998432517052 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795915447175503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41663902811706066 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038950005546212196 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06867898628115654 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457006722688675 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.160090159624815 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04827999509871006 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015238998457789421 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22558006457984447 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06920890882611275 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4856848865747452 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07468997500836849 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067991964519024 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048099085688591 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4165288992226124 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039289938285946846 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670100562274456 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07543014362454414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.164389843121171 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04833005368709564 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01488998532295227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2252499107271433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06847898475825787 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4838750939816236 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07361988537013531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593900378793478 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705903120338917 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4172390326857567 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03864988684654236 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702006794512272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07411022670567036 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15938002616167068 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04820013418793678 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248289529234171 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06839004345238209 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4810750726610422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2221090253442526 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31206896528601646 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.920203937217593 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435004226863384 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609399914741516 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06334995850920677 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4177589435130358 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040029874071478844 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06766896694898605 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075190095230937 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598901581019163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047588953748345375 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01516006886959076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2256899606436491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06946898065507412 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4999350532889366 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07826997898519039 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16320007853209972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0528490636497736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09124004282057285 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19921897910535336 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08880021050572395 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20561902783811092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08847005665302277 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1025698147714138 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20678900182247162 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13119890354573727 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38240780122578144 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2343160342425108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049770111218094826 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6468751709908247 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07499894127249718 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612191554158926 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047560082748532295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4165580030530691 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03850995562970638 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06720004603266716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16610906459391117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04903995431959629 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015400117263197899 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22648903541266918 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07154000923037529 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4998549595475197 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07334910333156586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591190230101347 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052690040320158005 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09129894897341728 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19663991406559944 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08829892612993717 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20692916586995125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08735992014408112 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10413001291453838 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20737899467349052 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12948992662131786 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38073910400271416 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2291260063648224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049770111218094826 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6376839485019445 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07589999586343765 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162418931722641 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04881015047430992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32039894722402096 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915000706911087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06734998896718025 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07498008199036121 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16048992983996868 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22578006610274315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05809892900288105 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3775150291621685 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738298986107111 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16003986820578575 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052298884838819504 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09134015999734402 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2155990805476904 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08846004493534565 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20654895342886448 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08846004493534565 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10391883552074432 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2078490797430277 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13035908341407776 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38252794183790684 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2518458534032106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04995008930563927 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6586650162935257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2262189518660307 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30956906266510487 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.839307982474566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13010995462536812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21906010806560516 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05279993638396263 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32093911431729794 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03947014920413494 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07075001485645771 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742690172046423 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15946896746754646 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047770095989108086 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1291888765990734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058609992265701294 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3472458813339472 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07343990728259087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15841005370020866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04178006201982498 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169010445475578 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.189248938113451 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08708983659744263 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1894291490316391 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09652017615735531 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10351883247494698 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24231895804405212 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08699879981577396 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37381798028945923 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.206016167998314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039049889892339706 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.591414911672473 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09153899736702442 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17836899496614933 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04866998642683029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22459891624748707 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039419857785105705 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06688013672828674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354002445936203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15737907961010933 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048170099034905434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522991806268692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13013905845582485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048560090363025665 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.189426053315401 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07291906513273716 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15856907702982426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04176981747150421 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19361916929483414 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08863001130521297 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19004987552762032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08855899795889854 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10344991460442543 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24200999177992344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08724885992705822 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3736591897904873 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2042159214615822 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03930996172130108 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5902849845588207 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07751001976430416 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16413885168731213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04801992326974869 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17657899297773838 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879982978105545 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07452000863850117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420009933412075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15823892317712307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04715984687209129 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12911902740597725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0424401368945837 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1309459805488586 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294910028576851 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1578889787197113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042140018194913864 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09050010703504086 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1885590609163046 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08868006989359856 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1913991291075945 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08923886343836784 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10362011380493641 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24277018383145332 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08739903569221497 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37510902620851994 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.201746053993702 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03906991332769394 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5866649337112904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23432006128132343 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3515789285302162 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.99730995297432 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2192188985645771 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37167896516621113 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10382011532783508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.177720095962286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03991997800767422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06677908822894096 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08446001447737217 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17045997083187103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04859990440309048 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015589874237775803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08193007670342922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05658995360136032 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.36921601369977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08360901847481728 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17026904970407486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03643985837697983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.096459174528718 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5475289653986692 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08962000720202923 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19082985818386078 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08952990174293518 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12713996693491936 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3520089667290449 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0669499859213829 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46450900845229626 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6557939816266298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033559976145625114 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.039812970906496 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16191904433071613 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27387915179133415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0727199949324131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295791007578373 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950018435716629 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702984683215618 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0837189145386219 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16863900236785412 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047930050641298294 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015169847756624222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08171913214027882 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03821984864771366 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1658358853310347 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08322019129991531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16967998817563057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036670127883553505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177997708320618 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.547928037121892 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09369011968374252 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1898601185530424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08866889402270317 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12674019671976566 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3521689213812351 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06784009747207165 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46461913734674454 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6534251626580954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03378908149898052 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.038253005594015 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12056995183229446 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23388885892927647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07340987212955952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12919912114739418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03835977986454964 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06591994315385818 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08267886005342007 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1676089596003294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05063996650278568 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015139812603592873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08154986426234245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03856886178255081 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.126025803387165 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08273008279502392 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1678699627518654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03654905594885349 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09063002653419971 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5489878822118044 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08906004950404167 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18874905072152615 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08804886601865292 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12720003724098206 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3526888322085142 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06693997420370579 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47157914377748966 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6533751040697098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033430056646466255 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0357631146907806 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.998477064073086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0852199736982584 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025919871404767036 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10851887054741383 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.786733938381076 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19990000873804092 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.046508852392435074 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.042400090023875237 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025690067559480667 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.037449877709150314 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24897907860577106 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06856000982224941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0672598835080862 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15607010573148727 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05070003680884838 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08241902105510235 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012999124825001 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06842007860541344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08371914736926556 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17227907665073872 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04980992525815964 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016919802874326706 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08278898894786835 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.951356953009963 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06367010064423084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15049008652567863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03696000203490257 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09397999383509159 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5503781139850616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09172013960778713 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19743898883461952 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09044003672897816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1271700020879507 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35345787182450294 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06762007251381874 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46772812493145466 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6774148680269718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034739961847662926 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0658541470766068 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08345977403223515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17294008284807205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05030003376305103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08116918615996838 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039950013160705566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0671998132020235 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08411891758441925 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17173890955746174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04981015808880329 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016330042853951454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08194008842110634 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9494370315223932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08179992437362671 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16952003352344036 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03674998879432678 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09291991591453552 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5459780804812908 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0908400397747755 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19223918206989765 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08970010094344616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1261590514332056 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35239895805716515 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0685499981045723 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4762578755617142 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6692450735718012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034159980714321136 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0574640948325396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07740897126495838 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1040890347212553 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.233809981495142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07702992297708988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16797985881567 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04937010817229748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07996009662747383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040529994294047356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06758002564311028 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453002035617828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159950228407979 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01593981869518757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1294100657105446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031589996069669724 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0297370608896017 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450906559824944 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609991304576397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04188995808362961 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09249919094145298 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18915999680757523 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09022909216582775 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1917099580168724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08942908607423306 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10271999053657055 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24263886734843254 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08670985698699951 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37469901144504547 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2112760450690985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03981008194386959 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6033248975872993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07589999586343765 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16341009177267551 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049170106649398804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1287488266825676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040220096707344055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712996400892735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387995719909668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16961898654699326 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049029942601919174 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015990110114216805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12938887812197208 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0363769251853228 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400987669825554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16076909378170967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04187994636595249 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09427987970411777 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18974882550537586 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08997996337711811 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1926789991557598 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08907006122171879 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10408996604382992 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24397880770266056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08770893327891827 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37837796844542027 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.217735931277275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03951997496187687 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6106050461530685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12569897808134556 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1535590272396803 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.538202123716474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07672002539038658 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16847997903823853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04900991916656494 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1277897972613573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03967992961406708 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06733904592692852 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0750902108848095 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16150996088981628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049669062718749046 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016069971024990082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22559985518455505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0368200708180666 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1798860505223274 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438985630869865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16178004443645477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05252892151474953 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09291991591453552 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20197918638586998 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048008359968662 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22546900436282158 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09107007645070553 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1040501520037651 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21003000438213348 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1305087935179472 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38676802068948746 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2718059588223696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05041016265749931 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6851951368153095 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07713888771831989 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1653588842600584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049980124458670616 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2240289468318224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0402799341827631 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06788992322981358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07493002340197563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1620999537408352 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049480004236102104 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015659956261515617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2255490981042385 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.218866091221571 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372908294200897 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1583588309586048 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05259993486106396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998881094157696 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19829999655485153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08887914009392262 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2163900062441826 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08936994709074497 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10310998186469078 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2072989009320736 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13000983744859695 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3826189786195755 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2493361718952656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05010911263525486 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6568638384342194 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22149900905787945 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24811900220811367 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.090339971706271 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07733004167675972 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16783014871180058 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049408990889787674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22527994588017464 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906921483576298 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06690993905067444 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467996329069138 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15900889411568642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048369867727160454 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248990349471569 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2130760587751865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387995719909668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588298473507166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04760990850627422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22388994693756104 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03857002593576908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0666698906570673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07449998520314693 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15928898938000202 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22565899416804314 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1971660424023867 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4529320653527975 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07404899224638939 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612589694559574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22416887804865837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038950005546212196 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06802007555961609 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07375911809504032 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16317912377417088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860013723373413 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015479978173971176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22513908334076405 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2100262101739645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16256910748779774 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05450006574392319 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09260885417461395 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19115000031888485 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890800729393959 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2075189258903265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08855992928147316 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09689992293715477 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19914889708161354 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13087992556393147 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3751788754016161 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2264358811080456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04944903776049614 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6570438165217638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07608020678162575 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16191997565329075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04882877692580223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2238699235022068 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038909027352929115 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06706011481583118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07467018440365791 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593390479683876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840991459786892 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01535983756184578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22454909048974514 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2031961232423782 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.164626123383641 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552001625299454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227900050580502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822993651032448 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41727907955646515 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03920006565749645 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0667390413582325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16059004701673985 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049198977649211884 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22517982870340347 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06863893941044807 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4868248254060745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15988992527127266 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047430163249373436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4165091086179018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038800062611699104 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06722891703248024 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07344991900026798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1583099365234375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05109002813696861 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015209894627332687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22511021234095097 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06802892312407494 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4798848424106836 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159319955855608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736892879009247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168690647929907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869994543492794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06668013520538807 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342989556491375 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15785987488925457 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483999028801918 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22508017718791962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06856909021735191 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4744049403816462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2213690895587206 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.29939902015030384 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.892323864623904 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475004531443119 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16041891649365425 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742015153169632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41715893894433975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04051998257637024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06808899343013763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748801976442337 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15959003940224648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049168942496180534 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015700003132224083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22545014508068562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06947899237275124 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4869249425828457 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384992204606533 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159319955855608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05243904888629913 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065004996955395 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19952887669205666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08837995119392872 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20711892284452915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08732988499104977 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10487879626452923 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2116889227181673 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1303190365433693 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3866280894726515 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2378559913486242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04987022839486599 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6446649096906185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749190803617239 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16178889200091362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048140063881874084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41656894609332085 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037700170651078224 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06699003279209137 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481989450752735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16046897508203983 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797009751200676 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.030010007321834564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22690906189382076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06908015348017216 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4996251557022333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362896576523781 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907897613942623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052270013839006424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110989049077034 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19898987375199795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08928892202675343 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2064600121229887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08789892308413982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1037700567394495 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20642997696995735 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12938003055751324 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38039893843233585 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2327858712524176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05025998689234257 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6393938567489386 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757498200982809 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16322010196745396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32061897218227386 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038319965824484825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06814999505877495 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370905950665474 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15860889106988907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015519792214035988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22554886527359486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058069825172424316 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.379295950755477 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07375888526439667 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15982892364263535 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05222996696829796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125005453824997 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20826887339353561 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08849892765283585 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20686001516878605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0892088282853365 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1040198840200901 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20777899771928787 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1299798022955656 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38220896385610104 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2458260171115398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05008885636925697 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6525241080671549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22603897377848625 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3096491564065218 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.821708081290126 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13020914047956467 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2197488211095333 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05231006070971489 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3213889431208372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04011020064353943 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673199538141489 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457006722688675 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16037002205848694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295700203627348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05888892337679863 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3459448236972094 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07262988947331905 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587499864399433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041669001802802086 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09056995622813702 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1894589513540268 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08841999806463718 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19042892381548882 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08872011676430702 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10501896031200886 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25152903981506824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08684908971190453 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3834280651062727 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2113761622458696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03948016092181206 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5973150730133057 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09057903662323952 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17814897000789642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04905997775495052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22473884746432304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950996324419975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06684986874461174 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07273000665009022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15725893899798393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015739817172288895 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12974906712770462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04837010055780411 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1890858877450228 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301918230950832 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15898910351097584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041740015149116516 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202002547681332 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18881913274526596 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08895015344023705 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19013904966413975 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08780998177826405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10379008017480373 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2431301400065422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08683884516358376 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3745888825505972 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2009458150714636 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039139995351433754 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5908449422568083 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07813004776835442 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16518891789019108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824995994567871 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.176489120349288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385199673473835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06649992428719997 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355003617703915 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15978887677192688 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015419907867908478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12987898662686348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04206015728414059 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1378058698028326 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07307901978492737 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1576088834553957 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041689956560730934 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09039998985826969 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18798909150063992 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891699455678463 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18965918570756912 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08701905608177185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10350998491048813 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24321000091731548 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08742883801460266 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3762489650398493 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1997059918940067 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03930996172130108 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5841650310903788 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23454008623957634 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3521090839058161 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.006970096379519 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21915906108915806 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37126895040273666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10324991308152676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17731892876327038 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0388899352401495 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07135886698961258 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08300994522869587 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16908999532461166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840013571083546 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015440164133906364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08216011337935925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04309997893869877 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3522061053663492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08415919728577137 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1780490856617689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036209821701049805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5477489903569221 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08922908455133438 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18875999376177788 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08878996595740318 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12689013965427876 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3514089621603489 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0666400883346796 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4630992189049721 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6453550197184086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0338300596922636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.038022968918085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1614391803741455 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2736488822847605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07267994806170464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12947898358106613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03952020779252052 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653997115790844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08311890996992588 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16839895397424698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821992479264736 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015199882909655571 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08179014548659325 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03853999078273773 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1662959586828947 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08291006088256836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16826996579766273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03599910996854305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0914200209081173 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5494379438459873 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0912100076675415 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18888898193836212 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.088860047981143 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12675998732447624 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35219802521169186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06678001955151558 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46402798034250736 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.65330502204597 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03362004645168781 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.03471421264112 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11908006854355335 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23273983970284462 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07350882515311241 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12904987670481205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899946957826614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06681913509964943 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0834898091852665 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16897008754312992 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796893335878849 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015560071915388107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08218991570174694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038800062611699104 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1287869419902563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08261902257800102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1681288704276085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036689918488264084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09069894440472126 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5486689042299986 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978997357189655 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18842006102204323 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08865003474056721 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12706010602414608 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35320897586643696 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06699003279209137 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4654878284782171 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6486439853906631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03395997919142246 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0315530709922314 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.974378161132336 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08613010868430138 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02516014501452446 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11253985576331615 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.35405590571463 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.1903800293803215 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.047378940507769585 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.040590064600110054 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02496992237865925 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.036980025470256805 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24367915466427803 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07797894068062305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06618979386985302 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15219999477267265 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04912889562547207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08204998448491096 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03970996476709843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06799004040658474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08397013880312443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1700189895927906 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822993651032448 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01597008667886257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08242903277277946 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9324471466243267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0620400533080101 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14734012074768543 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03651995211839676 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09206007234752178 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5513480864465237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08974899537861347 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2063398715108633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09014899842441082 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266198232769966 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35214913077652454 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06649992428719997 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46457909047603607 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6731149516999722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0338088721036911 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0511329639703035 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08295010775327682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16987998969852924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842900671064854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08038012310862541 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03916001878678799 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06692996248602867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08339015766978264 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16886997036635876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04837010055780411 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015500001609325409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0820790883153677 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9339069947600365 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08238991722464561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16791000962257385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03685988485813141 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09228009730577469 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5490980111062527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983002044260502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18948898650705814 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08876994252204895 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1262100413441658 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3502489998936653 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06613996811211109 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46087917871773243 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.647335011512041 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03371993079781532 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0304040517657995 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08152006193995476 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10776007547974586 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.1582401394844055 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07686996832489967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16743899323046207 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834006540477276 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07980014197528362 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03870995715260506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06670900620520115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733700580894947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15792017802596092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04799896851181984 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1292601227760315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.031010014936327934 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0163069237023592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355003617703915 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15872903168201447 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041939783841371536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09112898260354996 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1896300818771124 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08940906263887882 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1898601185530424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884887211024761 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10337005369365215 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24281907826662064 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08677993901073933 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.375199131667614 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2016759719699621 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03908993676304817 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5875038225203753 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07544993422925472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612100750207901 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884018562734127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12859911657869816 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039109960198402405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06672996096313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07368996739387512 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15799910761415958 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048179877921938896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1294291578233242 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.010396983474493 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301010191440582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571399625390768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041869934648275375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09248009882867336 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18949899822473526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08895993232727051 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19071903079748154 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08874991908669472 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10366900824010372 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24301884695887566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08752895519137383 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37589785642921925 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.214456046000123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039739999920129776 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5996650326997042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12520886957645416 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15160883776843548 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.46632194891572 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0767300371080637 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16653002239763737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04832888953387737 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12747012078762054 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918982110917568 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0674789771437645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07397006265819073 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15875999815762043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052380142733454704 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01519918441772461 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22514001466333866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036250101402401924 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1664459016174078 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415004074573517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15931017696857452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052449991926550865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078998118638992 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19941898062825203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909008465707302 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20713894627988338 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.088860047981143 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10407879017293453 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2238790038973093 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13063987717032433 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4002079367637634 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2538160663098097 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049900030717253685 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6625449061393738 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07553002797067165 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616491936147213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797009751200676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22398889996111393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03909994848072529 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06751017645001411 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07388903759419918 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159089220687747 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048519810661673546 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.017479993402957916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22544898092746735 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.207925844937563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402012124657631 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15866896137595177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052080024033784866 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034993126988411 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20040897652506828 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890800729393959 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20718900486826897 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08870894089341164 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1025698147714138 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20603998564183712 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12965896166861057 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3803789149969816 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2360857799649239 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050250208005309105 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.643375027924776 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22106990218162537 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2478589303791523 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.0274009592831135 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07657008245587349 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16631907783448696 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04929001443088055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22422894835472107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04006014205515385 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06730994209647179 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485994137823582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593299675732851 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015050172805786133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22559892386198044 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2105959467589855 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346994243562222 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15913904644548893 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04863995127379894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.223908806219697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03878003917634487 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0668501015752554 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415004074573517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15838001854717731 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822993651032448 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22494886070489883 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1972258798778057 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.450641943141818 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420987822115421 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16012904234230518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048350077122449875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22330903448164463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03927014768123627 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06772996857762337 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15843985602259636 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015029916539788246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22505898959934711 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1993160005658865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743200071156025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16214000061154366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05255010910332203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09141885675489902 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.191250117495656 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10575004853308201 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20725885406136513 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08880998939275742 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09683915413916111 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19938894547522068 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1305299811065197 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37469901144504547 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2400848791003227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04904996603727341 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6493340954184532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07612002082169056 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621791161596775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048759859055280685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22396910935640335 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039109960198402405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06703007966279984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486902177333832 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1591790933161974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048380112275481224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22498914040625095 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2036759871989489 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.143845988437533 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07546995766460896 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1626389566808939 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04718988202512264 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41693798266351223 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03870995715260506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673800241202116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435888983309269 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922915190458298 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860013723373413 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22538891062140465 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06814999505877495 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4807351399213076 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369020022451878 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15897001139819622 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05684001371264458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4169279709458351 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039389822632074356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06697000935673714 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744890421628952 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600789837539196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014850171282887459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2251989208161831 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06819004192948341 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.487666042521596 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16024988144636154 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047209905460476875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41648815385997295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038869911804795265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06657000631093979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408903911709785 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15938887372612953 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806998185813427 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759832993149757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22506900131702423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06800983101129532 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4763749204576015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22179004736244678 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2990288194268942 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.89622401073575 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07463013753294945 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16085989773273468 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04838989116251469 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4169382154941559 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03929995000362396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07078005000948906 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07513980381190777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16038911417126656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488602090626955 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015039928257465363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22573885507881641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06922008469700813 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4882960822433233 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15842006541788578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05230004899203777 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09150896221399307 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1990899909287691 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09342003613710403 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2063990104943514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08770008571445942 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10316004045307636 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20592892542481422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13046013191342354 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3808089531958103 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2371961493045092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04973006434738636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.643243944272399 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16177003271877766 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770979285240173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4164590500295162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03936979919672012 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673290342092514 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07470999844372272 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1595399808138609 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04766997881233692 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22537005133926868 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0688689760863781 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.479635015130043 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07323990575969219 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17448002472519875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0526688527315855 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913599506020546 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1981891691684723 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08882000111043453 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2061289269477129 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08826004341244698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10383897460997105 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2066588494926691 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1300191506743431 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.380747951567173 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2314359191805124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04968000575900078 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.653594896197319 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545901462435722 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16227900050580502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04854006692767143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32082898542284966 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03887992352247238 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0671090092509985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07409998215734959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587399747222662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22541009820997715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05777901969850063 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3740349095314741 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15945988707244396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052069080993533134 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09046006016433239 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20071910694241524 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891699455678463 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20750914700329304 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09852997027337551 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10350905358791351 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20675919950008392 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12891902588307858 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3798580728471279 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2442260049283504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049999915063381195 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6503848601132631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2259691245853901 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.309809111058712 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.805178036913276 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12952997349202633 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21859980188310146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05247979424893856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3206890542060137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039319973438978195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0669499859213829 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07352884858846664 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582489348948002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475000124424696 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015180092304944992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1296401023864746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0591999851167202 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3427550438791513 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258984260261059 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15665008686482906 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190998151898384 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09125005453824997 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18901890143752098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884300097823143 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18990878015756607 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08770008571445942 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10347901843488216 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2425189595669508 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08656014688313007 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37360796704888344 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1979159899055958 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03961008042097092 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5819650143384933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09190896525979042 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18136901780962944 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04816008731722832 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22461917251348495 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03887992352247238 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06664986722171307 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362989708781242 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15715998597443104 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015650177374482155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13034907169640064 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048590125516057014 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1988859623670578 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07802899926900864 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16271905042231083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041869934648275375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09163911454379559 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18915999680757523 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08875899948179722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19021984189748764 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08759996853768826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10396982543170452 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24301884695887566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08727982640266418 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3742990083992481 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1992561630904675 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03992905840277672 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.592263812199235 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07637008093297482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16549997963011265 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048579880967736244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1768998336046934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03998889587819576 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06680004298686981 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494003511965275 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16024895012378693 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048479996621608734 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015929806977510452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12946990318596363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04264898598194122 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1410759761929512 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385993376374245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16031996347010136 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04249904304742813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09635998867452145 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18900888971984386 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902798492461443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19171880558133125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08887005969882011 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10335911065340042 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24393899366259575 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08761999197304249 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3778489772230387 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.217066077515483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03920006565749645 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6082250513136387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23441901430487633 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3532790578901768 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.045629994943738 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21950993686914444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37308898754417896 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10360986925661564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.178199028596282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04029995761811733 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06749993190169334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08407002314925194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17084903083741665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049040187150239944 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016009900718927383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08232914842665195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04365015774965286 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3643261045217514 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08344999514520168 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1699291169643402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036949990317225456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09296019561588764 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5581481382250786 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09244889952242374 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19024009816348553 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09043887257575989 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1272200606763363 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35362900234758854 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06661983206868172 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46691903844475746 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6735449898988008 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034389086067676544 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0617530681192875 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16203918494284153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27534901164472103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07320009171962738 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13023917563259602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04041008651256561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702984683215618 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08413894101977348 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17074914649128914 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04918989725410938 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08196011185646057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038519036024808884 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.180825987830758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08352985605597496 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17098011448979378 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03684009425342083 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920700840651989 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5479680839926004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08912011981010437 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20746910013258457 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08894992060959339 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12693903408944607 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3526480868458748 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06624008528888226 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4633879289031029 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.665945164859295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03393995575606823 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.052073832601309 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11960999108850956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2312399446964264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07378892041742802 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12910994701087475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03943988122045994 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0667688436806202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08394010365009308 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16859988681972027 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048449961468577385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015220139175653458 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08196011185646057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0384598970413208 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1214970145374537 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0826388131827116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16734888777136803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036060111597180367 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122979827225208 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5477990489453077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08875899948179722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18828990869224072 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918903768062592 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1276899129152298 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3531391266733408 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06649014540016651 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46450900845229626 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6464248765259981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03475998528301716 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.036373130977154 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.040586814284325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08761999197304249 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024850014597177505 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10756985284388065 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.19341603294015 ms for forwarding
--------------------
No. 4
<class 'diffusers.models.embeddings.Timesteps'> take 0.25248900055885315 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.056168995797634125 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.046010129153728485 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.036659883335232735 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.038040103390812874 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.27967896312475204 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.16384897753596306 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0986889936029911 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21033897064626217 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05122995935380459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09792903438210487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03908993676304817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0773200299590826 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08707912638783455 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17620879225432873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04824995994567871 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.017869984731078148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08447002619504929 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0517160408198833 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07426994852721691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628100872039795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03820005804300308 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09539001621305943 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6060178857296705 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10894984006881714 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20289001986384392 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094900451600552 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13169017620384693 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3784091677516699 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07111998274922371 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5040289834141731 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.8182850908488035 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0395399983972311 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.247412921860814 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08794013410806656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17774896696209908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04965020343661308 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08310982957482338 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03890995867550373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0698890071362257 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0876300036907196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17459015361964703 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050729140639305115 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015409896150231361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08458993397653103 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9674669709056616 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08680904284119606 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17376895993947983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03774999640882015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09236996993422508 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.557358842343092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021908044815063 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1911199651658535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944002911448479 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1296401023864746 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3587789833545685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06880005821585655 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47536916099488735 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6783939208835363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035800039768218994 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.076893113553524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0795398373156786 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10791001841425896 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.570898927748203 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07760990411043167 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1705600880086422 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04831003025174141 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08280901238322258 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039109960198402405 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07210997864603996 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07616914808750153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1636191736906767 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04758010618388653 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01576007343828678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13256887905299664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03316998481750488 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0472368448972702 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17155916430056095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044369837269186974 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0915799755603075 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19428902305662632 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901601742953062 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1922999508678913 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890588853508234 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1050999853760004 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24725892581045628 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08927006274461746 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3838690463453531 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2265359982848167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040869927033782005 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6349151264876127 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07646996527910233 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1648298930376768 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048380112275481224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13199890963733196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03942009061574936 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06915000267326832 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742599368095398 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16162008978426456 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05082995630800724 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016220146790146828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1332201063632965 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0405471548438072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07439893670380116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16277912072837353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04343991167843342 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09265006519854069 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19274000078439713 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09308895096182823 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2224799245595932 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08997996337711811 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10659010149538517 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24986895732581615 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09058997966349125 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3902888856828213 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2745759449899197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0417300034314394 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6776840202510357 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12868992052972317 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15768012963235378 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.669931182637811 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07751001976430416 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17105997540056705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04973006434738636 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13100914657115936 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042009823024273 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06983010098338127 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07659010589122772 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16498006880283356 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049530062824487686 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01625996083021164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23116893135011196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03840983845293522 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2088671792298555 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07531000301241875 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16387994401156902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053270021453499794 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0924400519579649 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2111590001732111 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094015695154667 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.213259132578969 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10569905862212181 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21231896243989468 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13087992556393147 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3918290603905916 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.277104951441288 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05137990228831768 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7044448759406805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08717901073396206 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18399907276034355 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04998990334570408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22590882144868374 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040449900552630424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07740012370049953 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07637008093297482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.165038974955678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0498499721288681 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016110017895698547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2277689054608345 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2661959044635296 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07452908903360367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16385898925364017 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05400995723903179 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09303889237344265 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20291004329919815 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08857902139425278 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20789005793631077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927006274461746 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10356004349887371 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20866887643933296 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13156002387404442 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38691889494657516 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2527660001069307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0563089270144701 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6897039022296667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22631906904280186 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2565188333392143 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.243688985705376 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0775300431996584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16793003305792809 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049558933824300766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22628996521234512 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0403490848839283 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07535004988312721 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0792990904301405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17222901806235313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04855007864534855 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015380093827843666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22800895385444164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2577460147440434 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075190095230937 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17851893790066242 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048380112275481224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22657890804111958 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03836001269519329 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06804987788200378 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751689076423645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16199913807213306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048900023102760315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015219906345009804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22748904302716255 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2371758930385113 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5410018861293793 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487903349101543 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16395887359976768 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048690009862184525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22631906904280186 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03891997039318085 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06819004192948341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07483013905584812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1618091482669115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04961015656590462 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015289988368749619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22987904958426952 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2301360256969929 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07537007331848145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16589881852269173 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05346001125872135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09282887913286686 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19809999503195286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10248902253806591 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22743898443877697 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08898996748030186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09732902981340885 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20117918029427528 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13210996985435486 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3819891717284918 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2817960232496262 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05058012902736664 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7043740954250097 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07650000043213367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1649491023272276 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04921993240714073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22667879238724709 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06819004192948341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07543899118900299 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16191904433071613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787021316587925 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015510013327002525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22727902978658676 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2229059357196093 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.267486045137048 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07625902071595192 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16523897647857666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4205291625112295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03961916081607342 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06866990588605404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07598986849188805 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16463897190988064 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04879012703895569 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015719793736934662 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22791908122599125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0692899338901043 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5081451274454594 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0788089819252491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17544906586408615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.055999960750341415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42399787344038486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038479920476675034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08279015310108662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480895146727562 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16180891543626785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048640184104442596 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015420140698552132 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.227669021114707 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06989017128944397 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5460250433534384 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16109994612634182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047640176489949226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42053801007568836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03860006108880043 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0684601254761219 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07437984459102154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16080890782177448 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047999899834394455 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015770085155963898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22812909446656704 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06948993541300297 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4974139630794525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22704899311065674 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3362288698554039 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.052533000707626 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07568905130028725 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16542919911444187 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04904996603727341 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.421679113060236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048209214583039284 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0715500209480524 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07629883475601673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16390904784202576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049170106649398804 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016139820218086243 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22830883972346783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0716100912541151 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5365851577371359 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489905692636967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16479892656207085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0538600143045187 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.092698959633708 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20259013399481773 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995900861918926 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2101899590343237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0906200148165226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10429020039737225 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21075899712741375 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1313299871981144 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3909091465175152 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.263876212760806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05122995935380459 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6866240184754133 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07625017315149307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1662890426814556 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41978899389505386 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06819912232458591 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07668021135032177 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16529997810721397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049459049478173256 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22598984651267529 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07057981565594673 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.516534946858883 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07397006265819073 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1627798192203045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05383999086916447 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09373994544148445 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2104791346937418 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09129010140895844 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20941905677318573 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08845911361277103 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10440009646117687 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20978995598852634 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13091904111206532 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.387219013646245 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.266595907509327 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05063996650278568 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6854749992489815 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07620989345014095 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1663400325924158 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048819929361343384 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3213391173630953 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03964896313846111 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07108994759619236 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074049923568964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16128900460898876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048590125516057014 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01864996738731861 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23019895888864994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05879998207092285 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.411524834111333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434911094605923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16129901632666588 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05321996286511421 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09238999336957932 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20163902081549168 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10661897249519825 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21001999266445637 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900882676243782 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10394002310931683 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22529903799295425 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13267993927001953 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4058191552758217 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3007060624659061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05091913044452667 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7154638189822435 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22696913219988346 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31130900606513023 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.083816945552826 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13115908950567245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2221090253442526 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0528399832546711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.322198960930109 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03955001011490822 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06787898018956184 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459986954927444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16084988601505756 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047798966988921165 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015260186046361923 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12996001169085503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06008986383676529 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3592960312962532 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445993833243847 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16190996393561363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042899977415800095 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09250896982848644 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.190479913726449 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986983448266983 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19130902364850044 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983002044260502 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11058896780014038 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2628990914672613 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08823000825941563 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39782910607755184 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2379460968077183 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0402799341827631 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6345051117241383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09892997331917286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.189649173989892 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049530062824487686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2254289574921131 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03921985626220703 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06858003325760365 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745188444852829 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16176910139620304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047770095989108086 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015530036762356758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13081892393529415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0488900113850832 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.218416029587388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444992661476135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16110902652144432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04321988672018051 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09129010140895844 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19034906290471554 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08839997462928295 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19286898896098137 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08868006989359856 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10366993956267834 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24444982409477234 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08828891441226006 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38031814619898796 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2145559303462505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04061986692249775 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6108851414173841 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07867999374866486 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16697891987860203 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0485200434923172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17820904031395912 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03945990465581417 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06817001849412918 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415004074573517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16026897355914116 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015059951692819595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13035908341407776 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042839907109737396 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1360659264028072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08174893446266651 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1692690420895815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04247017204761505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09252992458641529 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18970901146531105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08964980952441692 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1918300986289978 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08788891136646271 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10363990440964699 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24397997185587883 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08796993643045425 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3791491035372019 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2158658355474472 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040010083466768265 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6184551641345024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23421901278197765 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3515179269015789 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.1497499961406 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.22096908651292324 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37911906838417053 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10580895468592644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17916993238031864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06828899495303631 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08701998740434647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17441995441913605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048138899728655815 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01574005000293255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08351006545126438 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047369860112667084 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3966760598123074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08461996912956238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17845002003014088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03751995973289013 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09418884292244911 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5498279351741076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09305006824433804 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19148900173604488 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09059999138116837 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1282789744436741 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35576801747083664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0671599991619587 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4716981202363968 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6783240716904402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03472995012998581 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.084502950310707 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1630301121622324 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2767988480627537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07365993224084377 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13078004121780396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04005897790193558 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06804009899497032 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08958904072642326 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19407900981605053 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05328003317117691 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015810132026672363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08262996561825275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03919913433492184 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2253960594534874 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08520996198058128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17448002472519875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03702007234096527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09355996735394001 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5631980020552874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006913751363754 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20074006170034409 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909990631043911 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1278200652450323 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35619898699223995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06680004298686981 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47134910710155964 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6942149959504604 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03473018296062946 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0921030081808567 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12271897867321968 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23617898114025593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07390999235212803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1308091450482607 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04011997953057289 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06900983862578869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08331914432346821 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1714089885354042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04943995736539364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0823799055069685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039268983528018 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1556860990822315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08253008127212524 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18135993741452694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042348867282271385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10660989210009575 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.551378121599555 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169010445475578 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22360985167324543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09033898822963238 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12708990834653378 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35592890344560146 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843009032309055 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47315889969468117 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7246350180357695 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03485986962914467 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.144862897694111 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.33392595127225 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08535012602806091 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.026300083845853806 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.15196902677416801 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.356039019301534 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19591907039284706 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.049830181524157524 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04284013994038105 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025690067559480667 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0367690809071064 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2510589547455311 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07041986100375652 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06863009184598923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15686918050050735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05043996497988701 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0823501031845808 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04733004607260227 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07574912160634995 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08572987280786037 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.179939903318882 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05049002356827259 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016819918528199196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08245999924838543 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9895069524645805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06524892523884773 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15120906755328178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03674998879432678 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09551900438964367 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5748390685766935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10338006541132927 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19583897665143013 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09476998820900917 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12716883793473244 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3532890696078539 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06731902249157429 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4668780602514744 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7173439264297485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03454997204244137 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.104602986946702 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08440902456641197 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17255917191505432 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04982994869351387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08097989484667778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03925897181034088 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06721005775034428 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08356990292668343 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17061899416148663 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04942994564771652 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01610000617802143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08189002983272076 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.945917097851634 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08283904753625393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16960897482931614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03706011921167374 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11077895760536194 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5626881029456854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11778995394706726 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.24206913076341152 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09053992107510567 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12646010145545006 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35185785964131355 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06677000783383846 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4644980654120445 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7895440105348825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033880118280649185 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1851928904652596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07719011045992374 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10290904901921749 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.432739086449146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07757009007036686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16836891882121563 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792003892362118 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07962994277477264 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03942986950278282 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06730901077389717 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07401010952889919 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15752995386719704 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795985296368599 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015168916434049606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12909993529319763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03133993595838547 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0171469766646624 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370905950665474 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15844893641769886 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041659921407699585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09118905290961266 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18869992345571518 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08810893632471561 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19022985361516476 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08776900358498096 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10278006084263325 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24206889793276787 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08709984831511974 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3742189146578312 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.199045917019248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0395399983972311 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5834649093449116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581990212202072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16190996393561363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04818011075258255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12953905388712883 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703993909060955 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06720004603266716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355981506407261 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1584689598530531 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825997166335583 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12964894995093346 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0451271664351225 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0737698283046484 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599190291017294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04213000647723675 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065982885658741 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18833903595805168 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08908985182642937 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19067898392677307 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08803000673651695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1035898458212614 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24378998205065727 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08760904893279076 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.376309035345912 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2022159062325954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03908993676304817 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5897450502961874 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1254791859537363 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15179882757365704 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.490141920745373 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07700896821916103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16705901362001896 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12777000665664673 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04650908522307873 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07495004683732986 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07573887705802917 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16795890405774117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048579880967736244 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22569880820810795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03672996535897255 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.212715869769454 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07507903501391411 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16141892410814762 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05185999907553196 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09148893877863884 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20184996537864208 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08921907283365726 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21722912788391113 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029009379446507 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10405993089079857 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20860903896391392 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1303099561482668 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3846888430416584 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2546859215945005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05037989467382431 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6650240868330002 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07580011151731014 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215001232922077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048198970034718513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22382009774446487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03825896419584751 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06720004603266716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07397984154522419 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582489348948002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047269975766539574 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015190104022622108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22464897483587265 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1993160005658865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320009171962738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15958002768456936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05286885425448418 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10816007852554321 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2170591615140438 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08890009485185146 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20829890854656696 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08911988697946072 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10270997881889343 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.223289942368865 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13605901040136814 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40819798596203327 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.305395970121026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049869995564222336 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7166249454021454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22154883481562138 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24765916168689728 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.142640020698309 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07697008550167084 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16730884090065956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892004653811455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22417912259697914 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03891997039318085 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.067130196839571 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07461011409759521 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15999982133507729 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04837010055780411 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015189871191978455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22540916688740253 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2105961795896292 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364898920059204 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15835906378924847 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04860991612076759 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22367900237441063 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039190053939819336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0671599991619587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07461011409759521 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159549992531538 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048930058255791664 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015249941498041153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22515910677611828 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.199816120788455 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4541819002479315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357983849942684 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601390540599823 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796008579432964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22398913279175758 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03895978443324566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702006794512272 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07773004472255707 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17310003750026226 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05856994539499283 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018588965758681297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22916006855666637 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2414960656315088 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489998824894428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16333023086190224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0668899156153202 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09229988791048527 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21739909425377846 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08989009074866772 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20784884691238403 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08846982382237911 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09693903848528862 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2007889561355114 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13122009113430977 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3767581656575203 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.260286197066307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04935986362397671 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6883050557225943 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0763989519327879 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1628489699214697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875008016824722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22378889843821526 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03866991028189659 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670398585498333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418892346322536 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15882891602814198 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0489500816911459 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015500001609325409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2253290731459856 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.205255975946784 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.229386104270816 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07493910379707813 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16091903671622276 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41700899600982666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04425994120538235 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06717001087963581 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07468997500836849 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067898832261562 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048740068450570107 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01532980240881443 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2254790160804987 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0688300933688879 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4979050029069185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08927891030907631 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18349895253777504 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04697986878454685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41620805859565735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03853999078273773 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06684008985757828 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0856888946145773 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18136901780962944 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.058519886806607246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018349848687648773 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22486899979412556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0684000551700592 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5458448324352503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428019307553768 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15983008779585361 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04713982343673706 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41680806316435337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879005089402199 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0668501015752554 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412908598780632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15829899348318577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04820013418793678 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01516006886959076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22478890605270863 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06813998334109783 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.476044999435544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22182008251547813 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3002490848302841 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.981464007869363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480988278985023 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16225012950599194 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0654999166727066 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4169279709458351 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915000706911087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06821006536483765 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459986954927444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598589587956667 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04751002416014671 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015180092304944992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22514909505844116 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06897002458572388 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.500885933637619 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335003465414047 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15847990289330482 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05258014425635338 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09066890925168991 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19980012439191341 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08903001435101032 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20673894323408604 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08792011067271233 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10253000073134899 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20676897838711739 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1292899250984192 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.381238991394639 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2329260352998972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04973006434738636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6393838450312614 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751300249248743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16156909987330437 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4165489226579666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842986188828945 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06691901944577694 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07468997500836849 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1688101328909397 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048748916015028954 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015410128980875015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22567994892597198 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06936001591384411 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4964649453759193 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418007589876652 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16026897355914116 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05221995525062084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028008207678795 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.197479035705328 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08891010656952858 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20692916586995125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0882190652191639 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10421988554298878 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2069501206278801 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12928899377584457 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38017891347408295 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.229176064953208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05009002052247524 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6376450657844543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757100060582161 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16228994354605675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0489500816911459 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3205779939889908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869016654789448 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0677499920129776 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07505901157855988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16094883903861046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0478401780128479 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01532980240881443 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22600917145609856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05826982669532299 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.37967593036592 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475889287889004 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16033882275223732 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05258014425635338 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09091920219361782 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20788982510566711 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08897902444005013 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.205939169973135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08854013867676258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10344013571739197 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20715896971523762 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1307101920247078 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3824490122497082 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2438660487532616 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050099100917577744 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6515138559043407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22641895338892937 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31060888431966305 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.825496934354305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12972881086170673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2177390269935131 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052720075473189354 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32155890949070454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039389822632074356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06781890988349915 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419008761644363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15903986059129238 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04825904034078121 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015189871191978455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12952997349202633 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058820005506277084 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3460859190672636 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733998604118824 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1579697709530592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04230998456478119 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09163911454379559 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18998002633452415 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884300097823143 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18971995450556278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1058101188391447 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10416004806756973 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24320906959474087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08714990690350533 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37496909499168396 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2199960183352232 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039828941226005554 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6063849907368422 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09129010140895844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.178460031747818 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22564991377294064 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03855000250041485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06682006642222404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07328996434807777 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15704892575740814 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737009294331074 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015829922631382942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13004988431930542 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04801899194717407 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1879659723490477 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07417984306812286 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159720191732049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04176911897957325 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0910500530153513 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18931902013719082 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08856016211211681 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1894589513540268 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08761999197304249 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10372907854616642 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24262908846139908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08687004446983337 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3746792208403349 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.198675949126482 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039380043745040894 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5868449117988348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07794983685016632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16540917567908764 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843994975090027 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1766399946063757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06773998029530048 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07319008000195026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15805894508957863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.056609977036714554 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015169847756624222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12978003360331059 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04215980879962444 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.143686007708311 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602000556886196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04180893301963806 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09089987725019455 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18753902986645699 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08848006837069988 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18939911387860775 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08888007141649723 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10387902148067951 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24263886734843254 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08741999045014381 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37517910823225975 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.199736027047038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03937003202736378 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5887951012700796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23368909023702145 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3514289855957031 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.023560909554362 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2183699980378151 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3715190105140209 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10354002006351948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17773895524442196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03894977271556854 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06737001240253448 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08421903476119041 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16966904513537884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048620160669088364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08196895942091942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051679788157343864 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3607849832624197 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08341018110513687 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1702390145510435 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03699003718793392 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09174016304314137 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5476579535752535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08962000720202923 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18780888058245182 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08919904939830303 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1268500927835703 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3520089667290449 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06604986265301704 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4629790782928467 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6466551460325718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03379001282155514 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.032042946666479 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1621600240468979 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27363887056708336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07356004789471626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13007991947233677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939890302717686 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06724009290337563 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0831501092761755 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677891705185175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04818011075258255 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08164998143911362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03864988684654236 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1668168008327484 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08343998342752457 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16836985014379025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036499928683042526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197881445288658 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.551487784832716 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09054993279278278 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18981890752911568 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08838996291160583 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12647896073758602 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3515779972076416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06631994619965553 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4627180751413107 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6536340117454529 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033739954233169556 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0407040137797594 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12137996964156628 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2332900185137987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07319892756640911 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12974999845027924 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039430102333426476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06719911471009254 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08329004049301147 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16813003458082676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048909103497862816 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015430152416229248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08338992483913898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03848015330731869 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1282269842922688 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08279993198812008 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16945903189480305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03669015131890774 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09205890819430351 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5487478338181973 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0894500408321619 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18866895698010921 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09590992704033852 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12722890824079514 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35212887451052666 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0667900312691927 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4634079523384571 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6555041074752808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03396999090909958 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.041453029960394 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.994377149268985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08499901741743088 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025240005925297737 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11117011308670044 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.75241493433714 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19952887669205666 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04710000939667225 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04130997695028782 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02478901296854019 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03603985533118248 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.24261907674372196 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06836000829935074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06658886559307575 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1545790582895279 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0504001509398222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08244020864367485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04063895903527737 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06757001392543316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08354010060429573 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17071887850761414 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0491398386657238 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01643993891775608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08233985863626003 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9443871676921844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06242003291845322 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14871894381940365 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036770012229681015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0936589203774929 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5515189841389656 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09170989505946636 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19630882889032364 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048986248672009 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1272689551115036 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3531991969794035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06742984987795353 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46727806329727173 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6734739765524864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03428012132644653 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0593630615621805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08330913260579109 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1733491662889719 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05037989467382431 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08076010271906853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039739999920129776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06713904440402985 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08367002010345459 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1704399473965168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04970910958945751 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016110017895698547 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08208001963794231 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9479068685323 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08240877650678158 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17067906446754932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0370098277926445 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09411992505192757 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5479389801621437 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029894135892391 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19270996563136578 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978997357189655 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1262701116502285 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.351558905094862 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06944010965526104 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4827589727938175 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6783641185611486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03423006273806095 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.068963134661317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0776899978518486 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10492000728845596 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.2320600263774395 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07763993926346302 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16950001008808613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05014985799789429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07983902469277382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04017003811895847 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0676899217069149 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07575913332402706 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1619888935238123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049339840188622475 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0159598421305418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12947898358106613 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03190990537405014 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0330670047551394 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07398007437586784 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612398773431778 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0420399010181427 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09233993478119373 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18937909044325352 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09052013047039509 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19658892415463924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986005559563637 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10343012399971485 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24423887953162193 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08691893890500069 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3773481585085392 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.217446057125926 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04000007174909115 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6094050370156765 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07606903091073036 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16298913396894932 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04930002614855766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.128990039229393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039980048313736916 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06727990694344044 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07481011562049389 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16143894754350185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04819990135729313 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016050180420279503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12959004379808903 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.025536097586155 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07334980182349682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598801463842392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04197005182504654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09255879558622837 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18962007015943527 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944002911448479 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19180914387106895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08908985182642937 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10379008017480373 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2444989513605833 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08798995986580849 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37846900522708893 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2128159869462252 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039659906178712845 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6037351451814175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12539001181721687 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15300000086426735 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.529402056708932 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07810001261532307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1705398317426443 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05005998536944389 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12852903455495834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04032999277114868 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0727800652384758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07558008655905724 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610091421753168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04855007864534855 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016059959307312965 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2257989253848791 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0370698980987072 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1878660880029202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16197911463677883 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052679795771837234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09300909005105495 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20270003005862236 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09116996079683304 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2248890232294798 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09124004282057285 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10416912846267223 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20933919586241245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13025011867284775 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3855389077216387 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.271605957299471 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049999915063381195 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6861441545188427 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07665017619729042 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16499916091561317 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04970002919435501 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2239991445094347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039499951526522636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06758002564311028 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07561012171208858 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16229902394115925 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04955008625984192 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01600012183189392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22528902627527714 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2205059174448252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07462012581527233 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16120914369821548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052800169214606285 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09103002957999706 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19975914619863033 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09020022116601467 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2091890200972557 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08951989002525806 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10288995690643787 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20839995704591274 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13045896776020527 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38499897345900536 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2496961280703545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050449976697564125 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.663394970819354 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22169901058077812 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24984893389046192 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.111009977757931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07816986180841923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17100898548960686 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05013006739318371 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22466899827122688 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04016980528831482 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06750901229679585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538986392319202 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1623800490051508 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04970002919435501 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015929806977510452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22549997083842754 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2265949044376612 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16095000319182873 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04929001443088055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22420892491936684 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040079932659864426 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06686896085739136 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07512001320719719 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16200006939470768 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049868831411004066 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016180099919438362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2254289574921131 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.21638597920537 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4876208044588566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07584993727505207 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16564014367759228 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04961993545293808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22463896311819553 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04009995609521866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06791017949581146 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475912570953369 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16182893887162209 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015129800885915756 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2252291887998581 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.217806013301015 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08295918814837933 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1723291352391243 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05279993638396263 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078998118638992 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1917700283229351 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08921907283365726 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20827981643378735 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08828914724290371 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09685009717941284 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.1998299267143011 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13101007789373398 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37515885196626186 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.223286148160696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04980992525815964 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.645403914153576 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07583014667034149 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215001232922077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048339832574129105 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22416887804865837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906991332769394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06637908518314362 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15901005826890469 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048659974709153175 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015559839084744453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22501987405121326 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2038969434797764 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.162546014413238 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07572001777589321 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622301060706377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04794890992343426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41720899753272533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842008300125599 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06743986159563065 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07464992813766003 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1617399975657463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04864996299147606 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015399884432554245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23985886946320534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0691788736730814 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.50047498755157 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07478008046746254 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16056979075074196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04735891707241535 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41669909842312336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038590049371123314 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06665010005235672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376004941761494 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15770993195474148 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22534909658133984 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06852997466921806 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4768352266401052 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408997043967247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16039004549384117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168478772044182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039090169593691826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06618001498281956 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07440987974405289 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15921914018690586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04856986925005913 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01532980240881443 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22502895444631577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06841984577476978 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4804359525442123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22147013805806637 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3008190542459488 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.9105738289654255 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07372000254690647 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594198402017355 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.056379009038209915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4165989812463522 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903987817466259 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06714998744428158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541989907622337 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16057980246841908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04763994365930557 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01500011421740055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22531882859766483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06887898780405521 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.488545211032033 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734599307179451 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15900982543826103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052530085667967796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09058997966349125 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1992390025407076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884300097823143 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20663905888795853 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08777016773819923 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10267901234328747 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20686909556388855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13028993271291256 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3815791569650173 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2318158987909555 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04960992373526096 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6388250514864922 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07509905844926834 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16133906319737434 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04784995689988136 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168190062046051 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039319973438978195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06810016930103302 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0746299047023058 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15886989422142506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050419941544532776 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015430152416229248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22543896920979023 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06883987225592136 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5006051398813725 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07470021955668926 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604489516466856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0524700153619051 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030010551214218 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19774911925196648 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08861999958753586 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20740902982652187 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08785910904407501 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10406016372144222 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20680995658040047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1296689733862877 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3809989430010319 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2299360241740942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049720052629709244 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6376248095184565 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07509998977184296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621099654585123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0487098004668951 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32060896046459675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03909901715815067 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06799004040658474 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738298986107111 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15866989269852638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22566900588572025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058420002460479736 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3769450597465038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07436005398631096 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16086897812783718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05219993181526661 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167985990643501 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20943908020853996 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08839997462928295 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2076991368085146 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910894393920898 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10396004654467106 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2079398836940527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1289991196244955 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38138916715979576 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2453862000256777 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05000992678105831 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.653734128922224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2259989269077778 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3100989852100611 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.81572805903852 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1291788648813963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21794880740344524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052830204367637634 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3206490073353052 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03847014158964157 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820890121161938 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07356982678174973 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15773996710777283 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0478189904242754 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015230150893330574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295800320804119 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058510107919573784 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3406658545136452 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07297005504369736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15748990699648857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04191999323666096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09092898108065128 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18843007273972034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08809007704257965 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1887890975922346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0966100487858057 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10390905663371086 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2438090741634369 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08678995072841644 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3753190394490957 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.207795925438404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0392599031329155 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5929450746625662 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09149010293185711 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17910893075168133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04855985753238201 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22472906857728958 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03884988836944103 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06677000783383846 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07278891280293465 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15732902102172375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04756986163556576 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015360070392489433 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13002892956137657 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048720045015215874 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1906260624527931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335981354117393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15962007455527782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04155002534389496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09115994907915592 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18862891010940075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0885501503944397 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18996908329427242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08763978257775307 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10335911065340042 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2422388643026352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08671008981764317 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37329806946218014 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1968659237027168 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03968016244471073 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5863650478422642 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07744901813566685 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16348902136087418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047919806092977524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17647887580096722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03864988684654236 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06663007661700249 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07314886897802353 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15800888650119305 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015219906345009804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12969900853931904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04190998151898384 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.122236019000411 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07290998473763466 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15702005475759506 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041828956454992294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09168009273707867 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18702889792621136 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08859997615218163 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18923915922641754 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08774013258516788 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10382919572293758 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24314899928867817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08758995682001114 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3757281228899956 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.199045917019248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03891997039318085 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5844849403947592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23413891904056072 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.35208906047046185 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.979411097243428 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21935999393463135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3722989931702614 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10382011532783508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17746887169778347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039560021832585335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06747012957930565 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08314987644553185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16848905943334103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047759851440787315 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08188909851014614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04327995702624321 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3574960175901651 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08380995132029057 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17049908638000488 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037109944969415665 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09089987725019455 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5481680855154991 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08943001739680767 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18726009875535965 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08883909322321415 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12683006934821606 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.352329108864069 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06623007357120514 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46320888213813305 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6448050737380981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033980002626776695 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0317528396844864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16156001947820187 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2724691294133663 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07310998626053333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12951018288731575 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03939890302717686 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670100562274456 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08283997885882854 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16783899627625942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04864996299147606 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015590107068419456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08160993456840515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038090161979198456 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1652270331978798 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08377991616725922 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16923993825912476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03637000918388367 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0899790320545435 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.547527801245451 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19007897935807705 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08931988850235939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12647919356822968 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35245902836322784 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06612902507185936 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4630880430340767 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6489140689373016 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0332400668412447 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0332441199570894 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11808914132416248 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2298590261489153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1289891079068184 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039460137486457825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06697000935673714 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08348003029823303 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16797007992863655 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048419926315546036 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01553981564939022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0817989930510521 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0380899291485548 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.123795984312892 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08251983672380447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1677689142525196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036430079489946365 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09076995775103569 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5477180238813162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895399134606123 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18888991326093674 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0880989246070385 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12654997408390045 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3528690431267023 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736000068485737 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4671690985560417 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.648514997214079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03397907130420208 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0313130225986242 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.96674713678658 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08686003275215626 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.0252497848123312 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11359900236129761 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.368195943534374 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19261916168034077 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04977989010512829 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04226993769407272 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025480054318904877 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03733998164534569 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.250769080594182 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07039983756840229 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06551877595484257 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15481887385249138 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05029002204537392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08244998753070831 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.041010091081261635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06862008012831211 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08410005830228329 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17150002531707287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04988000728189945 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016579870134592056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08267001248896122 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9471771772950888 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06299884989857674 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15048915520310402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03680982626974583 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09302003309130669 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5505788139998913 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09123887866735458 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20834896713495255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071012027561665 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12677907943725586 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3530790563672781 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06738887168467045 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46698818914592266 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6801140736788511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034449854865670204 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.064622938632965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08393893949687481 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17293891869485378 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05009002052247524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08137989789247513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039428938180208206 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0673800241202116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08402997627854347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17202901653945446 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04893983714282513 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01598987728357315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08227000944316387 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9497669525444508 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08197990246117115 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16885995864868164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03674998879432678 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09165890514850616 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5499981343746185 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09027007035911083 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19125919789075851 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08999998681247234 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12666895054280758 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3522280603647232 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06652995944023132 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46433787792921066 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6584538388997316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03409991040825844 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.05304310657084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07861992344260216 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10569905862212181 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.226029014214873 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07757009007036686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16897893510758877 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049350084736943245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08001993410289288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399099662899971 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06693904288113117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419008761644363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922007150948048 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488299410790205 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015900004655122757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12883986346423626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03163982182741165 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0271670762449503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07453002035617828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16186898574233055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042349798604846 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09206891991198063 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18925010226666927 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09068893268704414 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19180006347596645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08947914466261864 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10296003893017769 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24252897128462791 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08721998892724514 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3757788799703121 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2105058412998915 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03951997496187687 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6022040508687496 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07573002949357033 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16411999240517616 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048980116844177246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1289290376007557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025991074740887 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06722006946802139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418985478579998 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922985039651394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048430170863866806 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01604994758963585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12901891022920609 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0233970824629068 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08761999197304249 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17517898231744766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04235981032252312 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09278999641537666 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18966919742524624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972990326583385 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19224896095693111 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905003778636456 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10410998947918415 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24521001614630222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08814898319542408 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3786790184676647 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2160558253526688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03946991637349129 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6242649871855974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1252889633178711 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15267892740666866 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.533631891012192 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07760990411043167 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16875891014933586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1279700081795454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03961008042097092 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06767897866666317 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0756201334297657 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1613700296729803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049099093303084373 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016220146790146828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22569880820810795 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03688992001116276 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.178276026621461 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16215001232922077 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05252892151474953 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09162002243101597 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20021898671984673 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067007340490818 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21045911125838757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0988899264484644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10470999404788017 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21006003953516483 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13132905587553978 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3872280940413475 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2625160161405802 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050350092351436615 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.67729496024549 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07650908082723618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16576889902353287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922994412481785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22405898198485374 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963988274335861 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06719003431499004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07485016249120235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16228994354605675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04887999966740608 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016220146790146828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22573908790946007 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2197261676192284 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376004941761494 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604489516466856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05286000669002533 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09211897850036621 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20007998682558537 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08979905396699905 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.209389952942729 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089599983766675 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10280008427798748 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20826887339353561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13050995767116547 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3849789500236511 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2488660868257284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05036895163357258 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6605339478701353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22132904268801212 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24867919273674488 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.088189780712128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07780012674629688 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17810985445976257 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051039038226008415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22426992654800415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04078890196979046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06751017645001411 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07579009979963303 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16269879415631294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04978012293577194 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015979865565896034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22558891214430332 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.237556105479598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400009781122208 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16112998127937317 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04953891038894653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2240799367427826 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040138838812708855 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06780005060136318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610389444977045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04963995888829231 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01592002809047699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22551906295120716 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2148360256105661 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4980218149721622 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07499987259507179 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1640289556235075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04950002767145634 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22431905381381512 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040140002965927124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765010766685009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07507903501391411 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16221916303038597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04957010969519615 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016120029613375664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2253891434520483 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2195759918540716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07537985220551491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16594910994172096 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052819959819316864 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1076599583029747 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19498891197144985 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09088893420994282 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20718993619084358 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08956901729106903 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09705987758934498 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20203902386128902 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13082008808851242 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37903920747339725 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2545359786599874 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049588968977332115 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6732439398765564 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07691001519560814 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16796006821095943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0496390275657177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22382009774446487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03968901000916958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06702984683215618 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480010390281677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610389444977045 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04968000575900078 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015999889001250267 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22546900436282158 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.222376013174653 ms for forwarding
<class 'diffusers.models.unet_2d_blocks.UNetMidBlock2DCrossAttn'> take 4.211616003885865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07535889744758606 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16421894542872906 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04884996451437473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4173589404672384 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04042009823024273 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06770994514226913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477984763681889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16122893430292606 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049320049583911896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0160199124366045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22573885507881641 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0693202018737793 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4990950003266335 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0752198975533247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16782921738922596 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04933006130158901 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4169279709458351 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039869919419288635 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0677499920129776 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07484899833798409 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16123894602060318 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049919821321964264 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015429919585585594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22508902475237846 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0687099527567625 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.500575104728341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457984611392021 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16234908252954483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04890980198979378 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41748792864382267 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978004679083824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06720004603266716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073959119617939 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15993905253708363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04943995736539364 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583993434906006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22546900436282158 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06876001134514809 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4935650397092104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22176909260451794 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3016488626599312 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.9513038247823715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07599988020956516 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16404013149440289 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04922994412481785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4171980544924736 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04171999171376228 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820983253419399 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492885924875736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1618389505892992 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894006997346878 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016189878806471825 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22602896206080914 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06982986815273762 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.504465937614441 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16126991249620914 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05307001993060112 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09202887304127216 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20022992976009846 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029987268149853 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20803906954824924 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0899098813533783 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10313885286450386 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20863907411694527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13049994595348835 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3849291242659092 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2473959941416979 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05023018456995487 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6615439672023058 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07531000301241875 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1638189423829317 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04849000833928585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168690647929907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03975001163780689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06781890988349915 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07403991185128689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16106991097331047 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04912912845611572 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24072988890111446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06946991197764874 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5117048751562834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07412000559270382 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16136886551976204 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05304999649524689 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09242002852261066 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20012911409139633 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09108986705541611 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20854896865785122 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944002911448479 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10417005978524685 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20937994122505188 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1303588505834341 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38515892811119556 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2484758626669645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05059991963207722 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6628250014036894 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07594004273414612 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16545015387237072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04982994869351387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32140896655619144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04043895751237869 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06798002868890762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0753400381654501 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16203895211219788 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048350077122449875 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016039935871958733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22587901912629604 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05855993367731571 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3944751117378473 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634990330785513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05259993486106396 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09241001680493355 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21126912906765938 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08988985791802406 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20874012261629105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09025889448821545 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10457984171807766 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20956993103027344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1305688638240099 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3863389138132334 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2626759707927704 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05061016418039799 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6788148786872625 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2266291994601488 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3112789709120989 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.940746938809752 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12962007895112038 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21946895867586136 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.053629977628588676 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.321368919685483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040140002965927124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06778002716600895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396982982754707 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604701392352581 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810001701116562 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015570083633065224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1298289280384779 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05892990157008171 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3556659687310457 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07434887811541557 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16112904995679855 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042530009523034096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09267893619835377 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19119004718959332 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090078916400671 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19119982607662678 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09680911898612976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10439986363053322 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24421908892691135 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.086979940533638 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3767788875848055 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.221825834363699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03977888263761997 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6150639858096838 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09116996079683304 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18037017434835434 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04985881969332695 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22513000294566154 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03960984759032726 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06738980300724506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594990026205778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048579880967736244 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015869969502091408 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1300990115851164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04916987381875515 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2123859487473965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333001121878624 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16023917123675346 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04167994484305382 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177019819617271 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18948898650705814 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895399134606123 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19146897830069065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08950009942054749 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10359007865190506 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24410011246800423 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08702906779944897 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3767090383917093 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2106560170650482 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03996002487838268 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.602324889972806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552001625299454 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16469997353851795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049579888582229614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17680879682302475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039670150727033615 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06868899799883366 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07378007285296917 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1609900500625372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015958910807967186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12983987107872963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04223012365400791 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1514269281178713 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298006676137447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16005896031856537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041950028389692307 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09172898717224598 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18792995251715183 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08907890878617764 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19116979092359543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08870894089341164 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10325992479920387 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24326913990080357 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08773012086749077 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37701893597841263 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2082858011126518 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03941007889807224 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5988938976079226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2344090025871992 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3532890696078539 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.104389930143952 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2196191344410181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37271901965141296 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10340893641114235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17790007404983044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025897942483425 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06768014281988144 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08335011079907417 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17085904255509377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04894984886050224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016279984265565872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0854399986565113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04475004971027374 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3688351027667522 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0830299686640501 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1706599723547697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03708992153406143 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0926600769162178 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5488379392772913 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09070010855793953 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19005895592272282 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944980800151825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12727896682918072 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3539589233696461 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06606918759644032 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46604801900684834 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6576240304857492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03374996595084667 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0473841577768326 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16186991706490517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27499906718730927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07334910333156586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12977002188563347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04071998409926891 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728898733854294 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08423998951911926 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17072982154786587 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04979991354048252 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01583993434906006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08215010166168213 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038640107959508896 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1818460188806057 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08341902866959572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1706387847661972 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0366999302059412 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254994802176952 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5483988206833601 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0910591334104538 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19109994173049927 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09005004540085793 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1268298365175724 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35368907265365124 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655999459326267 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4666089080274105 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6602238174527884 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03424007445573807 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0486030261963606 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11990917846560478 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2323391381651163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07339008152484894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12959912419319153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04067993722856045 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06743008270859718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08439901284873486 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1707191113382578 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04934985190629959 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015819910913705826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08188001811504364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03907014615833759 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1381960939615965 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08316012099385262 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17119012773036957 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03691995516419411 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09264983236789703 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5489778704941273 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089969951659441 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18986896611750126 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09280000813305378 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1269201748073101 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3527090884745121 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06704009138047695 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46523893252015114 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6745149623602629 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034289900213479996 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0648231729865074 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.076456936076283 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08773012086749077 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.025990186259150505 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.11319899931550026 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 59.812174178659916 ms for forwarding
