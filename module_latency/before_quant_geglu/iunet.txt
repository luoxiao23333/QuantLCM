--------------------
No. 1
<class 'diffusers.models.embeddings.Timesteps'> take 0.36605808418244123 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.09983999188989401 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.07736997213214636 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.06888003554195166 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04522001836448908 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.41525799315422773 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 1114.8308790288866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.27522898744791746 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.5103580188006163 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07253000512719154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14103995636105537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.043530017137527466 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1420500921085477 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1037700567394495 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19897997844964266 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048110028728842735 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.03663997631520033 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09543902706354856 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6265930607914925 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08154998067766428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17349899280816317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043899985030293465 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.12862903531640768 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.8125760359689593 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09471992962062359 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21494890097528696 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09395903907716274 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1493289601057768 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.44980808161199093 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0749389873817563 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5854869959875941 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 2.1619999315589666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04373001866042614 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.6334590511396527 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09512004908174276 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18578011076897383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049078953452408314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.089599983766675 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03967992961406708 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07519905921071768 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09522901382297277 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2074389485642314 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01585995778441429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09172898717224598 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0293149389326572 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09341898839920759 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18308893777430058 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03930006641894579 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09386905003339052 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6092879921197891 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0928100198507309 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19641895778477192 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09364995639771223 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14363008085638285 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.38360897451639175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0749389873817563 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5102379946038127 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.784341991879046 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03721006214618683 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.202810952439904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09055004920810461 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.12583902571350336 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 7.7820459846407175 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08538994006812572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18629897385835648 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04842004273086786 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09270908776670694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039229984395205975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07957906927913427 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07670000195503235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1675700768828392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047509907744824886 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015829922631382942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1453789882361889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03696000203490257 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1070049367845058 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413909770548344 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17085904255509377 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04549010191112757 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09282899554818869 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21077902056276798 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09222992230206728 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1994089689105749 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08902896661311388 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1152789918705821 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.265478971414268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09752903133630753 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41433796286582947 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.294194022193551 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04424992948770523 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.718243001960218 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07626006845384836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16667903400957584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834006540477276 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1458700280636549 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039319973438978195 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07475004531443119 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07468904368579388 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16398902516812086 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047170091420412064 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015389989130198956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14748901594430208 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0758850257843733 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07484899833798409 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16440905164927244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04585005808621645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09206903632730246 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20606897305697203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08956005331128836 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19804900512099266 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09254901669919491 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.116499955765903 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2748989500105381 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10174908675253391 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42900803964585066 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2979940511286259 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045170076191425323 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.711732940748334 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1441589556634426 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1718390267342329 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.9014150174334645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07778999861329794 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17333903815597296 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04867999814450741 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14736002776771784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039229984395205975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07912004366517067 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07905892562121153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17250911332666874 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015430036000907421 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25585899129509926 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042189029045403004 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2728440342471004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07616996299475431 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1666489988565445 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05953002255409956 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09313900955021381 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23150898050516844 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09023002348840237 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21637906320393085 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09512901306152344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11481891851872206 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22463896311819553 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14710903633385897 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42360799852758646 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3425840297713876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055870041251182556 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7824629321694374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07602002006024122 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16681000124663115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048168934881687164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2567489864304662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039739999920129776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07846904918551445 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07627008017152548 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16761000733822584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04721991717815399 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015170080587267876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2533990191295743 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3088550185784698 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07567997090518475 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16632897313684225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0606500543653965 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09339989628642797 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2130590146407485 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09089906234294176 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21116901189088821 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0928700901567936 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11391995940357447 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22393895778805017 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14570995699614286 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42044802103191614 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.316274981945753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056029995903372765 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7581620486453176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2525890013203025 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.27995905838906765 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.521291914395988 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07777998689562082 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17579994164407253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052360002882778645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2528990153223276 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03978994209319353 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07404899224638939 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08388992864638567 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1727190101519227 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04759989678859711 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015470082871615887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2537689870223403 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.310965046286583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07708999328315258 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16815902199596167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04875008016824722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2523089060559869 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03941007889807224 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07604993879795074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07602991536259651 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16436900477856398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047939945943653584 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015540048480033875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2550490899011493 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2911440571770072 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.6620490243658423 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07648894097656012 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16864901408553123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04880002234131098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2541090361773968 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039809965528547764 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09878003038465977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07626006845384836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1651090569794178 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25475898291915655 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3209839817136526 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07534900214523077 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667890464887023 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06123003549873829 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09304995182901621 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21534902043640614 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09415892418473959 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21184911020100117 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09367999155074358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10644900612533092 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2134989481419325 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14670903328806162 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41212793439626694 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3301749713718891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055468990467488766 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7690419917926192 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076299998909235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1678289845585823 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2514090156182647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03884988836944103 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0732390908524394 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749099999666214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16316899564117193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846008960157633 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015489989891648293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25352893862873316 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2869350612163544 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.534851061180234 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07809000089764595 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1713290112093091 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04792993422597647 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47095795162022114 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039409962482750416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07628893945366144 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07531000301241875 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1705290051177144 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04803994670510292 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015300000086426735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25312905199825764 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07685902528464794 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.622142968699336 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07532991003245115 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16668904572725296 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04713993985205889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47432794235646725 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03932998515665531 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07538893260061741 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07544003892689943 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.164669007062912 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047230045311152935 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2536890096962452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07859896868467331 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6084830276668072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07505994290113449 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1670790370553732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04685996100306511 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4739180440083146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879994619637728 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07343897596001625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07744005415588617 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16975903417915106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159952454268932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25562895461916924 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07933890447020531 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.612953026778996 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.254268990829587 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.43990800622850657 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.487357033416629 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07702899165451527 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1711889635771513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04746008198708296 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47541805543005466 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025001544505358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08527003228664398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07576902862638235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16536901239305735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048230052925646305 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015400000847876072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25411893147975206 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08022005204111338 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6363629838451743 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07770000956952572 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16888906247913837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.059360056184232235 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09679992217570543 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20629900973290205 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09350897744297981 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2120389835909009 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0920899910852313 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11520902626216412 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22981897927820683 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1455990131944418 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4333679098635912 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3312050141394138 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05703896749764681 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.777402008883655 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07630896288901567 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16788893844932318 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04810991231352091 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4752379609271884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04185002762824297 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07771002128720284 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07705902680754662 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17244892660528421 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755996633321047 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595995854586363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2573879901319742 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08091994095593691 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6442229971289635 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07547903805971146 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.168259022757411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06256997585296631 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09671994484961033 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2071290509775281 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09090895764529705 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21156901493668556 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09308999869972467 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11442997492849827 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22713898215442896 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14640996232628822 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42785797268152237 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3213850324973464 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06016995757818222 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.774411997757852 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07709907367825508 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17178896814584732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04885997623205185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36284804809838533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385999446734786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07780897431075573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0758699607104063 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17577898688614368 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04850002005696297 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015280093066394329 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2583189634606242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06613892037421465 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.520432997494936 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07650000043213367 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1721890876069665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060089980252087116 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09327905718237162 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20351901184767485 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09152002166956663 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21029904019087553 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09086902718991041 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11448992881923914 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2256289590150118 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14613894745707512 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.42680802289396524 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3086739927530289 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05614000838249922 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7583930166438222 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2590090734884143 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34882803447544575 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.709213907830417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14091003686189651 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23620901629328728 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05550007335841656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36524899769574404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.043859006837010384 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0819900305941701 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489905692636967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16560894437134266 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475000124424696 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015459954738616943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.15374994836747646 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06877898704260588 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4836930204182863 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1665189629420638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047930050641298294 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09397999383509159 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2011200413107872 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09332993067800999 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1958090579137206 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09007006883621216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11652009561657906 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26740902103483677 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09893998503684998 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4222489660605788 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2922539608553052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04803994670510292 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.720343017950654 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09953894186764956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19103894010186195 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806998185813427 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25332800578325987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038950005546212196 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07623003330081701 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07767998613417149 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16442895866930485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047650071792304516 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015198951587080956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14802999794483185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05472893826663494 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2820940464735031 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07567007560282946 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16564899124205112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.048230052925646305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09922892786562443 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20313006825745106 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0964500941336155 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19281904678791761 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09020895231515169 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11551997158676386 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2670889953151345 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09767001029103994 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41934894397854805 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3154339976608753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044629909098148346 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7392829759046435 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08546002209186554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1773090334609151 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04805997014045715 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19886007066816092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039008911699056625 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07579999510198832 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541896775364876 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1625589793547988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712992813438177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01522002276033163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14646898489445448 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04752993118017912 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2018750421702862 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419008761644363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16380008310079575 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047039007768034935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09169999975711107 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19934901501983404 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09236892219632864 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1881290227174759 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08863990660756826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11547899339348078 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26468909345567226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09985908400267363 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4152379697188735 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.262084930203855 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.046200002543628216 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6921829665079713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2618090948089957 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3878080751746893 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.740298031829298 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23679889272898436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39531791117042303 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10656006634235382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.20095903892070055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039849081076681614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07428997196257114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09506894275546074 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18444890156388283 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04682992585003376 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525005791336298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09234005119651556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047779991291463375 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.456134021282196 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09144993964582682 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18035003449767828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060049002058804035 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09594997391104698 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6034270627424121 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09030906949192286 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20235904958099127 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0941399484872818 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.142168952152133 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3809579648077488 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07756997365504503 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5252569681033492 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7952219350263476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037100049667060375 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.2297599352896214 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17683999612927437 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2921679988503456 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07734994869679213 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14553905930370092 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918004222214222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07556995842605829 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09175995364785194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18055993132293224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09144004434347153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04217901732772589 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.271323999390006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09332899935543537 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18417893443256617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042079947888851166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09229895658791065 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6029480136930943 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09269907604902983 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1909289276227355 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1005600206553936 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14257000293582678 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3815980162471533 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07289997301995754 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5039969692006707 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7689720261842012 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03700004890561104 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1836209343746305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.128990039229393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24605903308838606 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0776100205257535 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14679902233183384 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03804999869316816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07336994167417288 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09218999184668064 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17936003860086203 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047078938223421574 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09422004222869873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04401989281177521 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1989240301772952 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09136996231973171 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1795600401237607 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04138995427638292 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09445997420698404 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6147569511085749 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08963898289948702 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19374897237867117 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08958007674664259 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14245894271880388 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3811080241575837 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07305992767214775 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.502118025906384 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.779453014023602 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03903999458998442 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1907310001552105 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.783083969727159 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09342900011688471 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.027620000764727592 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 5.051978980191052 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 1185.983714996837 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.25341904256492853 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.10329904034733772 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.06733008194714785 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02803897950798273 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.039890059269964695 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.3459880826994777 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.09131000842899084 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08131004869937897 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1747689675539732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04921003710478544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09446998592466116 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03949902020394802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07698999252170324 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09284901898354292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1797290751710534 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04757009446620941 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.017840065993368626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09000999853014946 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9998460300266743 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07116899359971285 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15753903426229954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03951997496187687 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1165090361610055 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6126370280981064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08933001663535833 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19243895076215267 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957902900874615 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14285894576460123 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.38198905531316996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07176992949098349 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49990799743682146 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7941020196303725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03706000279635191 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1989899687469006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09148009121417999 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17872999887913465 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04784995689988136 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08803000673651695 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03861007280647755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07094000466167927 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09192898869514465 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17617910634726286 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04692003130912781 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015320023521780968 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09024899918586016 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9664660319685936 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09071000386029482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17524906434118748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03841007128357887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09109999518841505 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5990179488435388 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09816896636039019 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18810899928212166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08851999882608652 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14056905638426542 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.377457938157022 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07273000665009022 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4971380112692714 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7368029803037643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036319950595498085 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.138370997272432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08635001722723246 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1268989872187376 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.5415819408372045 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07756997365504503 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.169368926435709 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04846998490393162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08790998253971338 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03848003689199686 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0737400259822607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363897748291492 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15720899682492018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04716997500509024 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01537008211016655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14373904559761286 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0332400668412447 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0545660043135285 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0726599246263504 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569890882819891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04692003130912781 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09049999061971903 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.196568900719285 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09928003419190645 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19126897677779198 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08843897376209497 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11439889203757048 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26286905631422997 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09491899982094765 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4046979593113065 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2497040443122387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04233000800013542 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6469130059704185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07690000347793102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18006900791078806 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04851003177464008 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14329003170132637 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03909994848072529 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0713589834049344 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16610906459391117 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0489699887111783 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015759957022964954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14494906645268202 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.075195032171905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07341895252466202 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589189050719142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04454993177205324 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032897651195526 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19817904103547335 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08761999197304249 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18928898498415947 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08800008799880743 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11432997416704893 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26133900973945856 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09539001621305943 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40194892790168524 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.236974960193038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04199007526040077 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.62992300465703 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13896904420107603 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16516889445483685 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.675315973348916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07581990212202072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1666590105742216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04752993118017912 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14258897863328457 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03857002593576908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07160007953643799 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15993905253708363 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731991793960333 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014869030565023422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25239901151508093 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04106003325432539 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2178049655631185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07408903911709785 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16260892152786255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05607004277408123 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09167008101940155 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20285905338823795 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08840998634696007 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20854000467807055 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08954002987593412 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11222902685403824 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21844904404133558 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14469900634139776 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4090979928150773 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2664940441027284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05398003850132227 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6965719405561686 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0753599451854825 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16161997336894274 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724995233118534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2516689710319042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037929974496364594 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07174909114837646 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07402000483125448 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15885999891906977 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04652002826333046 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015030032955110073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2522389404475689 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2554950080811977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07262907456606627 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15822902787476778 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056240009143948555 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09541003964841366 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20073889754712582 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08776003960520029 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22451893892139196 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10192999616265297 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11331902351230383 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22071902640163898 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14426896814256907 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41264796163886786 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.300564967095852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054459087550640106 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7176219262182713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24892902001738548 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.27426902670413256 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.267213029786944 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07609894964843988 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1682490110397339 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04873005673289299 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2515689702704549 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03862893208861351 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07204001303762197 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07347902283072472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16070902347564697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475000124424696 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015050056390464306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25197898503392935 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2688139686360955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735200010240078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15948002692312002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04650908522307873 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2514289226382971 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03833009395748377 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07122999522835016 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07324002217501402 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15674997121095657 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047039007768034935 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015180092304944992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2517990069463849 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.248575048521161 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5631190510466695 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07566891144961119 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16366899944841862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05143997259438038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25145907420665026 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03846909385174513 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07468997500836849 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07352896500378847 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15858898404985666 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712992813438177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25206897407770157 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2811740161851048 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0747699523344636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1655590021982789 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057730008848011494 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.094949034973979 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1942189410328865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08906004950404167 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21032895892858505 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08810893632471561 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10557996574789286 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20854896865785122 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14384998939931393 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39720896165817976 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2554050190374255 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053750001825392246 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.67954305652529 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07454899605363607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16166898421943188 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25101902429014444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03837002441287041 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07152999751269817 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418903987854719 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600090181455016 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779000300914049 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014779972843825817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2520889975130558 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2599349720403552 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.348880960606039 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1625689910724759 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04670990165323019 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4694679519161582 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038139987736940384 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07280893623828888 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07479998748749495 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621099654585123 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047260080464184284 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014989986084401608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25294895749539137 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07491989526897669 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.574613037519157 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07468997500836849 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16098003834486008 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671991337090731 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4702580627053976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03857002593576908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07229996845126152 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588999293744564 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0469390070065856 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014839926734566689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2520090201869607 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07566006388515234 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5671940054744482 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07356994319707155 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15703996177762747 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047348905354738235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47036795876920223 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.040190061554312706 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07189996540546417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07446995005011559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15970005188137293 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046959961764514446 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014869961887598038 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2515689702704549 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07652002386748791 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5771440230309963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24902902077883482 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.33340801019221544 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.2234180038794875 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438997272402048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1619989052414894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731002263724804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47090707812458277 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0386600149795413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07541000377386808 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342989556491375 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1606689766049385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705996252596378 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25321892462670803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07664994336664677 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5855730744078755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741700641810894 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16060902271419764 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05659903399646282 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09073002729564905 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20769902039319277 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09002909064292908 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2182589378207922 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08947006426751614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11312891729176044 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22106908727437258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14345895033329725 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4132479662075639 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2925550108775496 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0546890078112483 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7151519423350692 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07562909740954638 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17243903130292892 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04929001443088055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4699679557234049 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899946957826614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07324898615479469 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07518997881561518 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16198900993913412 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736008122563362 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159952454268932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2536789979785681 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07668905891478062 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.6059629851952195 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07842003833502531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16582896932959557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05819008219987154 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09184901136904955 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2010590396821499 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08741999045014381 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2110289642587304 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016902185976505 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11295999865978956 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22003904450684786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14190992806106806 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4090189468115568 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2769439490512013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05496002268046141 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.706352923065424 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07633899804204702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16477901954203844 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36175805144011974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038280035369098186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0719389645382762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325003389269114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16067002434283495 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047119916416704655 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2556690014898777 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0653399620205164 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4751229900866747 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458007894456387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16183999832719564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057228957302868366 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09126996155828238 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20214903634041548 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09045901242643595 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21132908295840025 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.090269953943789 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11295999865978956 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22053904831409454 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14477898366749287 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41430804412811995 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2810250045731664 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054799020290374756 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7052129842340946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2531090285629034 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3402190050110221 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.351975914090872 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1418789615854621 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2355789765715599 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05531997885555029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3609280101954937 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03848993219435215 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07450999692082405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0737400259822607 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15768001321703196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04643900319933891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014789984561502934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14451995957642794 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06481900345534086 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4272339176386595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07283000741153955 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16874901484698057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04712003283202648 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09246892295777798 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19753910601139069 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09064003825187683 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1875490415841341 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08642999455332756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11402997188270092 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2617990830913186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09553006384521723 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40531891863793135 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.237874967046082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04413002170622349 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6520629869773984 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10099902283400297 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18905906472355127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737998824566603 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25270902551710606 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03917899448424578 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07135001942515373 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386901415884495 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15736895147711039 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047200010158121586 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015060068108141422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 4.032531986013055 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05461997352540493 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 5.158007028512657 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08102995343506336 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1756900455802679 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.046600005589425564 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09846000466495752 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20444998517632484 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08983002044260502 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19030901603400707 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10138005018234253 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11755991727113724 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.27118902653455734 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09707000572234392 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.41898898780345917 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2912850361317396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04463992081582546 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7196129774674773 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08526910096406937 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17414893954992294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049749971367418766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19797903951257467 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879994619637728 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07526890840381384 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413991261273623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599290408194065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04663004074245691 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016910023987293243 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1448789844289422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.046689994633197784 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1929150205105543 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16442895866930485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.045329914428293705 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09118998423218727 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19893900025635958 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08844002149999142 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20791904535144567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08909997995942831 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1147689763456583 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26294891722500324 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09591900743544102 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4073979798704386 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2652940349653363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04358007572591305 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.671542995609343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2610889496281743 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3873780369758606 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 13.432062929496169 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2379590878263116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39472896605730057 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10597903747111559 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.20031898748129606 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03908993676304817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07254991214722395 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09286007843911648 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18035003449767828 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04747894126921892 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015369965694844723 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09313994087278843 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04725903272628784 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4332339633256197 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0913599506020546 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17751904670149088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039139995351433754 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09193993173539639 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6030669901520014 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08879904635250568 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19584002438932657 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08904002606868744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14146906323730946 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37900800816714764 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07270998321473598 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49923802725970745 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7436130437999964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03659992944449186 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1478309063240886 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17574895173311234 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.29054901096969843 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0777889508754015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14553999062627554 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03945908974856138 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0719800591468811 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0908590154722333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17695908900350332 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047539942897856236 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525005791336298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09030895307660103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04115002229809761 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.229874906130135 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09122991468757391 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17714907880872488 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03911007661372423 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09164994116872549 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6022469606250525 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08838006760925055 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18897897098213434 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0879489816725254 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14095997903496027 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3781589912250638 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07419905159622431 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5015080096200109 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7377730691805482 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03703997936099768 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.140461001545191 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12873904779553413 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.24532899260520935 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07730000652372837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14603894669562578 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903999458998442 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07116899359971285 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09224994573742151 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17814897000789642 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047569978050887585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015310011804103851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09108893573284149 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042959931306540966 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1878149816766381 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0910189701244235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17711904365569353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03969005774706602 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09110895916819572 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6034080870449543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08767994586378336 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18829898908734322 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08836004417389631 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14196999836713076 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3798090619966388 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0722190598025918 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.498528010211885 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7378419870510697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036229961551725864 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.139350981451571 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.515964939258993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09409000631421804 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02647994551807642 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.18427905160933733 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 66.40265602618456 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.21813903003931046 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.06376998499035835 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04943995736539364 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024409964680671692 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03704999107867479 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.27315900661051273 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07939001079648733 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06999995093792677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15696894843131304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048149959184229374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09092001710087061 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038800062611699104 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07183896377682686 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09083992335945368 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1764199696481228 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015600002370774746 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0903300242498517 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9642250370234251 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0668700085952878 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15218998305499554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038278987631201744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09362003766000271 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6031770026311278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08795002941042185 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1887190155684948 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08805899415165186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14130899216979742 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3798289690166712 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07185002323240042 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.49731810577213764 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.735063036903739 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03658002242445946 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.129111089743674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0910699600353837 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1779090380296111 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048440066166222095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08840998634696007 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03862997982650995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07072999142110348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09198905900120735 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1762190368026495 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046539935283362865 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015159952454268932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08978904224932194 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9627159452065825 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09039998985826969 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17465895507484674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038310070522129536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953001815825701 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5997179541736841 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08812895976006985 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1871790736913681 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08756003808230162 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14109909534454346 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37769798655062914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07208995521068573 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4960679216310382 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.721382956020534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0367600005120039 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.117231022566557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08549995254725218 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11134007945656776 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.390252965502441 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07667997851967812 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17811998259276152 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05059002432972193 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08785002864897251 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03860995639115572 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07149891462177038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07380999159067869 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1569989835843444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04655006341636181 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015239929780364037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1437590690329671 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03257999196648598 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.054155989550054 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07227901369333267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15646906103938818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04437996540218592 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08907902520149946 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19510905258357525 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910999167710543 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18812890630215406 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08685898501425982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11309992987662554 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25979895144701004 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09434996172785759 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39998895954340696 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.224435050971806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04196004010736942 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6148330178111792 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07577904034405947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602389384061098 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04762003663927317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1426889793947339 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03900006413459778 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07117993663996458 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335003465414047 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1570400781929493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04735891707241535 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015090103261172771 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1441199565306306 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0371259413659573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07251999340951443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15686010010540485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044218963012099266 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08977006655186415 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20491902250796556 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08742907084524632 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18719909712672234 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08699996396899223 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11476909276098013 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.26133889332413673 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.094579067081213 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40092808194458485 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.234445022419095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04225003067404032 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6250229673460126 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13891002163290977 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.16478996258229017 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.599017022177577 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07619010284543037 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16628007870167494 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048290006816387177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14202005695551634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03832997754216194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07285003084689379 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392896804958582 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16038899775594473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047569978050887585 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2529479097574949 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040350016206502914 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2184249935671687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07290893699973822 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15842902939766645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05588994827121496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09007903281599283 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20109897013753653 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08868996519595385 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20717899315059185 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895489938557148 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11177896521985531 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22967893164604902 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14552893117070198 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4224278964102268 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2764140265062451 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05383999086916447 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6911029815673828 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15967001672834158 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787894431501627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510090125724673 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07098005153238773 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739699462428689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922891907393932 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04682992585003376 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014870078302919865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25182904209941626 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2530949898064137 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07265002932399511 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15698897186666727 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05625991616398096 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0902790343388915 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20008999854326248 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08826993871480227 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20693906117230654 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08746003732085228 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11271995026618242 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2194190165027976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14303997159004211 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4083990352228284 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2579149333760142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05372997839003801 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6720530111342669 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2485789591446519 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.27439906261861324 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.211244035512209 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1635790104046464 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04819000605493784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25133893359452486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039560021832585335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07179006934165955 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735200010240078 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15965895727276802 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04738999996334314 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014928984455764294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25125895626842976 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2611540732905269 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376004941761494 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15712901949882507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049569993279874325 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25073892902582884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039829988963902 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07234897930175066 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413001731038094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15782902482897043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04740001168102026 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014860066585242748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2510490594431758 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2521849712356925 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.55740899592638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07380999159067869 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15969888772815466 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04816998261958361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2506779273971915 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03841996658593416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07354002445936203 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0740300165489316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15823892317712307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014979974366724491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25153893511742353 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2523939367383718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429893594235182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16279902774840593 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057030003517866135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998904377222061 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21066900808364153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08907006122171879 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20849902648478746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09071896784007549 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1054699532687664 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20826898980885744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14328001998364925 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39770908188074827 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2663339730352163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05458004307001829 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.688523101620376 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07474899757653475 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15971902757883072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04806008655577898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2501989947631955 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03859901335090399 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07127993740141392 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386901415884495 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15831901691854 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04775007255375385 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014979974366724491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25175896007567644 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2547739315778017 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.307241062633693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0741099938750267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16089901328086853 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04715996328741312 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46972790732979774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03929995000362396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07098889909684658 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07331999950110912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15740992967039347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04691001959145069 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014850054867565632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25234895292669535 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07532001473009586 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5653030714020133 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327995263040066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15879003331065178 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0476400600746274 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4687979817390442 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07162895053625107 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0743999844416976 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15890994109213352 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04751002416014671 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014829915016889572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25146896950900555 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07469998672604561 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5619030455127358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294001989066601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1590390456840396 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700000863522291 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.46927796211093664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07111893501132727 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07399998139590025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15732995234429836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046440050937235355 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25187898427248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0754999928176403 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5587429516017437 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.24891900829970837 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3279490629211068 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.167398019693792 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423991337418556 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16111996956169605 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05739904008805752 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47124794218689203 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039439997635781765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0735200010240078 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07382000330835581 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15933997929096222 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755996633321047 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014940043911337852 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25306909810751677 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07619999814778566 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5897939447313547 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.076669966802001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16257900279015303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05589995998889208 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09127997327595949 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20154903177171946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08955993689596653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20994001533836126 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948007598519325 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11191901285201311 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2179490402340889 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14371902216225863 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4086679546162486 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.270524924620986 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05391996819525957 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.690101926214993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517996709793806 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1615199726074934 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.47030800487846136 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03850006032735109 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07239996921271086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477006874978542 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16200006939470768 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04702899605035782 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2527389442548156 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0758500536903739 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5817639650776982 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07190997712314129 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15840004198253155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056748976930975914 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20036904606968164 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896901272237301 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2097790129482746 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08890999015420675 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11294905561953783 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2195789711549878 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14193891547620296 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40893792174756527 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2732050381600857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054438947699964046 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6905119409784675 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07584993727505207 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17537898384034634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05013006739318371 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.36046793684363365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038650003261864185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07193000055849552 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07422000635415316 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16008899547159672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047170091420412064 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2522290451452136 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06433005910366774 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4689440140500665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751490006223321 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17112900968641043 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05650997627526522 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0911590177565813 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20154903177171946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08834002073854208 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20840892102569342 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08891907054930925 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.112948939204216 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22113905288279057 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14317897148430347 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4122980171814561 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2839040718972683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05476991645991802 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7161930445581675 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.25270902551710606 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3407290205359459 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.2914460003376 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.14045997522771358 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2307390095666051 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05539902485907078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3609289415180683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0383890001103282 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07170997560024261 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475900929421186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17905898857861757 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04910002462565899 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014820019714534283 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14526897575706244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06513996049761772 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4418840873986483 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07328006904572248 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15935010742396116 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04523899406194687 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09079009760171175 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21842902060598135 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08999905548989773 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18613901920616627 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08695002179592848 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11443905532360077 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2623489126563072 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09590003173798323 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.405498081818223 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.254915026947856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04352896939963102 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.650993013754487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.10089902207255363 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19957905169576406 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047990004532039165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.251718913204968 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03841903526335955 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07056002505123615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735489884391427 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15642901416867971 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046860077418386936 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015189987607300282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14331901911646128 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0511599937453866 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2486550258472562 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07172999903559685 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16500998754054308 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04456902388483286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08947995956987143 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19577890634536743 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08695898577570915 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18692901358008385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08940999396145344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11347897816449404 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2585289767012 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09404902812093496 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4056480247527361 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2296049389988184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041590072214603424 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6287920298054814 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08343008812516928 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17020897939801216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734995309263468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19691907800734043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03862997982650995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06971997208893299 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07289997301995754 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15701004303991795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04691991489380598 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014789984561502934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14318001922219992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044459011405706406 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1550849303603172 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724999699741602 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16059994231909513 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04397996235638857 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08875003550201654 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.195898930542171 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08691905532032251 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1861689379438758 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09907002095133066 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11422904208302498 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2602789318189025 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09404891170561314 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3977679880335927 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2269050348550081 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041619990952312946 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6165629494935274 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2599590225145221 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3834989620372653 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.335609967820346 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.23623905144631863 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.39200903847813606 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10610988829284906 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.19935891032218933 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03966002259403467 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0701600220054388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09067996870726347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17468002624809742 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04658999387174845 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014630029909312725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0893899705260992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04592898767441511 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.403974019922316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09027007035911083 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1743190223351121 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03767991438508034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09041000157594681 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6004370516166091 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08714897558093071 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18640002235770226 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08752010762691498 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14065904542803764 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37493789568543434 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07137004286050797 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4907379625365138 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7116429517045617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035698991268873215 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1162109915167093 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.17487898003309965 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2882690168917179 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0775789376348257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14649995137006044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867899067699909 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06996002048254013 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09157904423773289 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18925894983112812 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.049350084736943245 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015150057151913643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08965900633484125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03999995533376932 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2290949234738946 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09025901090353727 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17386896070092916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037890044040977955 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065901394933462 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6018480053171515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08776003960520029 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18567906226962805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08816993795335293 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.14015997294336557 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.37542893551290035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07054000161588192 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4899469204246998 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7137319082394242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035280012525618076 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1053010132163763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12494996190071106 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23856910411268473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07852993439882994 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.14339899644255638 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903999458998442 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06969994865357876 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0915600685402751 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18333899788558483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04915997851639986 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015049939975142479 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08963991422206163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040380051359534264 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1796550825238228 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0901299063116312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1737800193950534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038029043935239315 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09857001714408398 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6014569662511349 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08757994510233402 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1859490294009447 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08756900206208229 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13998907525092363 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3743680426850915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07141998503357172 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.48941804561764 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7199130961671472 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03540900070220232 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.111000008881092 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.375855024904013 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09421003051102161 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024958979338407516 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10554993059486151 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.4684569882229 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19236898515373468 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.042930012568831444 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03900995943695307 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02487003803253174 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03720005042850971 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2359891077503562 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.07110997103154659 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06917002610862255 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1560300588607788 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04786904901266098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09068998042494059 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039460021071136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07030903361737728 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09098893497139215 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17555896192789078 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595995854586363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08994899690151215 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.966046005487442 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06601994391530752 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15090894885361195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037770019844174385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09181001223623753 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6017269333824515 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08850905578583479 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18710002768784761 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08686003275215626 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12831890489906073 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3533479757606983 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06627000402659178 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4627880407497287 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6898129833862185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0336299417540431 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0708610536530614 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08417002391070127 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16975996550172567 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048260088078677654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08095998782664537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385199673473835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06567896343767643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08466991130262613 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16868894454091787 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04634005017578602 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014650053344666958 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08167000487446785 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9204660309478641 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0837499974295497 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16970897559076548 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03672996535897255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08980999700725079 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5506869638338685 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890200026333332 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18634891603142023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0867689959704876 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12807990424335003 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3529490204527974 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06557896267622709 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46093796845525503 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6372730024158955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034100026823580265 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0320418989285827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07680896669626236 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10180904064327478 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.190843996591866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07555889897048473 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16434898134320974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07911003194749355 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869994543492794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0656399643048644 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370905950665474 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15549897216260433 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0472100218757987 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015030032955110073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12987898662686348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.030529918149113655 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9992660488933325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07265002932399511 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15655893366783857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04120997618883848 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948997128754854 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1895989989861846 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08679996244609356 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18684891983866692 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08652894757688046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10367005597800016 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24331908207386732 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08799007628113031 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37548900581896305 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1998240370303392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03899994771927595 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.581573043949902 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466902025043964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1598490634933114 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047810026444494724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12898899149149656 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03857002593576908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06593996658921242 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07378996815532446 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1564800040796399 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04682003054767847 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015060068108141422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12990005780011415 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9984560310840607 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07145002018660307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15425891615450382 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04094990435987711 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08944002911448479 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18684891983866692 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0864699250087142 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1879190094769001 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08633907418698072 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10371999815106392 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24309905711561441 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0872600357979536 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37388899363577366 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1834950419142842 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03970006946474314 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5754629857838154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.125508988276124 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1503590028733015 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.4037770023569465 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07524900138378143 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16310892533510923 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047680106945335865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12771901674568653 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03815989475697279 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06650993600487709 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364002522081137 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15815894585102797 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04775996785610914 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014738994650542736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2267890376970172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036920071579515934 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1529849143698812 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07322896271944046 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15879899729043245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05162006709724665 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08915900252759457 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19577902276068926 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08823000825941563 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20347896497696638 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08804909884929657 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10425003711134195 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20625896286219358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12998003512620926 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3792489878833294 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.217884011566639 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050119939260184765 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6213130438700318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07549894507974386 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16126898117363453 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04817999433726072 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2250290708616376 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0402890145778656 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06771995685994625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07332907989621162 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15753903426229954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046410015784204006 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01471000723540783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22667902521789074 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.206484972499311 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07290998473763466 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15746999997645617 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051979091949760914 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09024003520607948 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19695900846272707 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08717901073396206 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20244892220944166 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09122001938521862 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10462908539921045 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20724895875900984 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12984895147383213 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3808679757639766 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2349149910733104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05018897354602814 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6374130500480533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22208900190889835 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24716893676668406 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.964234005659819 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486005779355764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16369903460144997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224979012273252 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03855000250041485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06871891673654318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738899689167738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15805906150490046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046810018830001354 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22609904408454895 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.207775087095797 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07274909876286983 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15548896044492722 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047289999201893806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2250390825793147 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037809950299561024 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06636895705014467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0727999722585082 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15571899712085724 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04697998519986868 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014359946362674236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2257790183648467 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1813649907708168 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4303499376401305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07313000969588757 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15907897613942623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04783004987984896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22519903723150492 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038038939237594604 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07463002111762762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333897519856691 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15795906074345112 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047269975766539574 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2261990448459983 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1998050613328815 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333990652114153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16072997823357582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05176907870918512 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09029998909682035 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18824893049895763 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08731905836611986 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20520901307463646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0873300014063716 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0976689625531435 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20000908989459276 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13060891069471836 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3746079746633768 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.205815002322197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04966999404132366 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6124930698424578 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15942996833473444 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048618996515870094 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2252690028399229 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038260011933743954 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06648001726716757 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07368996739387512 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15720899682492018 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722992889583111 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22706901654601097 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1940250406041741 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.09726204816252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15979900490492582 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04751002416014671 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41999807581305504 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03987993113696575 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06717001087963581 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07428997196257114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15744997654110193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047218985855579376 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479999627918005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22654898930341005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0681999372318387 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4762741047888994 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07266004104167223 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1570889726281166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046740053221583366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41967793367803097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038618920370936394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06683007813990116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07341906893998384 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575889764353633 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703004378825426 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014599994756281376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22611906751990318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06749900057911873 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4669530792161822 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310998626053333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15702901873737574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046410015784204006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41981798131018877 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03797002136707306 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06639002822339535 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363001350313425 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1568590523675084 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700000863522291 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014399061910808086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22641895338892937 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06866001058369875 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4689438976347446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22374896798282862 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30195899307727814 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.865139024332166 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362000178545713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16017898451536894 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04752993118017912 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42025791481137276 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03762997221201658 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06728002335876226 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07440999615937471 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16073998995125294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04717905540019274 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014479970559477806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22697902750223875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06872008088976145 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.479443977586925 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348996587097645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15858898404985666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05241006147116423 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19691907800734043 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08827901910990477 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20321900956332684 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08754001464694738 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10385899804532528 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20570901688188314 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1304690958932042 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3798579564318061 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2208250118419528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050089904107153416 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6269020270556211 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456995081156492 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16043009236454964 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04715996328741312 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4206380108371377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0397699186578393 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06761995609849691 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335992995649576 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15684892423450947 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047110021114349365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014828983694314957 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2268790267407894 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06876001134514809 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4781940262764692 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0731790205463767 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1580389216542244 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0516700092703104 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08949893526732922 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19635900389403105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08759007323533297 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2037188969552517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0870689982548356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10335003025829792 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20509900059551 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12983998749405146 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37915899883955717 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2165550142526627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04962005186825991 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6204130370169878 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07491896394640207 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16121903900057077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04754995461553335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32252795062959194 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038280035369098186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06667000707238913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07274001836776733 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575089991092682 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742003511637449 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015018973499536514 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22719893604516983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05818996578454971 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3697039103135467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733890337869525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15842891298234463 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052239978685975075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08991907816380262 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19646889995783567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08728005923330784 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20375894382596016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08677900768816471 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10395993012934923 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20557898096740246 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12961996253579855 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3791090566664934 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.217545010149479 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05022005643695593 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6237329691648483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2272389829158783 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31034904532134533 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.716158965602517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13047992251813412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21871901117265224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05133997183293104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3235889598727226 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899946957826614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06684998515993357 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733289634808898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15659898053854704 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04640000406652689 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13005896471440792 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058750039897859097 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3316050171852112 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07267005275934935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15668000560253859 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04106003325432539 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884499168023467 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20269898232072592 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08741999045014381 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18641003407537937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08685898501425982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10442000348120928 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24239905178546906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08800998330116272 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3739380044862628 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1996650137007236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039010075852274895 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.582163036800921 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09216996841132641 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17815909814089537 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04733004607260227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22579904180020094 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038779922761023045 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06695999763906002 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07273908704519272 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1552490284666419 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046819914132356644 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014750054106116295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12987898662686348 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04792003892362118 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.184884924441576 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07671001367270947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599389361217618 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040950020775198936 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08770998101681471 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18650898709893227 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0866899499669671 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1868890831246972 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0864989124238491 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10419997852295637 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24316890630871058 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0875500263646245 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3742090193554759 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1817249469459057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038529979065060616 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5654030721634626 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07511896546930075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16172905452549458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724005702883005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17729902174323797 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03855000250041485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06648898124694824 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07288006599992514 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1561290118843317 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04587008152157068 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12958899606019258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04109996370971203 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1054050410166383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07277901750057936 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15660899225622416 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041400082409381866 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08857902139425278 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18772901967167854 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08660007733851671 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18530897796154022 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08685898501425982 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10426994413137436 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24303898680955172 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08738995529711246 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37388899363577366 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1818839702755213 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03929005470126867 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5644630184397101 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2348489360883832 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3510480746626854 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.90128209721297 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2200889866799116 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3716079518198967 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10380998719483614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17886899877339602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03918004222214222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07290905341506004 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08455896750092506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1696390099823475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046410015784204006 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015030032955110073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0821590656414628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04283001180738211 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3502339133992791 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08355907630175352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16837904695421457 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03575999289751053 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08996890392154455 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.54799800273031 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.086418935097754 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18693995662033558 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08664000779390335 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12805894948542118 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.352138071320951 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06630003917962313 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46181795187294483 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6292730579152703 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03404996823519468 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.010870957747102 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16221904661506414 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27141906321048737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07289997301995754 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13007898814976215 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039229984395205975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06529898382723331 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08483999408781528 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1687499461695552 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04673004150390625 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01458998303860426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0820700079202652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03780890256166458 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1541350977495313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08387991692870855 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16927893739193678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 3.104486968368292 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11269899550825357 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.563167966902256 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08946890011429787 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2128490014001727 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08942000567913055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1314389519393444 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3614379093050957 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06680004298686981 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4754479741677642 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.724442932754755 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.034920056350529194 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 5.203027976676822 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12378895189613104 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23459899239242077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07314898539334536 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13145001139491796 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0389699125662446 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0682988902553916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08496001828461885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16859895549714565 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0476400600746274 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01725996844470501 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08229899685829878 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03845000173896551 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.130875083617866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08430902380496264 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16953900922089815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03629992716014385 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09343901183456182 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5497779930010438 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08791894651949406 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18645997624844313 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08725002408027649 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12845906894654036 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35255798138678074 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07445004303008318 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47211803030222654 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.653303042985499 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033549964427948 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0380810601636767 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 13.114774017594755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08950999472290277 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02596003469079733 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.1321900635957718 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.861116089858115 ms for forwarding
--------------------
No. 2
<class 'diffusers.models.embeddings.Timesteps'> take 0.245968927629292 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05376001354306936 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04910002462565899 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03776000812649727 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03875000402331352 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.28598890639841557 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.16262894496321678 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09154900908470154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2082389546558261 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05162903107702732 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09695999324321747 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0396000687032938 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08292903658002615 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08588994387537241 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1723390305414796 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834996070712805 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01883006189018488 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08552905637770891 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0552650783210993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749099999666214 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16291893552988768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03881996963173151 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09535998106002808 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6039279978722334 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09106902871280909 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2069190377369523 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067996870726347 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.133019988425076 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3787080058827996 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06842007860541344 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5035480717197061 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.791591988876462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03612996079027653 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.205791068263352 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09086902718991041 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18001894932240248 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08285907097160816 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038560014218091965 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0687700230628252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08783000521361828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17516000661998987 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731910303235054 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01488998532295227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08402008097618818 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9552659466862679 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08953001815825701 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17716898582875729 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03779993858188391 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09361002594232559 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5568780470639467 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08994992822408676 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1921390648931265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08856994099915028 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13006001245230436 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3584889927878976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06899004802107811 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4766179481521249 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6777230193838477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0353700015693903 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.076731063425541 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08072890341281891 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11014891788363457 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.527652032673359 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07770897354930639 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17245893832296133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04795007407665253 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08235906716436148 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869005013257265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07136003114283085 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07565005216747522 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16310904175043106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014888937585055828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13274000957608223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03270001616328955 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.040364964865148 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07379008457064629 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601299736648798 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042819068767130375 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10901002679020166 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20776002202183008 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08827995043247938 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19321904983371496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0879199942573905 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10663003195077181 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.25753898080438375 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09663996752351522 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40748901665210724 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.287505030632019 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0412199879065156 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6865230863913894 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07611897308379412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16440893523395061 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1325389603152871 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038019963540136814 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07005000952631235 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16091996803879738 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04718895070254803 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015149940736591816 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13376993592828512 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0260050185024738 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08224998600780964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1704399473965168 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04325003828853369 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09095994755625725 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19277899991720915 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08835899643599987 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19108899869024754 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08846994023770094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10630895849317312 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2491689519956708 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09039998985826969 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3874580143019557 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2226050021126866 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042350031435489655 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6332630766555667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1295899273827672 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.15718990471214056 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.655195913277566 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07764995098114014 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17105997540056705 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04750897642225027 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13205991126596928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03837898839265108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07072999142110348 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0905690249055624 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1913490705192089 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05628995131701231 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015199999324977398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23169897031039 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039220089092850685 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2435049284249544 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744090648368001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16268901526927948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05388003773987293 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09396905079483986 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22983900271356106 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08861999958753586 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2176789566874504 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09065901394933462 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1070189755409956 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21404901053756475 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13746891636401415 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.40131795685738325 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3096240581944585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05243998020887375 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7321329796686769 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07724994793534279 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16653898637741804 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04775996785610914 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23054995108395815 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03875000402331352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07254001684486866 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08859008084982634 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19138900097459555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047359964810311794 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23108895402401686 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2685040710493922 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345003541558981 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16126909758895636 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.055240001529455185 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10464899241924286 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2252089325338602 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08978997357189655 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20557898096740246 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969893679022789 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10623002890497446 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21325901616364717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1350600505247712 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3998590400442481 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2985439971089363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05170993972569704 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7180530121549964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22456899750977755 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2522589638829231 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.323843961581588 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0763799762353301 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1697000116109848 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04856893792748451 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22732897195965052 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038220081478357315 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06890005897730589 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16259890981018543 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680000711232424 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01502898521721363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22793898824602365 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2213350273668766 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350998930633068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16098900232464075 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047530047595500946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22662896662950516 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0382100697606802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06938003934919834 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385993376374245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16147003043442965 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796893335878849 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015249941498041153 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22771907970309258 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2116049183532596 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4810690665617585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541908416897058 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16406900249421597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822004120796919 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22631895262748003 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038999016396701336 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07529999129474163 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07925904355943203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17698900774121284 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057150027714669704 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.017900019884109497 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2311189891770482 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.261683995835483 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16506901010870934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05419005174189806 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09148893877863884 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2153489040210843 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890200026333332 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20740902982652187 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0903489999473095 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09918899741023779 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20387896802276373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13246899470686913 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3851780202239752 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2684239773079753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051480019465088844 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.699353102594614 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07457006722688675 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16168889123946428 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04840991459786892 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22681895643472672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038650003261864185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06770004983991385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08576991967856884 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1825790386646986 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05612894892692566 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018150079995393753 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22789905779063702 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.257783966138959 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.35083091724664 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745400320738554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1646089367568493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4215589724481106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03835896495729685 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07000996265560389 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0915289856493473 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18150894902646542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04822004120796919 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759949408471584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22805901244282722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06985000800341368 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5226730611175299 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487891707569361 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16474910080432892 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046860077418386936 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4214479122310877 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06939901504665613 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16246899031102657 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0467600766569376 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014839926734566689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22882898338139057 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07026991806924343 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5016839606687427 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07303000893443823 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16197003424167633 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047069042921066284 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.424167956225574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038280035369098186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06911007221788168 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07559999357908964 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16331905499100685 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04694005474448204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22767891641706228 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07305003236979246 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.504543935880065 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2283800859004259 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32632797956466675 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.024297977797687 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07640896365046501 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16637903172522783 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048140063881874084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.423428020440042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03890995867550373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07116899359971285 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517996709793806 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16434001736342907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047140056267380714 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22939895279705524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07345003541558981 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.528774038888514 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07509998977184296 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16401009634137154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05320890340954065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09276007767766714 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20154903177171946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09294890332967043 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2334390301257372 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08933001663535833 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10522000957280397 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2111499197781086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13269006740301847 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3966079093515873 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.299084979109466 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053060008212924004 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7279230523854494 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07580907549709082 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17086905427277088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755996633321047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4197480157017708 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04094000905752182 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07078901398926973 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07521000225096941 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645389711484313 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04701002035290003 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22718892432749271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07089006248861551 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5206229873001575 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444003131240606 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16381999012082815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054949079640209675 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0932400580495596 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2001790562644601 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957902900874615 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2072890056297183 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948997128754854 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10493001900613308 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21101906895637512 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13275898527354002 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3981180489063263 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.267694984562695 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05656899884343147 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7132629873231053 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07561896927654743 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1683190930634737 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734005779027939 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3222780069336295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03851007204502821 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07467006798833609 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08357001934200525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17926900181919336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047128996811807156 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0147699611261487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2285590162500739 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06055994890630245 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4390239957720041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07482990622520447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16464898362755775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05437992513179779 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09206996764987707 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20001898519694805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0891189556568861 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20612904336303473 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016005787998438 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1055689062923193 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21512899547815323 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13319903519004583 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39895798545330763 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2628049589693546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05183904431760311 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6881220508366823 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22893899586051702 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31132902950048447 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.16537705436349 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1300000585615635 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22274896036833525 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052909948863089085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3232089802622795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03849004860967398 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07098005153238773 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07440894842147827 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16250903718173504 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046020024456083775 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01567997969686985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13363990001380444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06260001100599766 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.374743995256722 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459998596459627 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.165638979524374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04396005533635616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09415997192263603 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19024999346584082 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08972000796347857 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19199890084564686 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895399134606123 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10463001672178507 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2483590506017208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08995004463940859 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3888679202646017 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.233835006132722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04149903543293476 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6512630973011255 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09109894745051861 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17962895799428225 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048569985665380955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22611895110458136 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039380043745040894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0699689844623208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07373001426458359 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15937897842377424 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712992813438177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014960067346692085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13081904035061598 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04922004882246256 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2060049921274185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07373897824436426 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16296899411827326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044800108298659325 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09215902537107468 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19182893447577953 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09022001177072525 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22370892111212015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09047903586179018 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1043189549818635 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24851900525391102 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09252899326384068 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39306795224547386 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2801340781152248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04112999886274338 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6878030728548765 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07958000060170889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1733499811962247 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04708894994109869 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17777003813534975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03865896724164486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06901996675878763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386889774352312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1595490612089634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04584994167089462 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015199999324977398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13030902482569218 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0442001037299633 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1427050922065973 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08783000521361828 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18236006144434214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04367902874946594 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09177997708320618 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19043905194848776 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08915900252759457 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20224903710186481 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10557996574789286 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10419904720038176 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24619896430522203 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09018892887979746 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38448802661150694 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2549649691209197 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04076003096997738 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6764820320531726 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2345390385016799 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3499190788716078 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.316030074842274 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2186790807172656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3723989939317107 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10254897642880678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17879903316497803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0391700305044651 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678900396451354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08748890832066536 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17470901366323233 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0461899908259511 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014840043149888515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08236989378929138 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04464003723114729 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.359154935926199 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08384999819099903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17048895824700594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03720005042850971 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09238009806722403 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5500779952853918 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08899997919797897 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18953904509544373 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0952399568632245 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13385992497205734 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3661089576780796 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06809900514781475 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.48585806507617235 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6859530005604029 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03520003519952297 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0797509932890534 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16240007244050503 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27312804013490677 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0754200154915452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13473897706717253 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03899005241692066 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06787001620978117 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09123992640525103 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1795090502128005 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680897109210491 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08284999057650566 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03942998591810465 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1925440048798919 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0860199797898531 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1726299524307251 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036980025470256805 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09227998089045286 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5509769544005394 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09842996951192617 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2186790807172656 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08751999121159315 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12742902617901564 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3554380964487791 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06731005851179361 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47035806346684694 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6988329589366913 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0349490437656641 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.092100912705064 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12165901716798544 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23465906269848347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07291906513273716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13084011152386665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03879005089402199 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06727001164108515 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08361891377717257 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17046905122697353 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680000711232424 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08306908421218395 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040229992009699345 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.137514947913587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08300901390612125 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16922899521887302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03791996277868748 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1007990213111043 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.551357981748879 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08869892917573452 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1898389309644699 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08794001769274473 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12715905904769897 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35417801700532436 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06760004907846451 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46942802146077156 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.669152989052236 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03436999395489693 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0596709800884128 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.149645968340337 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08599006105214357 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024760025553405285 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.17345999367535114 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.478575924411416 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.18552900291979313 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04205992445349693 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03902893513441086 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02465001307427883 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03560003824532032 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23229909129440784 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0689999433234334 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07791002281010151 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1736590638756752 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05665002390742302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08545001037418842 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03834895323961973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06671994924545288 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08337898179888725 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16871897969394922 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712003283202648 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015089986845850945 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0811589416116476 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9585260413587093 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06562995258718729 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1493289601057768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035710050724446774 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0934000127017498 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5484869470819831 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08729007095098495 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18678908236324787 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08705898653715849 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12708001304417849 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3519890597090125 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06595894228667021 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4617879167199135 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6392129473388195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03384996671229601 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.017502090893686 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08481997065246105 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16974995378404856 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04780897870659828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07975008338689804 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0380099518224597 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06587908137589693 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08367002010345459 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16830000095069408 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703004378825426 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015129917301237583 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08095998782664537 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9187170071527362 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08330994751304388 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17685000784695148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04129891749471426 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10051997378468513 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5454070633277297 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08917995728552341 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21881901193410158 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08738902397453785 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1264300663024187 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35050895530730486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06543903145939112 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45963795855641365 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.682003028690815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03356998786330223 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0896110218018293 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07589894812554121 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10066898539662361 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.184733938425779 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07593899499624968 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16452895943075418 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04830001853406429 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0787100289016962 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0391700305044651 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06617989856749773 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07264898158609867 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15572900883853436 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04678999539464712 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015029916539788246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1292290398851037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03390992060303688 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0123460087925196 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07668999023735523 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16589998267591 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04088995046913624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08898007217794657 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18547906074672937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0867500202730298 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18603994976729155 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08664897177368402 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10218005627393723 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24001894053071737 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0870400108397007 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3717090003192425 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1803850065916777 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03830005880445242 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5731030143797398 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385900244116783 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15811901539564133 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04696997348219156 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12772902846336365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038689933717250824 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06545998621731997 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333001121878624 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15497999265789986 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04695903044193983 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014779972843825817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12892007362097502 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9891749359667301 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07150997407734394 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15455996617674828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04109903238713741 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10631000623106956 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19874901045113802 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0865390757098794 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1855690497905016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08659996092319489 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1027790131047368 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24092895910143852 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08649996016174555 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3707279684022069 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2139850296080112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03879005089402199 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5973929548636079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12413004878908396 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14870997983962297 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.4218670120462775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07443001959472895 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16267993487417698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047408975660800934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12721994426101446 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03812997601926327 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0659100478515029 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07350894156843424 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1576590584591031 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2249690005555749 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03639992792159319 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1512850178405643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07310998626053333 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582399709150195 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05126895848661661 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08957996033132076 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19759905990213156 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08819904178380966 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20427897106856108 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08789997082203627 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10260904673486948 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20478898659348488 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12973893899470568 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3778779646381736 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2187149841338396 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04934996832162142 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6203230479732156 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075190095230937 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1604199642315507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736892879009247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2232890110462904 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03774010110646486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06722006946802139 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08685991633683443 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18739898223429918 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04723900929093361 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22452895063906908 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2274750042706728 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07160997483879328 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15603902284055948 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051480019465088844 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08906004950404167 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19521999638527632 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08805992547422647 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20160898566246033 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08807005360722542 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10353000834584236 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20576000679284334 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13018003664910793 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3792489878833294 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2294749030843377 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04995905328541994 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6303829615935683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22024905774742365 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24488905910402536 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.972053972072899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07477891631424427 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16279902774840593 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047560082748532295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22324896417558193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04596996586769819 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07321906741708517 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736890360713005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15674892347306013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046110013499855995 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501990482211113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22435910068452358 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2158050667494535 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07230998016893864 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15544902998954058 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.050840084441006184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22646901197731495 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03763905260711908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06548990495502949 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07360894232988358 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15656906180083752 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680000711232424 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014560064300894737 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2254989231005311 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2011650251224637 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4622390046715736 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07575994823127985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664899755269289 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04763994365930557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22371893282979727 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04106003325432539 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06690004374831915 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07336004637181759 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1574999187141657 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047699082642793655 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2247589873149991 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2101649772375822 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07216900121420622 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15888898633420467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051690032705664635 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897690188139677 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1895290333777666 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08766993414610624 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2036290243268013 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08714897558093071 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09684998076409101 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19947905093431473 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1300900476053357 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3735689679160714 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2080249143764377 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049410038627684116 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.61212298553437 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08129992056638002 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17618900164961815 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05807995330542326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22595899645239115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038650003261864185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06591901183128357 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07295003160834312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15619909390807152 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04704995080828667 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22506900131702423 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2314548948779702 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.153652931563556 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07326994091272354 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15940004959702492 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04723900929093361 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41699898429214954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04130997695028782 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0665601110085845 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07410894613713026 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15808898024260998 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737009294331074 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22491905838251114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06794009823352098 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4738240279257298 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07256900426000357 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15732902102172375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04833005368709564 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41679805144667625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038319965824484825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06567896343767643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07329997606575489 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15600898768752813 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046600005589425564 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014659948647022247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22492907010018826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07071008440107107 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4701730106025934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0724900746718049 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15643902588635683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046720029786229134 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4162280820310116 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03804999869316816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06614008452743292 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348996587097645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1564699923619628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04718999844044447 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014639925211668015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22433907724916935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06846000906080008 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4615739928558469 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22117991466075182 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2984279999509454 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.855508101172745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739699462428689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596290385350585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047269975766539574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41672796942293644 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038560014218091965 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06736000068485737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284001912921667 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15678000636398792 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779000300914049 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015050056390464306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22531894501298666 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06836000829935074 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4700039755553007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07279892452061176 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15709898434579372 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05147000774741173 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09612902067601681 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23065891582518816 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08885993156582117 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20519900135695934 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0877799466252327 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10326900519430637 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20689901430159807 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13502896763384342 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38952799513936043 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2821839191019535 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04971004091203213 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6892029670998454 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400999311357737 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15899993013590574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05647900979965925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168989835307002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0384089071303606 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06602995563298464 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07383909542113543 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1576789654791355 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047690002247691154 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015030032955110073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248389646410942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0685590784996748 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.481754006817937 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07236993405967951 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15634892042726278 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05147000774741173 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19577995408326387 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08805003017187119 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20445894915610552 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0871800584718585 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10309007484465837 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2096099779009819 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1282090088352561 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38215797394514084 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2225150130689144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049919006414711475 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.624822965823114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07447006646543741 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15922903548926115 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04776997957378626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32024900428950787 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039209029637277126 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06651005242019892 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294898387044668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1553489128127694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04721991717815399 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015020021237432957 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2264389768242836 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05746888928115368 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3741940492764115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333001121878624 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1589789753779769 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05191995296627283 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09014003444463015 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19551999866962433 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0874899560585618 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2020290121436119 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08714990690350533 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10341999586671591 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20673905964940786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13013894204050303 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38128800224512815 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2184149818494916 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04972890019416809 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6242030542343855 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22611895110458136 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30889897607266903 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.781599044799805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12918992433696985 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21627999376505613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05259003955870867 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3210790455341339 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03932998515665531 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06652995944023132 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07329997606575489 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1559889642521739 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680000711232424 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1289290376007557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05797005724161863 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.325465040281415 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07257901597768068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15730899758636951 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04146003630012274 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09040895383805037 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18687895499169827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08720997720956802 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18565903883427382 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08670904207974672 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10337005369365215 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24087901692837477 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08695002179592848 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3715789644047618 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1822640663012862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03867002669721842 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5720529481768608 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09097892325371504 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17751904670149088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480900052934885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22442894987761974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039089005440473557 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.066359993070364 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325899787247181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15525904018431902 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046930043026804924 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1287190243601799 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04759000148624182 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.175994984805584 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07246993482112885 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15679001808166504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04093989264219999 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0895399134606123 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18574902787804604 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08650892414152622 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1868800027295947 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08676003199070692 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10273896623402834 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24108903016895056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08695898577570915 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37199794314801693 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1807649862021208 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039529986679553986 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5715929912403226 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07784005720168352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16438995953649282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04820898175239563 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1764999469742179 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04681898280978203 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653997115790844 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07235899101942778 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15493901446461678 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04695006646215916 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014939927496016026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1286490587517619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041380058974027634 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1261659674346447 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07198995444923639 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15573902055621147 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04096003249287605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08878996595740318 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18651899881660938 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08610996883362532 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1863189972937107 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08614896796643734 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10257004760205746 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2405489794909954 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08684000931680202 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3704590490087867 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1781050125136971 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03930996172130108 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5574030112475157 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23285998031497002 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34845806658267975 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.88564100023359 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2195789711549878 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3742980770766735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10241009294986725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17744896467775106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04115002229809761 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06656895857304335 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08411996532231569 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16871001571416855 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0471300445497036 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015030032955110073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0853600213304162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04543899558484554 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.361663918942213 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08391996379941702 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1676789252087474 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03560003824532032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08955993689596653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5458169616758823 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08709006942808628 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18654903396964073 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08642894681543112 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12600002810359 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3496389836072922 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06939901504665613 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46394800301641226 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.630443031899631 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03347999881953001 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.01034196652472 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16211997717618942 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27009902987629175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07259007543325424 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12872903607785702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03862997982650995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06528000812977552 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08344999514520168 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1675700768828392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04701991565525532 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0810400815680623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037349993363022804 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1470960453152657 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08358003105968237 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1670799683779478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03553996793925762 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08934002835303545 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5453379126265645 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08702999912202358 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18494902178645134 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08615001570433378 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1272499794140458 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3559779142960906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0659299548715353 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4652380011975765 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6275430098176003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033179996535182 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.005430986173451 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12001895811408758 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22991898003965616 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07254898082464933 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1287099439650774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0431290827691555 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06528000812977552 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.083498889580369 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16628892626613379 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0470099039375782 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08064904250204563 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03730005118995905 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1100550182163715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0830390490591526 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16579905059188604 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036280020140111446 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08914899080991745 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5480680847540498 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08850905578583479 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18652004655450583 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08838996291160583 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.126248924061656 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3499679733067751 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0658499775454402 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45947800390422344 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6348130302503705 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03331899642944336 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.012840937823057 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.870228008367121 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08598004933446646 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024499022401869297 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10806997306644917 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.72467893641442 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19090005662292242 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.042980071157217026 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.039680046029388905 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.0247700372710824 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03614893648773432 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2349090063944459 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06822997238487005 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06490899249911308 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1505790278315544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08112902287393808 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038890051655471325 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06714998744428158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08362997323274612 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16785901971161366 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04684901796281338 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015440047718584538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08129992056638002 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9150559781119227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06198999471962452 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15024899039417505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03581005148589611 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09223900269716978 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5595879629254341 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10614003986120224 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19006896764039993 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0871199881657958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12624007649719715 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3510590177029371 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06549002137035131 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46079896856099367 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6797520220279694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033829943276941776 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0582210272550583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08292903658002615 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16917893663048744 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04733004607260227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08006894495338202 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03818992991000414 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06560003384947777 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0838900450617075 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16846891958266497 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04636007361114025 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014808960258960724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08095998782664537 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9173459839075804 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08248910307884216 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16632897313684225 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036010053008794785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08969905320554972 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5452980985864997 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08724897634238005 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18633902072906494 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08685002103447914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13431895058602095 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35883800592273474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06580003537237644 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4677879624068737 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6349030192941427 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03743905108422041 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.03278090339154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07913005538284779 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10639999527484179 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.137164076790214 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07597997318953276 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16449997201561928 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047258916310966015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07861002814024687 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03796000964939594 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.065059051848948 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416994776576757 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15828001778572798 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742993041872978 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01488998532295227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311700325459242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.030789989978075027 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.001776079647243 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07182999979704618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15584903303533792 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04088005516678095 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0890899682417512 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1854489091783762 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08795899339020252 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21726905833929777 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08731998968869448 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10218005627393723 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.23996899835765362 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08730008266866207 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3708780277520418 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2119850143790245 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03879994619637728 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5920030418783426 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07445993833243847 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587499864399433 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047078938223421574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12765999417752028 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03779900725930929 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06577000021934509 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08942896965891123 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17262890469282866 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712992813438177 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015179975889623165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1285590697079897 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.008935971185565 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07201905827969313 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15488907229155302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04081008955836296 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08986901957541704 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18756999634206295 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08594000246375799 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1859589247033 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08570996578782797 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10287994518876076 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24120800662785769 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08646002970635891 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37118804175406694 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1790749849751592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03877899143844843 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5576329315081239 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1242300495505333 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14896993525326252 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.406736978329718 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07504003588110209 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645199954509735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04753901157528162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12866000179201365 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046079978346824646 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06661994848400354 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07301999721676111 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15696894843131304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047029927372932434 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014639925211668015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22519903723150492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03608001861721277 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1652559041976929 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07891003042459488 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17458992078900337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05563907325267792 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015994146466255 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19687891472131014 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08920906111598015 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20322902128100395 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09192002471536398 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10926998220384121 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21641002967953682 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13049005065113306 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39324804674834013 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2532949913293123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04945998080074787 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6870329855009913 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07492001168429852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1595490612089634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04782003816217184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2237389562651515 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04588998854160309 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0722990371286869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07237994577735662 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1563990954309702 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04601001273840666 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014770077541470528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22441905457526445 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2042248854413629 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07197901140898466 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15527906361967325 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05163997411727905 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968997281044722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19709893967956305 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08818006608635187 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20228000357747078 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08770008571445942 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10287901386618614 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20567909814417362 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1311489613726735 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38297800347208977 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.22625392396003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04964007530361414 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6256629023700953 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2207489451393485 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24723901879042387 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.027152994647622 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.075560063123703 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16561895608901978 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779000300914049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2234990242868662 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867002669721842 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06609002593904734 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355993147939444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1586599973961711 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04652910865843296 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2241089241579175 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1909049935638905 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07274001836776733 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15607906971126795 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046900007873773575 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22353907115757465 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842008300125599 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06565998774021864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07297005504369736 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15599001199007034 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0473089748993516 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22486899979412556 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1862448882311583 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4205800145864487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320893928408623 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15953904949128628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722003359347582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2230089157819748 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038260011933743954 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06620900239795446 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07311999797821045 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1568590523675084 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479010097682476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2244689967483282 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.185104949399829 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08093996439129114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1676690299063921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05175999831408262 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015004616230726 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1886490499600768 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08797005284577608 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2036800142377615 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08665001951158047 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09616906754672527 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19821897149085999 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13001903425902128 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37182809319347143 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2030849466100335 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04892004653811455 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.614512992091477 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0737590016797185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15815894585102797 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04784995689988136 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22380903828889132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03831891808658838 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06551994010806084 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07404899224638939 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.157649046741426 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014870078302919865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22466899827122688 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1868650326505303 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.0800729766488075 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365003693848848 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158880022354424 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046339002437889576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4158379742875695 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0386600149795413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0664399703964591 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073118950240314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15566905494779348 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04704005550593138 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22517901379615068 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06846897304058075 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4634339604526758 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07255899254232645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622689887881279 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05547003820538521 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4201479023322463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03857992123812437 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06617908366024494 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284991443157196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15562993939965963 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703004378825426 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22429903037846088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06766000296920538 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4814740279689431 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07281999569386244 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1574100460857153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04669895861297846 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4157889634370804 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03823894076049328 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06962998304516077 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293897215276957 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1559889642521739 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0467000063508749 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014499993994832039 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22398901637643576 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06723904516547918 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4613040257245302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22104894742369652 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2965989988297224 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.853290040045977 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07368007209151983 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15827908646315336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04701002035290003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4170479951426387 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04061905201524496 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06700004450976849 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384899072349072 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15732890460640192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700000863522291 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014410004951059818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2255490981042385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06816897075623274 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4715431025251746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07239996921271086 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15688908752053976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05155999679118395 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0907090725377202 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19590905867516994 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08712999988347292 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20433892495930195 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08683907799422741 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1026099780574441 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20359898917376995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12989994138479233 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3776589874178171 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2161650229245424 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04980002995580435 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.616833033040166 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07414899300783873 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15942903701215982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41656801477074623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06617896724492311 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07331999950110912 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15710899606347084 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046650064177811146 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01501000951975584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22494897712022066 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06885000038892031 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.468572998419404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07278996054083109 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16665994189679623 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05552999209612608 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09336997754871845 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19635900389403105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08826900739222765 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20310899708420038 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08754001464694738 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1030690036714077 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2044489374384284 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12875895481556654 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3765079891309142 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.222635037265718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049669062718749046 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6484030056744814 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07464003283530474 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16077898908406496 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04759000148624182 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.320559018291533 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04572898615151644 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655999459326267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07304898463189602 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15642901416867971 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046289991587400436 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014489982277154922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248990349471569 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057129072956740856 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3730140635743737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07362000178545713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15845999587327242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05170993972569704 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.089599983766675 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21112896502017975 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08847902063280344 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20363903604447842 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0880600418895483 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10295992251485586 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.204798998311162 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12906896881759167 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.377387972548604 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.234745024703443 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04954903852194548 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6381631139665842 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22593897301703691 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30862889252603054 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.730887948535383 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1348300138488412 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23058895021677017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.052270013839006424 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3206990659236908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03854988608509302 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0668989960104227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369997911155224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15674892347306013 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04694005474448204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13094895984977484 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05818996578454971 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3460940681397915 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.071709044277668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15642901416867971 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041100080125033855 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09024899918586016 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18684903625398874 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08706003427505493 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1866089878603816 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08709903340786695 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11120992712676525 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24992902763187885 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08729007095098495 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38140907417982817 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1944450670853257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038520083762705326 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5746729914098978 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09172898717224598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17720903269946575 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22429891396313906 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03881996963173151 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06597896572202444 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335992995649576 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15667895786464214 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04640000406652689 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12890901416540146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0470699742436409 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1753650614991784 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07187901064753532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15552889090031385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040689948946237564 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08914899080991745 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18642889335751534 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08684000931680202 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18560898024588823 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08760998025536537 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10289996862411499 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24226901587098837 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08725002408027649 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37331890780478716 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1825549881905317 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03862893208861351 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.56146299559623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07832993287593126 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16508903354406357 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731002263724804 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17567898612469435 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03811006899923086 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06578001193702221 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0716199865564704 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16149994917213917 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04778895527124405 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12869993224740028 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04119903314858675 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1117550311610103 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07176992949098349 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15569909010082483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0408500200137496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08848996367305517 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18614903092384338 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08633907418698072 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18653902225196362 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08629006333649158 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1028390834107995 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24078902788460255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08682906627655029 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37032796535640955 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1777350446209311 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038770027458667755 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.556233037263155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23334892466664314 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34873897675424814 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.882722002454102 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21929899230599403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37138897459954023 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10222895070910454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17722905613481998 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038869911804795265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06582005880773067 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08362997323274612 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16880000475794077 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0469390070065856 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01446995884180069 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0810499768704176 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.042440020479261875 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.334275002591312 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.083319959230721 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16822002362459898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03535894211381674 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09185005910694599 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5476269870996475 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08649006485939026 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18557894509285688 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08627900388091803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12648000847548246 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3497389843687415 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06585894152522087 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4592180484905839 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6278330003842711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0332400668412447 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0214520627632737 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1602099509909749 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.26997807435691357 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1287889899685979 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03848003689199686 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06590003613382578 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08344894740730524 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16636902000755072 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046810018830001354 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08085998706519604 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03708002623170614 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.147595001384616 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08300901390612125 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1670289784669876 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0355100492015481 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08992897346615791 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5475679645314813 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09533902630209923 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18615892622619867 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08596992120146751 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12604903895407915 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3488480579108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06590003613382578 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45829801820218563 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6342130256816745 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03333005588501692 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0127909956499934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11825899127870798 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22808904759585857 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07221999112516642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12904999312013388 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06605894304811954 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08334999438375235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16661896370351315 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046989996917545795 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014729914255440235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08084997534751892 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03788003232330084 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1098850518465042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08304999209940434 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16703899018466473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03554997965693474 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09799993131309748 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.563467969186604 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08723000064492226 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18517998978495598 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10299007408320904 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13099901843816042 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35789795219898224 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06896001286804676 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4961880622431636 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7189630307257175 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033739954233169556 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0984510192647576 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.945437079295516 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08586002513766289 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024629058316349983 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.12142991181463003 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.647769037634134 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.1843390055000782 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.041950028389692307 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.038619968108832836 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024549895897507668 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.037619960494339466 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23775908630341291 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.08257001172751188 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06469001527875662 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15053001698106527 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0472489045932889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08132995571941137 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038950005546212196 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655906327068806 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08277001325041056 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16700895503163338 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04655006341636181 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014979974366724491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08221995085477829 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9195860475301743 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06267998833209276 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15286006964743137 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03557000309228897 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08968997281044722 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5502570420503616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10404991917312145 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18747907597571611 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.11579901911318302 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12763903941959143 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35167799796909094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06577000021934509 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46167802065610886 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.702783047221601 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035220058634877205 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0894809858873487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0841100700199604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17085904255509377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047749956138432026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07999001536518335 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03856897819787264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06594997830688953 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08379900828003883 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16853900160640478 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047239940613508224 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015189987607300282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08098897524178028 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9236159967258573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0819900305941701 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16452895943075418 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03583007492125034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08934002835303545 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5487980088219047 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08767901454120874 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18647999968379736 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08656003046780825 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12627895921468735 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35007798578590155 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06558001041412354 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45993796084076166 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6306929755955935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033329008147120476 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.007000963203609 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07604993879795074 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10156992357224226 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.149023887701333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07568998262286186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16430893447250128 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04913902375847101 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07870001718401909 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03929995000362396 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0657099299132824 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355003617703915 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15871995128691196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046789064072072506 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01504004467278719 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12889993377029896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.030999071896076202 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0144850239157677 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15749898739159107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04138995427638292 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08838996291160583 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18617906607687473 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08634896948933601 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18665893003344536 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08669006638228893 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10249903425574303 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2417790237814188 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08635898120701313 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3718379884958267 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1801249347627163 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03832997754216194 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5618539182469249 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07860001642256975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1645199954509735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04697893746197224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1271700020879507 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385199673473835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06537896115332842 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327005732804537 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15581899788230658 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04776997957378626 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014949939213693142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12852903455495834 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.999665935523808 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07172010373324156 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15541899483650923 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041319988667964935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893700635060668 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1953189494088292 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08849892765283585 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18688896670937538 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08607993368059397 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303908493369818 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2415589988231659 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08669891394674778 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37112797144800425 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1929350439459085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038710073567926884 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5722130192443728 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12415996752679348 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14886993449181318 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.3971270099282265 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07439008913934231 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16252906061708927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04772993270307779 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13041996862739325 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038809957914054394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655894685536623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284001912921667 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15803903806954622 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014739925973117352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22439903113991022 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036430079489946365 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1546449968591332 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07225992158055305 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15776907093822956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05173997487872839 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08979008998721838 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1966289710253477 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08765992242842913 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20308000966906548 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08797994814813137 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10313896927982569 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2051789779216051 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12854894157499075 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3772379131987691 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.21499493252486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04959001671522856 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6278330003842711 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07463002111762762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16046897508203983 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04821002949029207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2230789978057146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03825908061116934 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06669992581009865 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07351895328611135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15778897795826197 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734995309263468 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2243989147245884 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1900750687345862 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07293000817298889 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15567999798804522 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05157897248864174 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927995804697275 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1962590031325817 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08850893937051296 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20212994422763586 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08710997644811869 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10377890430390835 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20608899649232626 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1329689985141158 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3836780088022351 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2202049838379025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049869995564222336 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6216329531744123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22007001098245382 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24606799706816673 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.9406840009614825 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494003511965275 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16388995572924614 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047869980335235596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22528902627527714 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038950005546212196 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06649002898484468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07314002141356468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1659700646996498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04796893335878849 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467007678002119 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248990349471569 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.203264924697578 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363001350313425 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15797908417880535 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047569978050887585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22316898684948683 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867992199957371 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0662800157442689 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07297995034605265 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1563399564474821 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04768895450979471 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014440040104091167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22520904894918203 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1905450373888016 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4379899259656668 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07280893623828888 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15885895118117332 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797999281436205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22354896645992994 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038229976780712605 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06689003203064203 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07227994501590729 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15607895329594612 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047900015488266945 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014989986084401608 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22413895931094885 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1895050993189216 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739500392228365 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16175908967852592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05209003575146198 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08928996976464987 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18853903748095036 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08730997797101736 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2031699987128377 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08771999273449183 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09861902799457312 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21808897145092487 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1295990077778697 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39203802589327097 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2212350266054273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05137000698596239 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6366930212825537 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.089230015873909 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19080995116382837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870991688221693 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22326898761093616 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03841007128357887 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06610993295907974 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07266993634402752 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15571992844343185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742899909615517 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22478902246803045 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.223224913701415 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.148622043430805 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07396901492029428 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15906896442174911 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04666997119784355 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4165580030530691 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.042260042391717434 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06770004983991385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733700580894947 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.156438909471035 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04715996328741312 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014619901776313782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22449903190135956 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06751902401447296 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4690030366182327 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07839000318199396 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1724290195852518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.055940006859600544 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4198180977255106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869994543492794 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06684905383735895 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400999311357737 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15794998034834862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761992022395134 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.016519916243851185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22764899767935276 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06779003888368607 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.508433953858912 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07149006705731153 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15557999722659588 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046828994527459145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41722902096807957 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038458965718746185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06566010415554047 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07238902617245913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15617907047271729 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046740053221583366 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014510005712509155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22411893587559462 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0675589544698596 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4583839802071452 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22193905897438526 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3008990315720439 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.887679009698331 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390009704977274 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158688984811306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047479989007115364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41734904516488314 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039149075746536255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0665000407025218 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07345899939537048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1570089953020215 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04718999844044447 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0144600635394454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22550905123353004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06986898370087147 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4750930713489652 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07278996054083109 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158019014634192 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05389004945755005 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10697892867028713 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19635993521660566 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08765002712607384 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20455894991755486 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0879199942573905 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1028900733217597 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20488002337515354 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1291589578613639 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.378388911485672 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2386650778353214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04987895954400301 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6524030361324549 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07416005246341228 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16011903062462807 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047839945182204247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4164979327470064 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03870995715260506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06684998515993357 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07295003160834312 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15679001808166504 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0470189843326807 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014779972843825817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2246591029688716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0679700169712305 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4682139735668898 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07236900273710489 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15719898510724306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0515900319442153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006902109831572 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20337908063083887 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0881499145179987 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20345894154161215 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08751905988901854 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10319007560610771 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20506896544247866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12954999692738056 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37890905514359474 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2263139942660928 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05022995173931122 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.62888306658715 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07494993042200804 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16035896260291338 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3200279315933585 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03881007432937622 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06712996400892735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354002445936203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15703996177762747 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687905311584473 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014810007996857166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224979012273252 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05758996121585369 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3623939594253898 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07648905739188194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16380904708057642 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05179003346711397 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896289711818099 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19680894911289215 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08741999045014381 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2019689418375492 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10497902985662222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10828906670212746 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2140790456905961 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12845895253121853 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3892179811373353 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2675540056079626 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04961004015058279 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6776029951870441 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22625899873673916 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.31193799804896116 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.786817943677306 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12982904445379972 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2229990204796195 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0519090099260211 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3207690315321088 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038989936001598835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06695894990116358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08840998634696007 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18558907322585583 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05209003575146198 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014959950931370258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12891890946775675 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05877995863556862 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3738339766860008 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07259997073560953 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15643902588635683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04139996599406004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08888007141649723 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18737895879894495 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08676003199070692 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18611899577081203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08754897862672806 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10324898175895214 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24127901997417212 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08742895442992449 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3729980671778321 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1836349731311202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039019971154630184 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5647630207240582 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0907799694687128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17691904213279486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0473889522254467 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22430892568081617 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03962998744100332 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06619002670049667 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07265899330377579 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15546893700957298 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04701002035290003 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014730030670762062 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12934894766658545 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04791002720594406 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1831650044769049 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07227994501590729 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.156259979121387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04088901914656162 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08926994632929564 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18714892212301493 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0865299953147769 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18597999587655067 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08687993977218866 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10230904445052147 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2403289545327425 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08676992729306221 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37113798316568136 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1809149291366339 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039190053939819336 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5629539266228676 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07433001883327961 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1597789814695716 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04784006159752607 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17596897669136524 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039499951526522636 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06614893209189177 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07280998397618532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15672901645302773 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047359964810311794 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014779972843825817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1289290376007557 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0412199879065156 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1050549801439047 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07232895586639643 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15544902998954058 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04118005745112896 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08910894393920898 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19497901666909456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08688005618751049 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18611899577081203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08604989852756262 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10670989286154509 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24611898697912693 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0872600357979536 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37737900856882334 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.193715026602149 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03885896876454353 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.572893001139164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2329589333385229 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34867902286350727 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.927412098273635 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2182490425184369 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3697589272633195 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10279903654009104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1776090357452631 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950006794184446 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06565998774021864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08465000428259373 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17925002612173557 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05086895544081926 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014880090020596981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08120003622025251 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04287005867809057 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3545339461416006 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08357001934200525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16821001190692186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035698991268873215 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09005004540085793 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5466470029205084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09946990758180618 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18869899213314056 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0861890148371458 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12595008593052626 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3489890368655324 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06591889541596174 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4584980197250843 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6409029485657811 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03350991755723953 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.020351937972009 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1615100773051381 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2706180093809962 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07337995339185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1293590757995844 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04685996100306511 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07187004666775465 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0840499997138977 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.168328988365829 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04717998672276735 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014888937585055828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08069002069532871 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038070022128522396 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1735949665307999 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08347001858055592 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.169368926435709 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03627000842243433 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09132002014666796 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.547108007594943 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08622999303042889 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18728908617049456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10410998947918415 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1283300807699561 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3559989854693413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06689003203064203 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4709279164671898 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6727930633351207 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03366998862475157 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.05649109557271 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12008007615804672 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23057905491441488 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07239996921271086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12875907123088837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06565998774021864 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08446897845715284 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16888894606381655 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014589051716029644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08083006832748652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03738002851605415 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1098450049757957 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08600996807217598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16981898806989193 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03560003824532032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08952000644057989 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5456380313262343 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0866700429469347 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1850089756771922 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08591997902840376 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12634997256100178 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35039905924350023 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06559002213180065 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4593079211190343 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.621933071874082 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03365997690707445 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.003111061640084 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.940117946825922 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08759892079979181 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.0252000754699111 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10920909699052572 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.791678049601614 ms for forwarding
--------------------
No. 3
<class 'diffusers.models.embeddings.Timesteps'> take 0.24450896307826042 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.05482998676598072 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.04796008579432964 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.03850890789180994 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0383799197152257 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.28518890030682087 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.16433896962553263 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09175902232527733 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.20584894809871912 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05217001307755709 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09822903666645288 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0399099662899971 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0843490706756711 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08680997416377068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17610902432352304 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047380104660987854 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018320046365261078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0849999487400055 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0567050194367766 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07508997805416584 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16438891179859638 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03971997648477554 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0964790815487504 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.6128979148343205 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08979905396699905 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19290903583168983 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09023002348840237 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13459997717291117 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.381538993678987 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0702589750289917 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.5088869947940111 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7927719745784998 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03641995135694742 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.2107709664851427 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09385996963828802 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18430000636726618 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870910197496414 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08354999590665102 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03875000402331352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07016898598521948 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08846004493534565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17705897334963083 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046650064177811146 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015169964171946049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08538889233022928 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9667660342529416 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08776003960520029 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17526897136121988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037190038710832596 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09212992154061794 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5644070915877819 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08964003063738346 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19159901421517134 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08870905730873346 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1316290581598878 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.36021892447024584 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0698799267411232 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47884800005704165 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6850029351189733 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03600004129111767 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0833410089835525 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0803100410848856 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10953005403280258 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.552462000399828 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07712002843618393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16962899826467037 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047269975766539574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08565897587686777 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046079978346824646 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07786904461681843 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07537996862083673 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612900523468852 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047659967094659805 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13292906805872917 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03281992394477129 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0802149772644043 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369904778897762 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16324897296726704 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04654005169868469 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10535900946706533 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19168900325894356 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884101027622819 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1916290493682027 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08847902063280344 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10496994946151972 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24699897039681673 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08907006122171879 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3834289964288473 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2338650412857533 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04104000981897116 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6476029995828867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450894918292761 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630790065973997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04738999996334314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1311990199610591 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03937003202736378 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06985000800341368 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07394002750515938 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15832995995879173 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04689896013587713 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015120021998882294 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13234000653028488 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0200859978795052 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07224001456052065 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15722005628049374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04311010707169771 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20234892144799232 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.1026990357786417 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1911489525809884 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09715999476611614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10587892029434443 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24748907890170813 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09020895231515169 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3877180861309171 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2651850702241063 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0439999857917428 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6763419844210148 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12850994244217873 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1547200372442603 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.6892059510573745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07676996756345034 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16887998208403587 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047829002141952515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13150996528565884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03844010643661022 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07095001637935638 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0755189685150981 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16309903003275394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047269975766539574 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2298089675605297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038860016502439976 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1887350119650364 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07384992204606533 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1611500047147274 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05333998706191778 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09958993177860975 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.23953896015882492 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08922000415623188 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20710902754217386 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998892735689878 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10617007501423359 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21502911113202572 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13896997552365065 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4072589799761772 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.3160750968381763 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05110003985464573 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7354629235342145 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0756789231672883 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17144891899079084 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04949001595377922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2289989497512579 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0389789929613471 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07078994531184435 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07543899118900299 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1631489722058177 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04695006646215916 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015110010281205177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2302390057593584 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.23048503883183 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377006113529205 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1605799188837409 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05420902743935585 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20053901243954897 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028904605656862 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20373903680592775 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015994146466255 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10573992040008307 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21243002265691757 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13448006939142942 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39649801328778267 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.255054958164692 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05185999907553196 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6716730315238237 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22668892052024603 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.2528690965846181 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.188272964209318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07651001214981079 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1703089801594615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04804995842278004 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22857892327010632 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03930996172130108 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06971997208893299 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0748300226405263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16270892228931189 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047509907744824886 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01595995854586363 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23101898841559887 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.245983992703259 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07973005995154381 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17822894733399153 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05708006210625172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22829906083643436 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037980033084750175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06930890958756208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07387995719909668 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1610990148037672 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04855007864534855 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22933899890631437 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2442349689081311 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.538059023208916 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475994061678648 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621190458536148 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048440066166222095 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22792001254856586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03885000478476286 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07634004577994347 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07853889837861061 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17617898993194103 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05710998084396124 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.018468941561877728 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2326589310541749 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.264974009245634 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07395900320261717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16436900477856398 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05452998448163271 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09143899660557508 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19661907572299242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08942000567913055 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20720891188830137 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09939889423549175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0996000599116087 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20485895220190287 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13296992983669043 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38664904423058033 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2530640233308077 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0517300795763731 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6744130989536643 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07472897414118052 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1619590912014246 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843005444854498 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22812909446656704 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03826897591352463 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0678299693390727 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07451896090060472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16081903595477343 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047359964810311794 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014819903299212456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22967904806137085 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2146050576120615 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.284342052415013 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07506005931645632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1657690154388547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734995309263468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42506796307861805 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03870995715260506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07026002276688814 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07389008533209562 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612199703231454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04742899909615517 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015069963410496712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22960896603763103 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06972998380661011 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5152939595282078 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754999928176403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1717300619930029 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046588946133852005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41844905354082584 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03882905002683401 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06919004954397678 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07421895861625671 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1612290507182479 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04678999539464712 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014750054106116295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2267389791086316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06939901504665613 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5026630135253072 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07382896728813648 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16252906061708927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046020024456083775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41887792758643627 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03888003993779421 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06889889482408762 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427996024489403 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1626800512894988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700908903032541 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22607902064919472 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07216993253678083 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.504104002378881 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22360903676599264 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3280190285295248 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 5.031829001381993 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07458997424691916 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16399892047047615 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047269975766539574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42002799455076456 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038380036130547523 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07122009992599487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09141897317022085 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18147891387343407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0470699742436409 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014949939213693142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22703898139297962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07051904685795307 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5242930967360735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07475900929421186 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16352895181626081 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053270021453499794 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0929790548980236 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2010889584198594 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09099999442696571 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20636897534132004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09020906873047352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10415900032967329 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21161895710974932 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13178889639675617 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3958080196753144 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2641239445656538 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05246989894658327 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6890530241653323 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08327991236001253 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18444005399942398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.056780059821903706 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42300799395889044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03860995639115572 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07008993998169899 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473991718143225 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16413000412285328 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04658906254917383 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015619909390807152 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22805901244282722 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0708199804648757 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.549614011310041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08209003135561943 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17193995881825686 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05518901161849499 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09243004024028778 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20077894441783428 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09067903738468885 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.207789009436965 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048997890204191 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10459893383085728 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21154899150133133 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13267900794744492 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3958080196753144 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2643750524148345 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053748954087495804 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7022419488057494 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0757700763642788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16922911163419485 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04791002720594406 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3225390100851655 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03818899858742952 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07020996417850256 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08921907283365726 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.19637902732938528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047110021114349365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014978926628828049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22787903435528278 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05997996777296066 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.443633926101029 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486902177333832 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644690055400133 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05522998981177807 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09274901822209358 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.202108989469707 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09013002272695303 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2241689944639802 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09074900299310684 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10463898070156574 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21066900808364153 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13230904005467892 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3938879817724228 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2814839137718081 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051350099965929985 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.7069129971787333 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22882898338139057 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3113779239356518 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.159395984373987 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1297390554100275 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22233894560486078 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05147990304976702 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32288895454257727 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03875000402331352 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07069902494549751 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450999692082405 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616699155420065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047110021114349365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015060068108141422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13166002463549376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06316998042166233 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3757640263065696 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07507903501391411 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1687590265646577 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04397006705403328 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09286904241889715 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19013893324881792 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09133003186434507 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19818893633782864 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08928892202675343 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10492000728845596 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24836906231939793 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0907600624486804 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.391248962841928 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2403939617797732 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04221999552100897 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6578129725530744 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09216996841132641 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1812089467421174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22629997693002224 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038619968108832836 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06836000829935074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348891813308 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15822902787476778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046230037696659565 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015180092304944992 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13070902787148952 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052270013839006424 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2114649871364236 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08184998296201229 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1756299752742052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04535901825875044 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09127997327595949 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19183894619345665 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017891716212034 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21311896853148937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09649991989135742 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10519998613744974 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24773902259767056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.089599983766675 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38777804002165794 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.268445048481226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041368999518454075 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6916929744184017 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.078550074249506 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1718290150165558 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047740060836076736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1775489654392004 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03845000173896551 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06897002458572388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09021000005304813 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.176509958691895 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046288943849503994 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015030032955110073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13043999206274748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04378997255116701 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1537550017237663 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07421895861625671 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16170903109014034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04396005533635616 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081896860152483 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1899789785966277 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08914992213249207 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19020901527255774 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08873897604644299 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10438996832817793 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24692900478839874 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08922000415623188 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3845489118248224 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.214604009874165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0404199818149209 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.611832994967699 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23473892360925674 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3534679999575019 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.291270049288869 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.223289942368865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.38076797500252724 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10289996862411499 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1796400174498558 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038489000871777534 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06794999353587627 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09252899326384068 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18481898587197065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04667998291552067 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014969962649047375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0828700140118599 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04404003266245127 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.392234000377357 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08467002771794796 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17332006245851517 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037280027754604816 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09262003004550934 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5486479494720697 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09053992107510567 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18882902804762125 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08904002606868744 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12751994654536247 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3530090907588601 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06848003249615431 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.469307997263968 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6583229880779982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0350000336766243 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.053500968031585 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16332999803125858 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2753579756245017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07442990317940712 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13401894830167294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03837002441287041 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06759993266314268 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08407002314925194 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16962899826467037 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04701991565525532 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079043805599213 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08283008355647326 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039260019548237324 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1812050361186266 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.085200066678226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17174903769046068 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037809950299561024 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09114004205912352 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5499180406332016 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0909899827092886 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18878898117691278 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08863001130521297 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1281100558117032 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35438896156847477 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06769993342459202 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46883895993232727 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.658843015320599 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03514997661113739 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0508209709078074 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12485007755458355 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23820903152227402 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07244001608341932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13082008808851242 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0386600149795413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0670100562274456 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08431903552263975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17098907846957445 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046569970436394215 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08336908649653196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.043199979700148106 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1533850338310003 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08823000825941563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18050894141197205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03767991438508034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0916400458663702 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5501980194821954 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09048997890204191 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1887190155684948 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0951599795371294 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12757990043610334 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35438896156847477 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06984896026551723 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.47196808736771345 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6759830759838223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03466999623924494 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.079100930131972 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.156036936677992 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08613907266408205 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024609966203570366 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.17121899873018265 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 61.38835696037859 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.18506997730582952 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04178006201982498 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.039079925045371056 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02501008566468954 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03550993278622627 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23229897487908602 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06856000982224941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0687289284542203 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16413896810263395 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.057879951782524586 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08521892596036196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03821996506303549 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06657000631093979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08366990368813276 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16746902838349342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04770001396536827 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015549943782389164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08099991828203201 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9493660181760788 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06532901898026466 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15036901459097862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03529002424329519 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09046902414411306 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5481280386447906 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08760904893279076 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18650898709893227 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08630997035652399 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266889739781618 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35051803570240736 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0654800096526742 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45919802505522966 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6317430417984724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03315007779747248 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0089709432795644 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08423905819654465 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16961898654699326 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04704995080828667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07977907080203295 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038830097764730453 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06543996278196573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0835490645840764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1674490049481392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0466200290247798 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759018085896969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0806800089776516 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9158860193565488 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08309900294989347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1678189728409052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040899962186813354 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10704901069402695 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5476779770106077 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08628994692116976 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21227903198450804 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09219895582646132 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12585008516907692 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3499190788716078 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06765895523130894 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4626479931175709 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6945020761340857 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033439951948821545 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.091021044179797 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07587892469018698 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10051904246211052 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.16525299847126 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07588998414576054 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16457901801913977 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04716997500509024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07844995707273483 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038849073462188244 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06588001269847155 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07269892375916243 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15613902360200882 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046479981392621994 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014820019714534283 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12874905951321125 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.030620023608207703 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9966859361156821 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07241894491016865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15565904323011637 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04157004877924919 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0887489877641201 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18634903244674206 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08681998588144779 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18725893460214138 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.101529061794281 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10763003956526518 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24958897847682238 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08709996473044157 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38325903005898 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.212134025990963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03925000783056021 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5928730135783553 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393897976726294 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585489371791482 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047600013203918934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12822903227061033 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04625995643436909 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07159996312111616 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07326004561036825 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1559500815346837 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04677893593907356 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014770077541470528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.136310001835227 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0217359522357583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07165002170950174 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1542199170216918 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040560029447078705 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08916005026549101 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18723902758210897 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08604000322520733 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18663005903363228 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08569902274757624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10271999053657055 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2415089402347803 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09283004328608513 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3842579899355769 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1980350827798247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04076003096997738 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5863229054957628 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12416893150657415 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14865898992866278 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.444177077151835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07425900548696518 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1625589793547988 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703004378825426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12690899893641472 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03841996658593416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820994894951582 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08213997352868319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17292005941271782 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047678942792117596 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014939927496016026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224609044380486 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035649980418384075 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.176535035483539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07399905007332563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1585390418767929 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05149003118276596 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08996902033686638 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19741908181458712 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08706992957741022 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21119893062859774 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08854898624122143 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10382989421486855 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20556896924972534 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13195001520216465 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38307905197143555 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2295040069147944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04924007225781679 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.632002997212112 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430894766002893 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1597889931872487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04737998824566603 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22326898761093616 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03817991819232702 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0660700025036931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07262011058628559 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15667895786464214 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04666997119784355 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014870078302919865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22452895063906908 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1848550057038665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07169903255999088 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15533901751041412 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05149003118276596 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08926889859139919 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2243289491161704 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09305006824433804 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20099000539630651 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08744001388549805 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10326900519430637 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2040789695456624 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1292990054935217 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37637806963175535 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2518439907580614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.050480011850595474 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6534629976376891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2199190203100443 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24445890448987484 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.989644909277558 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07714994717389345 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.181750045157969 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04836998414248228 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22316910326480865 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842008300125599 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06638001650571823 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07247005123645067 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15663995873183012 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04672899376600981 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22425898350775242 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2063250178471208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0723500270396471 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15556905418634415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700000863522291 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2232190454378724 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038409954868257046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06582902278751135 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369997911155224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1576590584591031 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687998443841934 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014479970559477806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22415898274630308 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1779150227084756 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.425180049613118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07796997670084238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16340892761945724 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047600013203918934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22361893206834793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03829994238913059 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06652995944023132 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07292896043509245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15624903608113527 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04707009065896273 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2245489740744233 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1888040462508798 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0736600486561656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.159728922881186 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05164009053260088 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09989994578063488 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18793006893247366 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08821999654173851 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20279909949749708 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0868398929014802 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09606999810785055 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.1977889332920313 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1302490709349513 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37129793781787157 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2141750194132328 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04946906119585037 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6189729794859886 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07440999615937471 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15924999024719 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047909910790622234 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22405898198485374 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038479920476675034 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0660099321976304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07377995643764734 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15733996406197548 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04683900624513626 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22470892872661352 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1887250002473593 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.088071989826858 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07400894537568092 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16075896564871073 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4163980484008789 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03825989551842213 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0663690734654665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07375993300229311 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15767000149935484 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04715996328741312 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014789984561502934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22451893892139196 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06743008270859718 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4863739488646388 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335992995649576 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1582399709150195 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04710897337645292 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41671900544315577 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03812997601926327 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06618001498281956 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07324898615479469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15709910076111555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04746997728943825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014479970559477806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224679009988904 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06816897075623274 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4687939547002316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07189903408288956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15982904005795717 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046590110287070274 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41622796561568975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03848993219435215 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06593007128685713 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0879800645634532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18781900871545076 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.054539996199309826 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014829915016889572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22425898350775242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06733008194714785 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5070239314809442 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2211789833381772 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.29809901025146246 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.9205690156668425 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08974899537861347 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18311897292733192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04743004683405161 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41729805525392294 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04012009594589472 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06629899144172668 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392000406980515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15853892546147108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04695006646215916 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22506900131702423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06794999353587627 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.496823038905859 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07237005047500134 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15658000484108925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05132902879267931 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09061000309884548 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2024590503424406 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08856900967657566 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2026190049946308 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08838006760925055 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10312895756214857 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20564894657582045 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1280789729207754 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37710799369961023 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2228750856593251 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04952994640916586 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6281430143862963 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07459009066224098 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1601200783625245 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4166880389675498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03836001269519329 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06685999687761068 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07366994395852089 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1621289411559701 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04692003130912781 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014949939213693142 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22559997159987688 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06864999886602163 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4776040334254503 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07311906665563583 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15830900520086288 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051489914767444134 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08901895489543676 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1957489876076579 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08786004036664963 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2023989800363779 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08762907236814499 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1031199935823679 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21084898617118597 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13509008567780256 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3958579618483782 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2338639935478568 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049329944886267185 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6365930205211043 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07554993499070406 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1616890076547861 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047329929657280445 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3201289800927043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037800054997205734 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06653903983533382 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0732300104573369 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16730907373130322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724005702883005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014959950931370258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22493896540254354 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05714001599699259 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3759840512648225 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07334002293646336 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15862996224313974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051628914661705494 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08985004387795925 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1962590031325817 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0869890209287405 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20339898765087128 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798995986580849 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10287901386618614 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20513893105089664 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12970902025699615 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3780380357056856 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2296749046072364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0498499721288681 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.632512896321714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22596900817006826 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3082890762016177 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.770967997610569 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12903998140245676 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21687906701117754 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05423894617706537 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3207989502698183 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038929982110857964 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06580899935215712 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07357995491474867 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571390312165022 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046440050937235355 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014459947124123573 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12840901035815477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06052001845091581 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3297339901328087 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07210904732346535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15535892453044653 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04079006612300873 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896901272237301 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18718000501394272 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08786004036664963 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18839898984879255 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08605001494288445 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1039999770000577 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24162896443158388 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09386008605360985 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37952803540974855 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1936649680137634 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038209022022783756 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.571933040395379 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09071000386029482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1761490711942315 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687998443841934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22413895931094885 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03911007661372423 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06629899144172668 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07269997149705887 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15529000665992498 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046140048652887344 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014969962649047375 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1285499893128872 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04703004378825426 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1690850369632244 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07137993816286325 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15454902313649654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041619990952312946 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08905003778636456 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1879000337794423 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08685002103447914 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18633902072906494 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08555990643799305 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10282895527780056 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24034897796809673 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08666899520903826 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3705780254676938 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1791449505835772 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038779922761023045 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5582529595121741 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07778999861329794 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16499997582286596 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17593905795365572 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867002669721842 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06585009396076202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07306900806725025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15586894005537033 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047079985961318016 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014690100215375423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12898899149149656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04270998761057854 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1118260445073247 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275003008544445 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15681004151701927 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0409900676459074 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10683003347367048 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19641895778477192 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08703896310180426 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18639001064002514 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.086240004748106 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303908493369818 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24182908236980438 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09384891018271446 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38632808718830347 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2343049747869372 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03869005013257265 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6135540790855885 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23371900897473097 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34922792110592127 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.910931996069849 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21754903718829155 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.36863796412944794 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10129006113857031 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1768699148669839 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03873906098306179 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06578001193702221 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08370901923626661 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16734900418668985 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046289991587400436 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014819903299212456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08155906107276678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04303001333028078 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3312639202922583 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08381903171539307 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16807892825454473 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035430071875452995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998904377222061 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5486379377543926 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08689891546964645 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18603901844471693 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08702999912202358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12613891158252954 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35037798807024956 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.065990025177598 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45978801790624857 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6415929421782494 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03336998634040356 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0211710361763835 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16315898392349482 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2725590020418167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07279904093593359 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12858002446591854 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03865896724164486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06476999260485172 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08337898179888725 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17152901273220778 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04715996328741312 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014780089259147644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08119898848235607 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03730994649231434 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.153815072029829 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0835589598864317 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16828905791044235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03545999061316252 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08932896889746189 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5471379263326526 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08657900616526604 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19628892187029123 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08751999121159315 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12627895921468735 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.34951791167259216 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06575998850166798 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45919802505522966 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.636412926018238 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03316998481750488 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0154110388830304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11936889495700598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22968894336372614 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07243896834552288 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1284700119867921 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03891997039318085 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0652089947834611 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08364999666810036 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1674490049481392 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680000711232424 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759949408471584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08118897676467896 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03753998316824436 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1112149804830551 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08328002877533436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16724900342524052 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03602996002882719 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09015889372676611 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5476078949868679 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08628005161881447 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.185479992069304 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08661893662065268 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12655893806368113 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3489890368655324 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06619002670049667 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45847799628973007 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6245029401034117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04761002492159605 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.019250998273492 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.872676921077073 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08579995483160019 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024320092052221298 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10765891056507826 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.71896794997156 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.1895190216600895 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.041619990952312946 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.039269099943339825 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024729990400373936 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03605009987950325 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23377907928079367 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06939005106687546 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327995263040066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1651189522817731 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047209905460476875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08143007289618254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03905990161001682 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06644998211413622 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08374894969165325 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16847893130034208 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046930043026804924 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01537008211016655 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08120900020003319 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9330160683020949 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07463991641998291 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17422903329133987 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03545999061316252 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09155995212495327 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5483480636030436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10459008626639843 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19765004981309175 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08670007809996605 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13449904508888721 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35912799648940563 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06610993295907974 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4688979825004935 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6771029913797975 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038489000871777534 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1011109929531813 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08561997674405575 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1773289404809475 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07982994429767132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038729049265384674 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06529001984745264 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08387898560613394 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16783899627625942 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04696007817983627 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014789984561502934 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08083903230726719 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9294459596276283 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08179992437362671 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16478903125971556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03593997098505497 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08946005254983902 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5450769094750285 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08677004370838404 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1856490271165967 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08733896538615227 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266599865630269 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3500989405438304 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06529898382723331 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.458558090031147 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6234030481427908 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033549964427948 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9996920600533485 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07635902147740126 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1011689892038703 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.173833971843123 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0761690316721797 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16558903735131025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712992813438177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08578994311392307 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03946991637349129 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06577000021934509 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370894309133291 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1574390335008502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04996999632567167 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.017669983208179474 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13251998461782932 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03051001112908125 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0240459814667702 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07258006371557713 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15591003466397524 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041068997234106064 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08831999730318785 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.185888959094882 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08693907875567675 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22242905106395483 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08605001494288445 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10213896166533232 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2401189412921667 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08696899749338627 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3704479895532131 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2169950641691685 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03872998058795929 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5978029696270823 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07335992995649576 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15820004045963287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04679907578974962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12719002552330494 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038578989915549755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06454996764659882 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07351895328611135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15667895786464214 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703993909060955 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759949408471584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12832903303205967 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9907860076054931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07130904123187065 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1540189841762185 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04088005516678095 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08857902139425278 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18697907216846943 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10369997471570969 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18783891573548317 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08600903674960136 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10291999205946922 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24092895910143852 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08683011401444674 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37117896135896444 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1972750071436167 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03850006032735109 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5739130321890116 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12400001287460327 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14855898916721344 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.433356040157378 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07438997272402048 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1627500168979168 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046749948523938656 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12695998884737492 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038209022022783756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06610993295907974 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07330905646085739 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15812902711331844 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047119916416704655 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2247589873149991 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03603997174650431 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1452250182628632 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07217994425445795 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1572199398651719 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05140888970345259 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09094993583858013 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19679905381053686 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08781906217336655 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20182901062071323 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08807005360722542 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10296900290995836 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20535895600914955 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12986897490918636 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3782679559662938 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2170250993221998 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04945008549839258 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6183030093088746 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07552991155534983 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16870000399649143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047468929551541805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22347900085151196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03857002593576908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06663007661700249 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07279007695615292 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1566990977153182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680000711232424 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01460895873606205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22500904742628336 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.195885008201003 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275003008544445 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15688000712543726 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05216000135987997 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09032001253217459 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19503897055983543 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08719903416931629 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20225904881954193 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08672999683767557 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10320893488824368 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2109688939526677 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13529893476516008 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39545807521790266 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2339749373495579 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04958908539265394 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6366030322387815 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.21977897267788649 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24457904510200024 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.938695045188069 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07521000225096941 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16392907127738 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22325897589325905 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0386600149795413 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06619899068027735 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07321999873965979 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15729002188891172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05617004353553057 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22464897483587265 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.199995051138103 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07389893289655447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16873900312930346 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05762011278420687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22689893376082182 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03891903907060623 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06581004709005356 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07276900578290224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15658896882086992 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04759000148624182 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014489982277154922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22441893815994263 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2135650031268597 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.454609959386289 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07356004789471626 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16052997671067715 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04780909512192011 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22337900009006262 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038710073567926884 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06595009472221136 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07393897976726294 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575989881530404 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014750054106116295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2243289491161704 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1875549098476768 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07296004332602024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16044999938458204 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05149003118276596 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18858909606933594 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08685898501425982 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2034889766946435 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0877799466252327 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09685906115919352 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2024590503424406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1300489529967308 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37683802656829357 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.208065077662468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04915997851639986 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6146429115906358 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08913001511245966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18912903033196926 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048359972424805164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2233089180663228 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038139987736940384 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0660099321976304 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355993147939444 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1577299553900957 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046999077312648296 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014610006473958492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22507889661937952 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2199250049889088 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.1120620444417 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07306900806725025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1588389277458191 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04673993680626154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41605799924582243 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046539935283362865 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07335899863392115 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07399998139590025 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16194896306842566 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04716997500509024 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014649936929345131 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22436899598687887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06759003736078739 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4968430623412132 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07253000512719154 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15795999206602573 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04716904368251562 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4160280805081129 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04731002263724804 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06640993524342775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07419998291879892 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15800900291651487 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046939938329160213 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014399993233382702 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22465898655354977 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06788002792745829 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4718539314344525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07276900578290224 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15653902664780617 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04655006341636181 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4162680124863982 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04522001836448908 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07274001836776733 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07342896424233913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15672901645302773 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046779983676970005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014489982277154922 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22436899598687887 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06773904897272587 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4858429785817862 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22073904983699322 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2972290385514498 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.900489002466202 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07363001350313425 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15930994413793087 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04688906483352184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4168989835307002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03836001269519329 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06623007357120514 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07311906665563583 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567989820614457 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046659959480166435 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01480989158153534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22566900588572025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06933894474059343 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4692129334434867 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.073309987783432 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15863892622292042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0513899140059948 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0905690249055624 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19661907572299242 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08775002788752317 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20249898079782724 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0880189472809434 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1026099780574441 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20445906557142735 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12977002188563347 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3774790093302727 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2162750354036689 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049449969083070755 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6198831144720316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07450894918292761 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15937897842377424 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.416408060118556 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038310070522129536 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06614997982978821 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07244001608341932 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15635997988283634 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047098961658775806 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014820019714534283 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22529903799295425 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06812997162342072 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.464274013414979 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07216003723442554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15630898997187614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05151994992047548 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08975004311650991 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19622896797955036 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09606999810785055 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2037289086729288 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08698890451341867 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10305002797394991 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2042289124801755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12813997454941273 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3765879664570093 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2234540190547705 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04973006434738636 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.624683034606278 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751999905332923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16051903367042542 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046909903176128864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32010802533477545 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038809957914054394 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06655999459326267 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07334002293646336 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15723996330052614 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04689907655119896 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479999627918005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22476899903267622 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057150027714669704 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3600640231743455 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07202906999737024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15728897415101528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05185999907553196 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09006902109831572 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19686901941895485 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0872600357979536 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2020790707319975 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08707900997251272 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10330998338758945 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20571891218423843 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12839992996305227 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37789903581142426 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.213634037412703 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04980002995580435 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6156629426404834 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2255900762975216 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3109379904344678 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.685417986474931 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12948899529874325 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2176789566874504 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05188002251088619 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3207180416211486 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038739992305636406 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06634998135268688 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07484003435820341 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15762902330607176 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046900007873773575 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014960067346692085 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1284000463783741 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058030011132359505 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3271040515974164 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07197004742920399 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15634996816515923 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041029066778719425 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08866994176059961 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1877089962363243 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08607900235801935 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18997990991920233 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08555990643799305 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10333897080272436 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24131895042955875 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0874389661476016 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3721079556271434 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1818549828603864 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03861007280647755 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5615239972248673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0911499373614788 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17584895249456167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724995233118534 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22433896083384752 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03862997982650995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07393991108983755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07299904245883226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15688897110521793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700000863522291 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014849938452243805 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1283399760723114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04727998748421669 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1832639575004578 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07278996054083109 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15662005171179771 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040579005144536495 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08827995043247938 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.188139034435153 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08607888594269753 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1859390176832676 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08555001113563776 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10282907169312239 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24042907170951366 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08752907160669565 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37157791666686535 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1776949977502227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038520083762705326 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5569039387628436 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0769599573686719 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.162540003657341 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04658906254917383 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17596990801393986 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038860016502439976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0657099299132824 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07241894491016865 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15551899559795856 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015099998563528061 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12862903531640768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04151999019086361 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.101676025427878 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07270998321473598 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15621993225067854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04080904182046652 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08999998681247234 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1859589247033 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0867689959704876 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18734997138381004 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08628994692116976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10254897642880678 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2413489855825901 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0867689959704876 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3714680206030607 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1869050795212388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03885000478476286 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5661540674045682 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23407896514981985 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3489280352368951 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.856851956807077 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21900900173932314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.36994798574596643 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10238995309919119 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17757900059223175 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906001802533865 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06604904774576426 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08313998114317656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16808893997222185 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046900007873773575 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492991577833891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08125894237309694 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0420999713242054 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.333044026978314 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08308899123221636 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1679690321907401 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03533996641635895 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09031896479427814 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5470280302688479 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0869600335136056 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1860699849203229 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10321894660592079 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266089966520667 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3509980160742998 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06595009472221136 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4607479786500335 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.645902986638248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03337999805808067 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0249009830877185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16390904784202576 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27375901117920876 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07314002141356468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12876908294856548 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03911997191607952 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06539898458868265 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08384999819099903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16754004172980785 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046689994633197784 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014740042388439178 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08034997154027224 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03749004099518061 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1513050412759185 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08328002877533436 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16793003305792809 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036010053008794785 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08927006274461746 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.547286937944591 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0871199881657958 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18611899577081203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08658901788294315 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12612994760274887 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.34953898284584284 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06605894304811954 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45897800009697676 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.624683034606278 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032920041121542454 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.00399209279567 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1186899608001113 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23298896849155426 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07297995034605265 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12913893442600965 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04616996739059687 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07267994806170464 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08463999256491661 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17057894729077816 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046749948523938656 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014829100109636784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08094997610896826 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037689926102757454 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1457150103524327 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08306000381708145 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16714888624846935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03588991239666939 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09111000690609217 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5468780873343349 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08669996168464422 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18608896061778069 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08627993520349264 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1260989811271429 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3485080087557435 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06630003917962313 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4581880057230592 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.626803888939321 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033199903555214405 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.005311078391969 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.884907980449498 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08715991862118244 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024990062229335308 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10661000851541758 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.5837799590081 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.1903790980577469 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.0414201058447361 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03875908441841602 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02441997639834881 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03517000004649162 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2299390034750104 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06877991836518049 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06471993401646614 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1506900880485773 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04744890611618757 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0813300721347332 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038619968108832836 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0668989960104227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08322007488459349 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16777904238551855 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046930043026804924 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015009893104434013 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08093006908893585 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9134160354733467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06213004235178232 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14595000538975 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035460107028484344 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09068998042494059 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5474670324474573 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08754001464694738 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18526904750615358 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08598901331424713 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1265100436285138 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3495189594104886 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06552902050316334 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45796798076480627 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.626263023354113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03344996366649866 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9991409499198198 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0835399841889739 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16971002332866192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04801899194717407 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07947999984025955 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03878003917634487 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06558909080922604 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09840005077421665 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18175900913774967 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04696007817983627 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08085998706519604 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9332960471510887 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08223007898777723 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16599998343735933 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03583997022360563 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08893001358956099 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5469770403578877 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0871800584718585 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18575892318040133 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08635898120701313 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12719002552330494 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3511289833113551 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0652889721095562 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.460107927210629 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6269430052489042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03320001997053623 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0042220130562782 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07598905358463526 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10109902359545231 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.050523952580988 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07554003968834877 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1644390868023038 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07881002966314554 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529047742486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06558990571647882 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07268902845680714 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1566290156915784 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04718999844044447 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12903905007988214 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.030579976737499237 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9972359985113144 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07267901673913002 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15524891205132008 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04046002868562937 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08928903844207525 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19378901924937963 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08684000931680202 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18782902043312788 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08607900235801935 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10226003360003233 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24064898025244474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08709996473044157 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37116906605660915 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1888149892911315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03836001269519329 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5667430125176907 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07405003998428583 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1590490574017167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725006874650717 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1275589456781745 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03861007280647755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06554997526109219 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07330905646085739 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15755905769765377 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04746997728943825 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014620018191635609 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12840901035815477 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9941559983417392 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07159903179854155 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15498895663768053 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04096003249287605 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08835899643599987 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19227899610996246 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08598994463682175 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18759898375719786 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08583895396441221 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10287004988640547 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2404690021649003 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08662999607622623 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3706190036609769 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1824551038444042 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03850006032735109 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5601030318066478 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1239789417013526 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14862895477563143 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.364116979762912 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08274998981505632 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17221900634467602 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475000124424696 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12734008487313986 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03823894076049328 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06631994619965553 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.072818947955966 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15681900549679995 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04691001959145069 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014870078302919865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22468902170658112 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03576907329261303 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.155224977992475 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07269007619470358 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567599829286337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051359995268285275 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08950999472290277 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19635900389403105 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08765899110585451 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20469899754971266 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08798995986580849 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10355899576097727 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20489899907261133 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1290889922529459 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37695805076509714 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2156650191172957 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04924996756017208 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.615112996660173 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456005550920963 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15948002692312002 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047829002141952515 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22339890711009502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0389699125662446 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06601994391530752 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333001121878624 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1580489333719015 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046110013499855995 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014838995411992073 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22439903113991022 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1872448958456516 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08566002361476421 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17025892157107592 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051659997552633286 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08955993689596653 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1985799754038453 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0876100966706872 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20175892859697342 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08687004446983337 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10310998186469078 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20359002519398928 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1284700119867921 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37543894723057747 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2117750011384487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049650087021291256 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.626483048312366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2200889866799116 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24941901210695505 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.931135034188628 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07451896090060472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1637190580368042 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22318901028484106 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038248952478170395 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06704009138047695 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08539902046322823 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1757190329954028 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680990241467953 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22425898350775242 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2178539764136076 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275003008544445 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15563005581498146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047979061491787434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22361893206834793 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045770080760121346 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06973999552428722 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07263000588864088 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15546998474746943 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05433894693851471 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01446995884180069 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22450904361903667 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.205134904012084 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.463779062964022 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07359904702752829 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16007910016924143 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04705996252596378 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22330903448164463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038109952583909035 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0663690734654665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07285992614924908 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15725905541330576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04664005246013403 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014389981515705585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224379007704556 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.185114961117506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0723500270396471 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1594990026205778 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05163997411727905 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08956994861364365 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18811901099979877 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08677004370838404 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20280899479985237 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08723896462470293 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09633006993681192 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.197849003598094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12990005780011415 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3720989916473627 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2002739822492003 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04898000042885542 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6049629775807261 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07395900320261717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15842891298234463 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04843994975090027 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2233490813523531 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038010068237781525 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06622006185352802 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294001989066601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17273903358727694 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04725996404886246 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014739925973117352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22462906781584024 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2041650479659438 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.086382919922471 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07339997682720423 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599800307303667 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04651898052543402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.416139024309814 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03812997601926327 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06599992047995329 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469905540347099 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602990087121725 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047149951569736004 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014770077541470528 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224379007704556 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06736896466463804 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.466964022256434 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07250893395394087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15651900321245193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04727998748421669 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4161880351603031 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038520083762705326 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06578001193702221 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07346994243562222 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1568300649523735 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046369037590920925 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014409888535737991 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2249690005555749 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0670100562274456 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4576941030099988 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07257005199790001 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15686906408518553 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04600000102072954 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4159179516136646 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03950996324419975 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06581004709005356 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07263000588864088 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15660899225622416 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046629924327135086 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014470075257122517 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22433896083384752 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06711995229125023 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4595530228689313 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22069900296628475 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2977689728140831 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.831488942727447 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07359904702752829 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15923904720693827 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05084997974336147 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41651795618236065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03833998925983906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06633903831243515 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07304002065211535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1577290240675211 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047200010158121586 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014859950169920921 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22519903723150492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06799993570894003 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.471483032219112 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07220008410513401 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15655008610337973 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05142891313880682 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08917006198316813 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19701907876878977 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08765899110585451 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2031289041042328 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08682999759912491 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10254897642880678 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2064789878204465 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12963905464857817 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37953793071210384 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2171550188213587 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05010992754250765 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.621933071874082 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08928996976464987 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18145900685340166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047230045311152935 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4166279686614871 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03825000021606684 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06639002822339535 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07348996587097645 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15966990031301975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04686892498284578 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014779972843825817 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2248389646410942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06828992627561092 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4950940385460854 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07294001989066601 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575089991092682 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0512200640514493 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08859997615218163 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.22247899323701859 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08723896462470293 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2036489313468337 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08706003427505493 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1027790131047368 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20404893439263105 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13057899195700884 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37966796662658453 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2480949517339468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049489084631204605 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6498520271852612 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488008122891188 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16099901404231787 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04755996633321047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3201389918103814 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0473889522254467 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06843998562544584 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07364898920059204 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16163906548172235 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0465599587187171 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479010097682476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22528902627527714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05759904161095619 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3900540070608258 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325003389269114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15822902787476778 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051419949159026146 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10125001426786184 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.21719897631555796 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08658994920551777 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2021690597757697 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08716003503650427 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10307994671165943 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20501005928963423 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1301799202337861 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3820389974862337 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2596049346029758 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04951003938913345 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6724730376154184 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22532895673066378 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30870898626744747 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.82520799152553 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12953998520970345 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21733890753239393 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05119992420077324 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32153900247067213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03898900467902422 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06652995944023132 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09497895371168852 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.192908919416368 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687998443841934 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014779041521251202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12881995644420385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05791999865323305 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3888339744880795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08823000825941563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18233899027109146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04105002153664827 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08915003854781389 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18626893870532513 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0872189411893487 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19890896510332823 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09176996536552906 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10315899271517992 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24173897691071033 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08705898653715849 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3725480055436492 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2066850904375315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03878003917634487 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6173130134120584 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09145005606114864 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17969892360270023 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05659996531903744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.227599055506289 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03883999306708574 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06573006976395845 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07234001532196999 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15644007362425327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04643900319933891 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014810007996857166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1283399760723114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04710908979177475 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1969549814239144 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08686992805451155 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18297892529517412 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04126993007957935 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08896901272237301 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19553001038730145 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08692999836057425 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1869789557531476 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10205002035945654 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10811001993715763 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24983903858810663 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0870400108397007 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38325903005898 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2217649491503835 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03872998058795929 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.631713006645441 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07413001731038094 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15988899394869804 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047410023398697376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17584895249456167 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038380036130547523 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06582995411008596 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07269007619470358 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15527999494224787 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0465089688077569 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014660065062344074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12838002294301987 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04124909173697233 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0995749616995454 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07166003342717886 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15555997379124165 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04111998714506626 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0893099932000041 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1857989700511098 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08621893357485533 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18476892728358507 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08589995559304953 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10301894508302212 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24107890203595161 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08666899520903826 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37092796992510557 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1744049843400717 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03893999382853508 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5534539707005024 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23406895343214273 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3514880081638694 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.056322043761611 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21892902441322803 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37068803794682026 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10246899910271168 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17736002337187529 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038899015635252 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06618001498281956 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08354894816875458 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16788893844932318 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04637008532881737 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014630029909312725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08086906746029854 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04208995960652828 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3338039861992002 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08402904495596886 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16768905334174633 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03565999213606119 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08890009485185146 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5467080045491457 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0865900656208396 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18538895528763533 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08581997826695442 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1264300663024187 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3497280413284898 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06558990571647882 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45868800953030586 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6234020004048944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03333995118737221 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0029409788548946 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16084895469248295 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.26902894023805857 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07355993147939444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1285590697079897 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03862997982650995 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06530003156512976 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0846589682623744 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16732898075133562 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04675996024161577 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08071900811046362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03729003947228193 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.155824982561171 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08272891864180565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16595900524407625 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035610049962997437 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08894898928701878 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5470679607242346 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08589890785515308 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1853490248322487 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08588004857301712 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12626906391233206 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3498980076983571 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06582005880773067 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.459898030385375 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6235830262303352 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03373902291059494 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.00022105127573 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11860893573611975 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22818893194198608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0724100973457098 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12838898692280054 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03836990799754858 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06590900011360645 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08449004963040352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16734900418668985 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046959961764514446 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014760065823793411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08096999954432249 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041338964365422726 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1130149941891432 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0828000484034419 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16772898379713297 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035980017855763435 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953001815825701 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5477169761434197 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0872600357979536 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18556893337517977 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08708902169018984 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12600002810359 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3499389858916402 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06569898687303066 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45902805868536234 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6269130865111947 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03304996062070131 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.005751943215728 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.833787917159498 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08834002073854208 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024790060706436634 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10703899897634983 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.573108981363475 ms for forwarding
--------------------
No. 4
<class 'diffusers.models.embeddings.Timesteps'> take 0.24499895516783 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.0542390625923872 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.048959976993501186 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.039130100049078465 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03899994771927595 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.28814899269491434 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.16023998614400625 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09419000707566738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2100690035149455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05129002965986729 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.09979004971683025 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03972894046455622 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08604000322520733 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0892590032890439 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17635896801948547 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04762003663927317 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01875904854387045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08724001236259937 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.068715937435627 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480895146727562 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16326899640262127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03891007509082556 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09605998639017344 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.602736952714622 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09132991544902325 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19498902838677168 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010906796902418 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13295002281665802 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3757589729502797 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06814906373620033 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4992680624127388 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.7748819664120674 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03612006548792124 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.1890399511903524 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08960999548435211 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17939996905624866 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048269983381032944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08308002725243568 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03911007661372423 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06854906678199768 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08764991071075201 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17577898688614368 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046810018830001354 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01513003371655941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08461892139166594 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9552859701216221 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08848996367305517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17526897136121988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03763008862733841 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09212002623826265 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5565269384533167 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09017996490001678 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19160902593284845 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010906796902418 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13052893336862326 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3599680494517088 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06923999171704054 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4784679040312767 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.681413035839796 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035229953937232494 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0791710121557117 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08014997001737356 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.11044007260352373 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.5257520182058215 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07809000089764595 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1714690588414669 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0480700982734561 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08230889216065407 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03900995943695307 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07085001561790705 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0751999905332923 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1605600118637085 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671898204833269 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014950055629014969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13295002281665802 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033608986996114254 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0348549112677574 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325003389269114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16030902042984962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04329998046159744 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09124004282057285 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1925899414345622 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08920906111598015 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19217899534851313 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08786004036664963 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10615005157887936 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2498480025678873 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09083002805709839 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3890079678967595 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2253849999979138 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04129903391003609 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6225130530074239 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07516006007790565 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16337900888174772 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047449953854084015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1321299932897091 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038289930671453476 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07055001333355904 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407902739942074 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1783390762284398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04660990089178085 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014810007996857166 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1336099812760949 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0424359934404492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07338006980717182 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16126898117363453 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04346005152910948 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09092001710087061 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1931691076606512 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0885099871084094 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19123998936265707 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08788995910435915 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1063590170815587 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24887907784432173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0900590093806386 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3873079549521208 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2211949797347188 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04036992322653532 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6193330520763993 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12886000331491232 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1558299409225583 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.584895960055292 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07766997441649437 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1700699795037508 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04735903348773718 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1324900658801198 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03841996658593416 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07049995474517345 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0774790532886982 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16646902076900005 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04730001091957092 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014959950931370258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2319790655747056 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03910902887582779 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1935140937566757 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07369008380919695 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16099901404231787 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05371007137000561 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09188009425997734 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2147490158677101 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09036995470523834 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20987901370972395 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995900861918926 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10623002890497446 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21302897948771715 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13339996803551912 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39507902693003416 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2745349667966366 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05179992876946926 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6928530531004071 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07520895451307297 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1634990330785513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04713993985205889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2303089713677764 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03806001041084528 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07146899588406086 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538008503615856 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1635890221223235 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04688999615609646 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2315689343959093 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.225405023433268 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327995263040066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16070902347564697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05471007898449898 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09151000995188951 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20152900833636522 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995889220386744 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2058199606835842 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08919998072087765 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10598893277347088 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2118090633302927 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13504899106919765 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3960879985243082 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.254094997420907 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051990034990012646 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.673632999882102 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22800895385444164 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.25439902674406767 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.148642976768315 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07702002767473459 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16996893100440502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04892004653811455 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23008906282484531 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03915000706911087 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07024896331131458 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07469998672604561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16260892152786255 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046989996917545795 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2311889547854662 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2307550059631467 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07423898205161095 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16109889838844538 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048079993575811386 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23021898232400417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03885000478476286 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06990996189415455 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07385900244116783 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15976896975189447 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047749956138432026 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014930032193660736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.231329002417624 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2208750704303384 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.5006199721246958 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07562001701444387 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16414991114288568 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04788907244801521 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23042899556457996 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03829994238913059 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07668999023735523 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07417995948344469 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16024999786168337 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04743901081383228 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0147699611261487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22967893164604902 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2302750255912542 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0749389873817563 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1655789092183113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.053990050218999386 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09211897850036621 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1972790341824293 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08971989154815674 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20791904535144567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09036902338266373 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09868002962321043 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20301900804042816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13267993927001953 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38405810482800007 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2456239201128483 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051449984312057495 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.668312936089933 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0752799678593874 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16174896154552698 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0488299410790205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22700894623994827 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03793893847614527 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06820994894951582 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488904520869255 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1614590873941779 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04692003130912781 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014959950931370258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22753910161554813 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.216875039972365 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.250071942806244 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517996709793806 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16593909822404385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04787999205291271 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4214480286464095 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03837002441287041 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07773004472255707 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420999463647604 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16274000518023968 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680897109210491 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014750054106116295 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22792897652834654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06969005335122347 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5096039278432727 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07429998368024826 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16366003546863794 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04692899528890848 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4212879575788975 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.042599975131452084 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06938993465155363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07466005627065897 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16180903185158968 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0467600766569376 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014579971320927143 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22709905169904232 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06962008774280548 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4986039604991674 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370999082922935 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.161609030328691 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04658009856939316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42070902418345213 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037758960388600826 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06943999323993921 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07427902892231941 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16148900613188744 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04661001730710268 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01458998303860426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22743898443877697 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07022905629128218 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4938029926270247 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22528902627527714 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.32461900264024734 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.997108946554363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07517996709793806 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16580906230956316 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671001806855202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42207795195281506 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03823998849838972 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0716199865564704 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07430999539792538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16199005767703056 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767905920743942 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015079975128173828 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2292189747095108 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07117004133760929 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.5122538898140192 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07442000787705183 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1629990292713046 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05365000106394291 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09211001452058554 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20218908321112394 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09296904318034649 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20663905888795853 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09104993660002947 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10464899241924286 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2108690096065402 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13226899318397045 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3953480627387762 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.269784988835454 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05267001688480377 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6953219892457128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0754800857976079 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16595900524407625 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04779000300914049 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.42271800339221954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04025991074740887 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07107004057615995 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07538998033851385 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16467995010316372 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047459034249186516 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015569967217743397 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22962898947298527 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07149996235966682 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.524144085124135 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07463991641998291 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16216898802667856 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05501997657120228 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.092889997176826 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19973900634795427 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09049999061971903 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20681996829807758 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901899766176939 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10447902604937553 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21062896121293306 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1330990344285965 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39514806121587753 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2614850420504808 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052329967729747295 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6854030545800924 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07545005064457655 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16771000809967518 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047258916310966015 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3247589338570833 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03818992991000414 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06924895569682121 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0738899689167738 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1630790065973997 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046720029786229134 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015060068108141422 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2295889426022768 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06131001282483339 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4077939558774233 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07486005779355764 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16468996182084084 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.054900068789720535 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09197997860610485 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20242901518940926 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897400313988328 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20783906802535057 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953001815825701 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10517006739974022 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21276995539665222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13366993516683578 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3966779913753271 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2651450233533978 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05158002022653818 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.692473073489964 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23078906815499067 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3140390617772937 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.064217960461974 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.13049994595348835 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22259901743382215 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051570008508861065 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32492890022695065 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03906001802533865 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.07120997179299593 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0745690194889903 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16041903290897608 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04616007208824158 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014959950931370258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13091904111206532 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.060429912991821766 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3633939670398831 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407995872199535 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16441999468952417 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0438690185546875 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09528000373393297 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19271904602646828 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09078893344849348 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19126897677779198 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09051000233739614 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10448007378727198 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2474189968779683 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08999998681247234 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.38859795313328505 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2379749678075314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041440012864768505 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6615029890090227 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0920990714803338 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18049892969429493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0483799958601594 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22631895262748003 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03903999458998442 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06876001134514809 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07455900777131319 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1607190351933241 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04710990469902754 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525995321571827 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1306090271100402 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049419933930039406 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.203955034725368 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07473002187907696 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16340892761945724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04494993481785059 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09306997526437044 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19123894162476063 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09055901318788528 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19116897601634264 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08952000644057989 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10449904948472977 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24932902306318283 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09199895430356264 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3925580531358719 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2367150047793984 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040689948946237564 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.644221949391067 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07874006405472755 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16844889614731073 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047629931941628456 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17787900287657976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03885000478476286 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0682600075379014 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07522001396864653 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16134907491505146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046740053221583366 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015228986740112305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13019994366914034 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04437996540218592 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1357349576428533 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07415004074573517 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1622200943529606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04370906390249729 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09081000462174416 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19108899869024754 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09000895079225302 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1904189120978117 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08825003169476986 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10374898556619883 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24586892686784267 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08901895489543676 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3849879140034318 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2187049724161625 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0412999652326107 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6193429473787546 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23422902449965477 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.34975900780409575 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.201850974932313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21885905880481005 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37305906880646944 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10761898010969162 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17894897609949112 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039479928091168404 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06786000449210405 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0870689982548356 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1733690733090043 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04616996739059687 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014860066585242748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08262996561825275 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.044569955207407475 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3633450726047158 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08357001934200525 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17084903083741665 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03696000203490257 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09243993554264307 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.548528041690588 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08915003854781389 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18860900308936834 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08837005589157343 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12740993406623602 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35282899625599384 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06778002716600895 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.46848796773701906 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6533529851585627 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0349399633705616 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0469409646466374 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16106897965073586 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2726289676502347 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07309997454285622 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13109005521982908 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03881996963173151 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06796000525355339 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08389900904148817 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1702990848571062 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.06226007826626301 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014868914149701595 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08274998981505632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03936002030968666 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1907750740647316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08546002209186554 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17226894851773977 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03714999184012413 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09203993249684572 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5488579627126455 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897400313988328 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1875689486041665 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08834991604089737 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12743996921926737 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35401899367570877 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06747990846633911 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4687278997153044 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6531829023733735 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03496999852359295 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.046320936642587 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12248009443283081 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2349090063944459 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07284898310899734 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13076001778244972 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038578989915549755 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06726989522576332 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08475908543914557 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17122901044785976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047110021114349365 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015029916539788246 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08283904753625393 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04036002792418003 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1381149524822831 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08302892092615366 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16951898578554392 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03730005118995905 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09157904423773289 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5592480301856995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08885003626346588 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18923904281109571 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884499168023467 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1273999223485589 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35422900691628456 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06773998029530048 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4693369846791029 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6681920969858766 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03438000567257404 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0599609706550837 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 10.073976940475404 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08511904161423445 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024799956008791924 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.17598900012671947 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 60.82338001579046 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.18506008200347424 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.041770050302147865 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.038890051655471325 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024609966203570366 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03676896449178457 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23421901278197765 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.0687700230628252 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06825907621532679 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15375902876257896 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773994442075491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08133891969919205 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039019971154630184 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06688002031296492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08328992407768965 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1673699589446187 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046549015678465366 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014950055629014969 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08212996181100607 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9377460228279233 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06526999641209841 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15074003022164106 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03560993354767561 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09590003173798323 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5489180330187082 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08778006304055452 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18726906273514032 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08743000216782093 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1268000341951847 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35013898741453886 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06559002213180065 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45954808592796326 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.638732966966927 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03357010427862406 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.032640972174704 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08418899960815907 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16955903265625238 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04789000377058983 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08018896915018559 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03869005013257265 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06575009319931269 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.082949991337955 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16615900676697493 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046779983676970005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014779041521251202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08054007776081562 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9180760243907571 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08315895684063435 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16720907296985388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035559991374611855 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0929889502003789 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5453280173242092 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0880989246070385 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1869690604507923 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08592999074608088 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266089966520667 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.350048067048192 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06531004328280687 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45866798609495163 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6309829661622643 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03310898318886757 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0092009799554944 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07612002082169056 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1012199791148305 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.099093938246369 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07589999586343765 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16518891789019108 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04797999281436205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07865903899073601 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03867002669721842 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06524007767438889 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07314002141356468 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15642005018889904 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0470189843326807 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014979974366724491 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12860994320362806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03047008067369461 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0004750220105052 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07231999188661575 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1573499757796526 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041368999518454075 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08885003626346588 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18616905435919762 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08638901636004448 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18613995052874088 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08631008677184582 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10265898890793324 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24101906456053257 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08701905608177185 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3720279783010483 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.178524922579527 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.039470032788813114 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5642439248040318 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0910500530153513 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1764589687809348 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04749000072479248 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12771005276590586 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037710065953433514 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06589898839592934 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07326004561036825 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15625893138349056 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046539935283362865 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014970079064369202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12886908371001482 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0117549682036042 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07244001608341932 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15454005915671587 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04088995046913624 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.088860047981143 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18725893460214138 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08576898835599422 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18728908617049456 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08579995483160019 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1029890263453126 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2411890309303999 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08663896005600691 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37097802851349115 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.178015023469925 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038380036130547523 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5562539920210838 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12454891111701727 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14960893895477057 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.380966933444142 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.074629089795053 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16257900279015303 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047289999201893806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12698909267783165 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03802007995545864 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06588001269847155 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07354991976171732 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15834998339414597 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047408975660800934 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014530029147863388 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22459891624748707 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036109937354922295 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1511450866237283 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08137896656990051 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16720895655453205 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05177001003175974 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08995900861918926 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19817892462015152 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08758995682001114 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2032789634540677 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08831906598061323 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10307994671165943 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2036389196291566 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12961996253579855 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3766289446502924 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2174739968031645 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04915997851639986 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6287530306726694 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07555901538580656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16094895545393229 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047020032070577145 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22284896112978458 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03774999640882015 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06672006566077471 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07256993558257818 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15560898464173079 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046269968152046204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479999627918005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22476899903267622 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1840349761769176 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07202906999737024 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15623902436345816 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051749986596405506 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09023991879075766 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19619904924184084 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08766003884375095 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2015690552070737 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08752895519137383 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10273000225424767 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.21321908570826054 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13061997015029192 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3877381095662713 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2246439000591636 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04972994793206453 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6263129655271769 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22034894209355116 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24519895669072866 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.934015032835305 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480010390281677 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.163209973834455 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048069050535559654 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22312893997877836 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03855000250041485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0662199454382062 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07291999645531178 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15664007514715195 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046869972720742226 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014690100215375423 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22449891548603773 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.187504967674613 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07206003647297621 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15606905799359083 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046689994633197784 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22325897589325905 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03803998697549105 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06592890713363886 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07435993757098913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15835999511182308 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724005702883005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014640041626989841 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22464909125119448 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.181845087558031 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.409999957308173 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07321895100176334 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1602190313860774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047810026444494724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22323906887322664 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03892893437296152 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06665999535471201 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07370894309133291 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15724904369562864 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724005702883005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.015279976651072502 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2254289574921131 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.2048040516674519 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386994548141956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16178993973881006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.052200048230588436 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034003596752882 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18838897813111544 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08816900663077831 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20256894640624523 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08723000064492226 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09661901276558638 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19876891747117043 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12986897490918636 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37259794771671295 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2065049959346652 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04970002919435501 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6141630476340652 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07480999920517206 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1608500024303794 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04848896060138941 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22334896493703127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03830005880445242 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06603007204830647 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15733903273940086 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687998443841934 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014659017324447632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22500904742628336 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1909749591723084 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.101152066141367 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376901339739561 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593489432707429 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04870002157986164 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4164079437032342 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03870995715260506 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06825895980000496 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07390999235212803 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15706999693065882 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047918991185724735 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014569959603250027 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2247788943350315 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06731005851179361 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4789439737796783 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07263000588864088 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15718000940978527 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046770088374614716 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4161689430475235 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04126899875700474 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06622995715588331 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07296900730580091 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15650899149477482 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04663004074245691 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014630029909312725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22404897026717663 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06767897866666317 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.462512998841703 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07234001532196999 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15707907732576132 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04754005931317806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4164580022916198 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03839004784822464 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06609002593904734 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0735399080440402 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1562789548188448 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04654005169868469 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014718971215188503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22406899370253086 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0673000467941165 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4583140145987272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22184895351529121 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30020903795957565 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.850259982049465 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0744799617677927 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16153999604284763 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046869972720742226 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41656801477074623 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03767001908272505 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06640003994107246 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0732900807633996 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15718897338956594 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0472100218757987 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014879973605275154 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22516993340104818 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0683399848639965 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4675039565190673 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0725890276953578 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1567690633237362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051600043661892414 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09016902185976505 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19758904818445444 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0882899621501565 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2194090047851205 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0887290807440877 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10235002264380455 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20455894991755486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12822996359318495 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3766389563679695 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2353849597275257 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04938000347465277 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6383830225095153 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0739790266379714 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15909899957478046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04773004911839962 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4166279686614871 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04836998414248228 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06672902964055538 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07299007847905159 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15680899377912283 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04697008989751339 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014880090020596981 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22489891853183508 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06826990284025669 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4788128901273012 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284991443157196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571900211274624 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05105906166136265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09028008207678795 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19567902199923992 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08842896204441786 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20134903024882078 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0884300097823143 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10270893108099699 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20445906557142735 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12823904398828745 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37617795169353485 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2134250719100237 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04978897050023079 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6139029758051038 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07591000758111477 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16133906319737434 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04654994700103998 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3196089528501034 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03834895323961973 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06627000402659178 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07292896043509245 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15675905160605907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0466200290247798 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479999627918005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22478902246803045 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05763897206634283 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.357093919068575 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07320998702198267 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17217907588928938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05313998553901911 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010906796902418 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19606901332736015 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08640997111797333 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20132900681346655 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08710904512554407 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303000453859568 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20498898811638355 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12973998673260212 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3782090498134494 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2116850120946765 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04975998308509588 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6327130142599344 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22571906447410583 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.30933902598917484 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.705338976345956 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1300199655815959 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2174390247091651 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051870010793209076 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32155890949070454 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039278995245695114 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06638001650571823 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07337902206927538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15771901234984398 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04599010571837425 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759949408471584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12875907123088837 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.058089965023100376 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3283549342304468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07268995977938175 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15661900397390127 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041310093365609646 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0887999776750803 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18749898299574852 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08731998968869448 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19599893130362034 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08671905379742384 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10261998977512121 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24077901616692543 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08666003122925758 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3705188864842057 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1894740164279938 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038400059565901756 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.569533022120595 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09029905777424574 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17630902584642172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.048569985665380955 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2239589812234044 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03829901106655598 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06552995182573795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07339892908930779 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158249051310122 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0458200229331851 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014560064300894737 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.128868967294693 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047110021114349365 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1727449018508196 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0725289573892951 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.154959037899971 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04090997390449047 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0874389661476016 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1865290105342865 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08718995377421379 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18656905740499496 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08599902503192425 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10286003816872835 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24081894662231207 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08687004446983337 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3713590558618307 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1753749568015337 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03842997830361128 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5528430230915546 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0786689342930913 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16479904297739267 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04754005931317806 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17630902584642172 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03946991637349129 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06578001193702221 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07334002293646336 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15695998445153236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046708970330655575 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014599994756281376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.13120006769895554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041338964365422726 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1137149995192885 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07280998397618532 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15698897186666727 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0412800582125783 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08935004007071257 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18694903701543808 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08636899292469025 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18611899577081203 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08611998055130243 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10305899195373058 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.241338973864913 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0870689982548356 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37147803232073784 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1807549744844437 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03871996887028217 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5610839473083615 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23317895829677582 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3488679649308324 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.852322003804147 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.2187490463256836 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.370507943443954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10249903425574303 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1769399968907237 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03865896724164486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06583996582776308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08452893234789371 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1687089679762721 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046479981392621994 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08608901407569647 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0439999857917428 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.341263996437192 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08395907934755087 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16867904923856258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03572995774447918 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09021908044815063 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5464080022647977 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08672894909977913 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18600898329168558 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08707004599273205 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12650899589061737 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.350788002833724 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06585998926311731 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4602280678227544 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.627302961423993 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03375997766852379 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.007901086471975 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16286899335682392 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.27227902319282293 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07245899178087711 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12887991033494473 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038130092434585094 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06552995182573795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08320901542901993 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667989417910576 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046810018830001354 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014650053344666958 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08073903154581785 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037479912862181664 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1482349364086986 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08362892549484968 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16775901895016432 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03581005148589611 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948007598519325 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5477980012074113 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08818891365081072 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18625904340296984 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08714001160115004 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12655893806368113 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.34972792491316795 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06598001345992088 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45919790863990784 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6281130956485868 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0336299417540431 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0083209965378046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.12129906099289656 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.23211899679154158 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07262895815074444 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12881995644420385 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0381499994546175 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0654890900477767 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08354999590665102 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16711908392608166 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04694005474448204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08114997763186693 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03748992457985878 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1098950635641813 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0830900389701128 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16697903629392385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03575009759515524 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08907995652407408 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5468280287459493 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08648005314171314 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1852790592238307 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08719903416931629 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1263889716938138 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3503690240904689 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06646895781159401 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.459737959317863 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.62461306899786 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03370002377778292 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.002952038310468 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.83961799647659 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08632999379187822 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024589942768216133 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10628893505781889 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.29834006726742 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.19298901315778494 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04188995808362961 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03938900772482157 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024580047465860844 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0359599944204092 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.23231899831444025 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06720994133502245 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06556999869644642 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15188998077064753 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04783901385962963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08159992285072803 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03925000783056021 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06698910146951675 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08339004125446081 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16821897588670254 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046569970436394215 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014939927496016026 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0810499768704176 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9160359622910619 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.062159961089491844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14606001786887646 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035609002225100994 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09041000157594681 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5478380480781198 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08807994890958071 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20213902462273836 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08809007704257965 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1266400795429945 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.354548916220665 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0654800096526742 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4637079546228051 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6520330682396889 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03333005588501692 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0245820051059127 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08363905362784863 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16844901256263256 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047720037400722504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07975904736667871 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03846001345664263 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0655400799587369 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08388003334403038 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16779894940555096 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046689994633197784 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014749937690794468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08088001050055027 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9186360985040665 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08148897904902697 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16607902944087982 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03579992335289717 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08974899537861347 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.544038019143045 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08644897025078535 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18569896928966045 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08634000550955534 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1256589312106371 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.34949800465255976 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06558001041412354 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4575479542836547 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6189130255952477 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033359043300151825 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9961509387940168 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07676996756345034 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.10258995462208986 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.057334947399795 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07541000377386808 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16522896476089954 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04703004378825426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07851992268115282 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03839994315057993 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06554997526109219 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07255002856254578 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1548699801787734 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04655902739614248 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014660065062344074 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1281800214201212 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03023992758244276 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9941549506038427 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07187004666775465 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15544891357421875 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04103989340364933 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08884002454578876 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18543004989624023 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08594896644353867 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18741900566965342 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08558004628866911 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10199006646871567 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2396689960733056 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08615991100668907 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3694780170917511 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1775250313803554 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03880891017615795 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5583329368382692 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392000406980515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.158019014634192 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04688999615609646 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1273800153285265 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03831903450191021 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.065250089392066 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07270905189216137 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.155768939293921 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046260072849690914 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01492002047598362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12846896424889565 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9885559557005763 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07154897321015596 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1535590272396803 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04075991455465555 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.10604900307953358 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18741900566965342 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08664000779390335 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18557894509285688 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08634896948933601 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10315992403775454 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24093897081911564 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08619006257504225 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.370489084161818 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.193715026602149 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03849004860967398 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5695630572736263 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12449908535927534 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14923897106200457 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.358255933970213 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07465004455298185 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16229902394115925 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04738999996334314 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12676999904215336 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038209022022783756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06602995563298464 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07337902206927538 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1580989919602871 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04627997986972332 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22426899522542953 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03570900298655033 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1430549202486873 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0727800652384758 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15665998216718435 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051539973355829716 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0901700695976615 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1952890306711197 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08710892871022224 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20276906434446573 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08730997797101736 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1034389715641737 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20493892952799797 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13051903806626797 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3794279182329774 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2140850303694606 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04924996756017208 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6142930835485458 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07460999768227339 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1593100605532527 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04662899300456047 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22340891882777214 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03827002365142107 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06629002746194601 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07277994882315397 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15629897825419903 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04652002826333046 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014760065823793411 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.224379007704556 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1815950274467468 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07131998427212238 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1544200349599123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05155906546860933 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08918996900320053 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19659893587231636 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08750904817134142 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2028190065175295 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08646002970635891 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10311894584447145 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20443892572075129 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12869900092482567 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3764079883694649 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2161850463598967 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04970002919435501 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6141829546540976 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.21987897343933582 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24493900127708912 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.894645000807941 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07488997653126717 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16384897753596306 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2232090337201953 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03799994010478258 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06614997982978821 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07333001121878624 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1573499757796526 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046739005483686924 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22490904666483402 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1973549844697118 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07276993710547686 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15562900807708502 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22333895321935415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385999446734786 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06606895476579666 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07347005885094404 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15759002417325974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04733004607260227 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0147699611261487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22436899598687887 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1816649930551648 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4202900240197778 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0734089408069849 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15920901205390692 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04706007894128561 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22313895169645548 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385199673473835 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06660004146397114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07273000665009022 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15695893671363592 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04767999053001404 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01479999627918005 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22419902961701155 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1867349967360497 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07317995186895132 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16028992831707 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05209899973124266 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897400313988328 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18899899441748857 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08827995043247938 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2035399666056037 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09044003672897816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09669899009168148 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.19833899568766356 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1297489507123828 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.372618087567389 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2236149050295353 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04909001290798187 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.629022997803986 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07479998748749495 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1599000534042716 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04834902938455343 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22317899856716394 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038409954868257046 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06669003050774336 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07325003389269114 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15651900321245193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04687008913606405 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014749937690794468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22457900922745466 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1888350127264857 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.095181939192116 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07420906331390142 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15953904949128628 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0472100218757987 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41592796333134174 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03837002441287041 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06684893742203712 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07365003693848848 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15702901873737574 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04670990165323019 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014499993994832039 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22425898350775242 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0674600014463067 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.461623003706336 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07298006676137447 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15821994747966528 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04604004789143801 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.415947986766696 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03963999915868044 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06624998059123755 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386005017906427 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1580489333719015 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046779983676970005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014330027624964714 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22401998285204172 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06733997724950314 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.462114043533802 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07221894338726997 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1563589321449399 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04704995080828667 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4157280782237649 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.037890044040977955 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06598897743970156 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07339997682720423 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15622004866600037 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0465089688077569 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22422894835472107 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06793008651584387 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4548239996656775 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2212390536442399 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.29750808607786894 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.82684897724539 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07406901568174362 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15880900900810957 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04771002568304539 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41644799057394266 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03774010110646486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06718002259731293 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07303000893443823 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15744997654110193 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04659895785152912 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014210003428161144 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22508902475237846 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06958004087209702 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4681939501315355 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07287994958460331 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1587490551173687 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05172996316105127 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0903300242498517 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19864900968968868 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08859008084982634 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2046390436589718 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08822896052151918 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10323897004127502 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20748900715261698 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12939004227519035 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3805580781772733 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2234150199219584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04963995888829231 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6289629274979234 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07456995081156492 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16023905482143164 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04775007255375385 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4163379780948162 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03842008300125599 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06670993752777576 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07284991443157196 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15616894233971834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04661001730710268 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014510005712509155 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22501894272863865 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06770004983991385 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4646230265498161 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0725799473002553 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15670002903789282 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05154998507350683 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08994992822408676 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20367896649986506 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08938892278820276 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20365905947983265 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0879000872373581 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1027790131047368 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20438898354768753 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12811901979148388 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37580798380076885 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2243050150573254 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04954996984452009 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.626033103093505 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07487006951123476 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16002997290343046 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04736997652798891 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3199690254405141 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03820995334535837 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06648898124694824 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07318996358662844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15695905312895775 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04680000711232424 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014610006473958492 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22480904590338469 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.056909979321062565 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3566840207204223 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07239996921271086 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15619001351296902 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05130900535732508 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08948997128754854 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19664899446070194 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08719903416931629 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20286894869059324 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913500552996993 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10297901462763548 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20405894611030817 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12876908294856548 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37681800313293934 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2177650351077318 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0510199461132288 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6354230465367436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22550905123353004 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3085889620706439 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.6937189809978 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1290700165554881 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21669897250831127 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05186907947063446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.32054889015853405 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03855000250041485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06639899220317602 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07386994548141956 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15735893975943327 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04579999949783087 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01471000723540783 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1279690768569708 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05744991358369589 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.322734053246677 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07208902388811111 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15572900883853436 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04127994179725647 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08967891335487366 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18722901586443186 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08680007886141539 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18614891450852156 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08697900921106339 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1027500256896019 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2410489832982421 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08735002484172583 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3724590642377734 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1824140092357993 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038650003261864185 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5622529899701476 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.09019894059747458 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17660902813076973 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04712992813438177 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22427900694310665 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0475799897685647 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06618991028517485 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07315899711102247 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1556389033794403 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04621990956366062 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.0147699611261487 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.128429033793509 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04704005550593138 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1796150356531143 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07221009582281113 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1548989675939083 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04064000677317381 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08810998406261206 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18713902682065964 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08614000398665667 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1869690604507923 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0861090375110507 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10259996633976698 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24071894586086273 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08651998359709978 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37074892316013575 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.177045051008463 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03849004860967398 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5544230118393898 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07682899013161659 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1633490901440382 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04745996557176113 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1757889986038208 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039019971154630184 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06603903602808714 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0729499151930213 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15567999798804522 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045930035412311554 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12842996511608362 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.040770042687654495 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1004650732502341 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07265002932399511 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15520898159593344 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04127004649490118 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0882099848240614 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18706999253481627 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09488908108323812 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18694903701543808 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08577003609389067 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10244897566735744 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24030893109738827 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08626899216324091 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3696979256346822 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1840150691568851 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03853999078273773 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5615429729223251 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2330790739506483 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.348358997143805 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.83562199305743 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21892902441322803 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.37108908873051405 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10188901796936989 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.17690996173769236 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03959995228797197 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06564997602254152 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08353905286639929 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16735901590436697 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046509900130331516 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014689983800053596 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08097896352410316 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04258996341377497 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.332184998318553 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08237909059971571 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1663590082898736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0356798991560936 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08998892735689878 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5470979958772659 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08655909914523363 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19988894928246737 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08781999349594116 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12663903180509806 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3501379396766424 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06580003537237644 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45998801942914724 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.643373048864305 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033250078558921814 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.020830987021327 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.16102904919534922 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.26930903550237417 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07294898387044668 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12876000255346298 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03839994315057993 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06548897363245487 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08370005525648594 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1679089618846774 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04643993452191353 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01458998303860426 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0811589416116476 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.037019955925643444 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1455649510025978 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08287990931421518 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16673898790031672 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035590026527643204 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08900894317775965 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.548367970623076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08607993368059397 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18632004503160715 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08614908438175917 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1263089943677187 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3498690202832222 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06615894380956888 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4598580999299884 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6258129617199302 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03324996214359999 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0049320301041007 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11757994070649147 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.22807903587818146 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0735200010240078 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12853904627263546 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038520083762705326 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06560003384947777 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08369993884116411 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1671599457040429 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046859029680490494 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014539924450218678 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08066999725997448 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03746990114450455 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1060150573030114 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.082949991337955 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1667899778112769 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.036070006899535656 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08913001511245966 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5457870429381728 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08560996502637863 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18627895042300224 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08609902579337358 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12707000132650137 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3500389866530895 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06590900011360645 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4593179328367114 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.622142968699336 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03307999577373266 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.000092063099146 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.837177000008523 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0866700429469347 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.02435990609228611 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10597903747111559 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.15137107856572 ms for forwarding
<class 'diffusers.models.embeddings.Timesteps'> take 0.20086904987692833 ms for forwarding
<class 'torch.nn.modules.linear.Linear'> take 0.04148005973547697 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.03881996963173151 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024879933334887028 ms for forwarding
<class 'diffusers.models.lora.LoRACompatibleLinear'> take 0.0358290271833539 ms for forwarding
<class 'diffusers.models.embeddings.TimestepEmbedding'> take 0.2322089858353138 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.06873998790979385 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06501888856291771 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1508089480921626 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047740060836076736 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0811390345916152 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.039350008592009544 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06693997420370579 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08351006545126438 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16796891577541828 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04694005474448204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01525005791336298 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08129992056638002 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9177060564979911 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.06187905091792345 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.14613894745707512 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035520060919225216 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0913089606910944 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5482879932969809 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08751894347369671 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1866089878603816 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08685002103447914 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12682902161031961 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3498379373922944 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06583007052540779 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45882805716246367 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.632972969673574 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03328907769173384 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.006690949201584 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08367002010345459 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17020897939801216 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04688999615609646 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07984996773302555 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038789003156125546 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06640993524342775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08326908573508263 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1673690276220441 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04704995080828667 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014830031432211399 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08100899867713451 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9287759894505143 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08201994933187962 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16500905621796846 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035890028811991215 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09034993126988411 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5466680740937591 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08728005923330784 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18599897157400846 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08700904436409473 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12663006782531738 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.35081896930933 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0660790828987956 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4601679975166917 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6279229894280434 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.032890005968511105 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0046719582751393 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0760799739509821 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.1008789986371994 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 6.058614002540708 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07534900214523077 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16385898925364017 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046410015784204006 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.07846998050808907 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03774010110646486 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06583996582776308 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305899634957314 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15616894233971834 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047079985961318016 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014699995517730713 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12935895938426256 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.030629918910562992 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9982259944081306 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0723500270396471 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15541899483650923 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.041499966755509377 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0887200003489852 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18672901205718517 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08679006714373827 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18647906836122274 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08603907190263271 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10296003893017769 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24202896747738123 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0867500202730298 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37299899850040674 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.188363996334374 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03830995410680771 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.568273059092462 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0742690172046423 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15932891983538866 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04722003359347582 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12735906057059765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038529979065060616 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0657099299132824 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07267994806170464 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1544299302622676 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046659959480166435 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014860066585242748 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.128690036945045 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 0.9906960185617208 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07182999979704618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1550390152260661 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04085991531610489 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09286997374147177 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18643890507519245 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08605001494288445 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18671900033950806 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08616899140179157 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10249996557831764 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24065899197012186 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08679996244609356 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37102901842445135 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1812149314209819 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.038720085285604 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5591830015182495 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12386892922222614 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.14856900088489056 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.363487056456506 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07520895451307297 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.18838897813111544 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671001806855202 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12712902389466763 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0385800376534462 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06665010005235672 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07392000406980515 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1575700007379055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046479981392621994 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01488998532295227 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22491905838251114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0354499788954854 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1723850620910525 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07233896758407354 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1572290202602744 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.05119992420077324 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09423901792615652 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19623897969722748 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08819997310638428 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20331901032477617 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08743908256292343 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10351999662816525 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20571902859956026 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12866000179201365 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3781979903578758 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2204940430819988 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04892004653811455 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.619603019207716 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07489905692636967 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15906896442174911 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046959961764514446 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2231589751318097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03873906098306179 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06630003917962313 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07355900015681982 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1574489288032055 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046449946239590645 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014759949408471584 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2244589850306511 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.185344997793436 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07238995749503374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15768990851938725 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051678973250091076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08892000187188387 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19646889995783567 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08817901834845543 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2029590541496873 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08765992242842913 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10319903958588839 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20486896391957998 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12881902512162924 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37713791243731976 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.21522496920079 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049690017476677895 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6207139706239104 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2202289178967476 ms for forwarding
<class 'quant.transformer_blocks.INTDownSample2D'> take 0.24571898393332958 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnDownBlock2D'> take 5.950293969362974 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07670000195503235 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16530905850231647 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04761992022395134 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22341893054544926 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03817991819232702 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06641005165874958 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07327995263040066 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1573300687596202 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04659895785152912 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014720018953084946 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22415898274630308 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1897749500349164 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07230998016893864 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1548889558762312 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04700000863522291 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22293906658887863 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03784999717026949 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0653300667181611 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730700558051467 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15681004151701927 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05530903581529856 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014850054867565632 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22441893815994263 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1859249789267778 ms for forwarding
<class 'quant.transformer_blocks.INTDownBlock2D'> take 2.4160799803212285 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07347902283072472 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1600789837539196 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047860085032880306 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22309902124106884 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03855000250041485 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0667390413582325 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07290008943527937 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1571390312165022 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04708999767899513 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.01467997208237648 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22460892796516418 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1894250055775046 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07291999645531178 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16089901328086853 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0517300795763731 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09053992107510567 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18914893735200167 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08719007018953562 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20359898917376995 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08743000216782093 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.09644997771829367 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.1987389987334609 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.13047002721577883 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37272798363119364 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2029949575662613 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049299909733235836 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6096030594781041 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07407902739942074 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15873904339969158 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04812004044651985 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22521894425153732 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03813905641436577 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.065990025177598 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0882190652191639 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17363904044032097 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0476400600746274 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014999997802078724 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2250390825793147 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.208215020596981 ms for forwarding
<class 'quant.transformer_blocks.INTUNetMidBlock2DCrossAttn'> take 4.099623067304492 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07353001274168491 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15857000835239887 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04636007361114025 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41586800944060087 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03887002822011709 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06644008681178093 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07444992661476135 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1579589443281293 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047269975766539574 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014640041626989841 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2242999617010355 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0675299670547247 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.464643981307745 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07213896606117487 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15688908752053976 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046939938329160213 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.4160180687904358 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038190046325325966 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06588897667825222 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305003236979246 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15650002751499414 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046479981392621994 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014669960364699364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22421905305236578 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06764999125152826 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4573740772902966 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.071900081820786 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15613995492458344 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0465599587187171 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41611900087445974 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03886898048222065 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06582995411008596 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0733289634808898 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15687895938754082 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04724005702883005 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014389981515705585 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22411905229091644 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06765907164663076 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4595231041312218 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22091902792453766 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.2968889893963933 ms for forwarding
<class 'quant.transformer_blocks.INTUpBlock2D'> take 4.828489967621863 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07334002293646336 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16274990048259497 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046528992243111134 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.41611806955188513 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03814010415226221 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06638001650571823 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0730389729142189 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15701900701969862 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04661001730710268 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014729914255440235 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22474909201264381 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06819004192948341 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.4686740469187498 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07212895434349775 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15643902588635683 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051539973355829716 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09010895155370235 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19651895854622126 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08849997539073229 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20301900804042816 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08663907647132874 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.11850998271256685 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.22073904983699322 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12983009219169617 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.39414793718606234 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.232523936778307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04924007225781679 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.637323061004281 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07418997120112181 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15836907550692558 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04734995309263468 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.416778028011322 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.038650003261864185 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06634008605033159 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07376004941761494 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15732902102172375 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04648999311029911 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014680088497698307 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2249189419671893 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.06791006308048964 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.464013010263443 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07283000741153955 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1568499719724059 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051378970965743065 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08962000720202923 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19741908181458712 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08777901530265808 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20264904014766216 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08834002073854208 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10330893564969301 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20419899374246597 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1295190304517746 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37703802809119225 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2149650137871504 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049690017476677895 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6166530549526215 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07502001244574785 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16102998051792383 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047410023398697376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3198990598320961 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03887992352247238 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06663904059678316 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07308006752282381 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15632901340723038 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046269968152046204 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014669960364699364 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2250290708616376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.057330005802214146 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3603550614789128 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07305992767214775 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15725998673588037 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.051678973250091076 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0896799610927701 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.19714899826794863 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08781894575804472 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.2027589362114668 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08739996701478958 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10319903958588839 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.20500901155173779 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.1292190281674266 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3777679521590471 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.2154149590060115 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.049480004236102104 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.6175740165635943 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.2252690028399229 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.3081290051341057 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.679537964984775 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1293590757995844 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.21826894953846931 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.051230075769126415 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.3207790432497859 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03836001269519329 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06662996020168066 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08113996591418982 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16485003288835287 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04614901263266802 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014910008758306503 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12846000026911497 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0579889165237546 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3324640458449721 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0720699317753315 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1550890738144517 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04097004421055317 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08955004159361124 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1860000193119049 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08656899444758892 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1867390237748623 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08570996578782797 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10326900519430637 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.242378911934793 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08680904284119606 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37301797419786453 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1795349419116974 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03853999078273773 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5573329292237759 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0906200148165226 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.17586001195013523 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.047350069507956505 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.22449903190135956 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03827002365142107 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06633007433265448 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07233989890664816 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15547999646514654 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04671898204833269 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014599994756281376 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12859003618359566 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.047678942792117596 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1768350377678871 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07238995749503374 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15426997561007738 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04084000829607248 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08835992775857449 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1877590548247099 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.086418935097754 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.20434893667697906 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08546002209186554 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10303896851837635 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.2419990487396717 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.08755899034440517 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.37317792885005474 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.1978850234299898 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0385199673473835 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5754429623484612 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07403991185128689 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1596900401636958 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0472700921818614 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1757090212777257 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03817002288997173 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0662800157442689 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07275992538779974 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15610898844897747 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.046289991587400436 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014768913388252258 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12813997454941273 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04115002229809761 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.0991949820891023 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.07182999979704618 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.15550001990050077 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04100904334336519 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08837005589157343 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18713902682065964 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08734897710382938 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18691900186240673 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08661998435854912 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.10307901538908482 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.24160905741155148 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0868489732965827 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.3715479979291558 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.181075000204146 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03885000478476286 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.5588339883834124 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.23310899268835783 ms for forwarding
<class 'quant.transformer_blocks.INTUpsample2D'> take 0.348498928360641 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 8.857061970047653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.21805893629789352 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.3701290115714073 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.10213896166533232 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1767800422385335 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03832997754216194 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06542005576193333 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.082758953794837 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1664990559220314 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.04627008456736803 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014899997040629387 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.0805490417405963 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.04230998456478119 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.3292350340634584 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08352892473340034 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.1673690276220441 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035060103982686996 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0897690188139677 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5472279153764248 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08664897177368402 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.1855089794844389 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.0858799321576953 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12603902723640203 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.34963805228471756 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0660700025036931 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.45852805487811565 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6216329531744123 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03350898623466492 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 1.9989509601145983 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.1606589648872614 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.26907899882644415 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07266004104167223 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.12829003389924765 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.03804999869316816 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.0797490356490016 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08377002086490393 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16822898760437965 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.045900000259280205 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014520017430186272 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08105009328573942 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03723008558154106 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1584650492295623 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08281995542347431 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16583898104727268 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.035680015571415424 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08953001815825701 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5472579505294561 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08672010153532028 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18651899881660938 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08679903112351894 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12639991473406553 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3492690157145262 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06560899782925844 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4581980174407363 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6241830307990313 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033130054362118244 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.001951914280653 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.11869997251778841 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.2287690294906497 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.07263000588864088 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.1286490587517619 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.0380099518224597 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06528000812977552 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0849900534376502 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16856903675943613 ms for forwarding
<class 'quant.activation.SiLUQ'> take 0.05038001108914614 ms for forwarding
<class 'torch.nn.modules.dropout.Dropout'> take 0.014829915016889572 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.08093996439129114 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03748002927750349 ms for forwarding
<class 'quant.transformer_blocks.INTResnetBlock2D'> take 1.1096459347754717 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.08309993427246809 ms for forwarding
<class 'torch_int.nn.fused.GroupNormQ'> take 0.16647903248667717 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.03562006168067455 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.09102991316467524 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.5473480559885502 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08626992348581553 ms for forwarding
<class 'torch_int.nn.attention.W8A8B8O8Attention'> take 0.18509896472096443 ms for forwarding
<class 'torch_int.nn.fused.LayerNormQ'> take 0.08580007124692202 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.12634997256100178 ms for forwarding
<class 'quant.activation.GEGLUQ'> take 0.3494479460641742 ms for forwarding
<class 'torch_int.nn.linear.W8A8B8O8Linear'> take 0.06623996887356043 ms for forwarding
<class 'quant.attn.INTFeedForward'> take 0.4588579759001732 ms for forwarding
<class 'quant.attn.INTBasicTransformerBlock'> take 1.6246140003204346 ms for forwarding
<class 'torch_int.nn.conv.W8A8B8O8Conv2D16'> take 0.033559976145625114 ms for forwarding
<class 'quant.transformer_blocks.INTTransFormer2DModel'> take 2.0101809641346335 ms for forwarding
<class 'quant.transformer_blocks.INTCrossAttnUpBlock2D'> take 9.83075809199363 ms for forwarding
<class 'torch.nn.modules.normalization.GroupNorm'> take 0.0867089256644249 ms for forwarding
<class 'torch.nn.modules.activation.SiLU'> take 0.024660024791955948 ms for forwarding
<class 'torch.nn.modules.conv.Conv2d'> take 0.10525004472583532 ms for forwarding
<class 'quant.transformer_blocks.INTUNet2DConditionModel'> take 58.22441098280251 ms for forwarding
